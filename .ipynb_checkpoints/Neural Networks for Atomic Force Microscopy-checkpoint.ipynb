{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Atomic Force Microscopy \n",
    "\n",
    "\n",
    "### Problem Statement:\n",
    "Our lab previously developed a cutting edge microscope technique to measure ultrafast kinetics in solar cells, batteries, and other optoelectronic devices. \n",
    "\n",
    "In an ideal world, the input data from this experiment would look something like this simulation I ran where the red and blue curves are corresponding to samples with different kinetics:\n",
    "\n",
    "<img src = \"Data/example_simulated_displacements.png\">\n",
    "\n",
    "However, in reality this technique generates very noisy data. See below for an example of the real raw data from this technique:\n",
    "\n",
    "<img src = \"Data/example_real_raw_displacements.png\">\n",
    "\n",
    "Note that the above two curves are basically indistinguishable even though the kinetics vary by 3 orders of magnitude.\n",
    "\n",
    "In order to extract kinetics from this noisy data, we typically collect hundreds (or thousands) of samples per pixel to be able to get a high enough signal-to-noise ratio to do some complicated mathematics to indirectly extract the kinetics.\n",
    "\n",
    "This approach is not great--it requires >4 hours per scan which makes imaging practically impossible.  Furthermore, it relies on cumbersomely difficult mathematics to give us an indirect relationship between kinetic distributions--it does not directly give us the value we wish to measure.\n",
    "\n",
    "**This technique would be far superior if we could directly input one raw data measurement per pixel and directly output the associated kinetic time constant.**\n",
    "\n",
    "I designed a way to directly translate from one raw input pixel to one outputted kinetic time constant using convolutional neural networks and I'll walk you through some of that work below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import configparser\n",
    "import sklearn.utils\n",
    "from igor.binarywave import load as loadibw\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Conv1D, MaxPooling1D, Flatten, Merge\n",
    "from keras.utils import np_utils\n",
    "from keras import callbacks\n",
    "from keras import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "\n",
    "class CNN():\n",
    "    \"\"\"This class contains every function needed to load training data, train a neural network, test a neural network, \n",
    "    save the trained network, and visualize various metrics/parameters.  See specific functions for more in-depth \n",
    "    explanations, or cells below this cell for examples of using this class.\n",
    "    \n",
    "    This neural network is designed to work with two different types of input data:\n",
    "    simulated or experimentally measured cantilever displacement data.\n",
    "    The simulations are run in Python (utilizing generate_train_data.py) and are in .npy format\n",
    "    The experiments are run on an Asylum Cypher ES atomic force microscope which uses Igor so the experimental data\n",
    "    are collected into .ibw files\n",
    "    \n",
    "    All functions for simulated data have been removed from this class for clarity.  They are located in a cell at the \n",
    "    bottom of this notebook, and should still be functional with minimal changes needed.\n",
    "    \n",
    "    Note that there are currently several blocks of code commented out with the comment \"#many tips implementation\".\n",
    "    These blocks of code are for a feature I am developing that makes the neural network tip-agnostic.\n",
    "    Currently the neural network needs to be trained on the same AFM tip the experiment is ran on.\n",
    "    The #many tips implementation would eliminate this requirement by training the neural net on MANY different tips\n",
    "    with different tip states.  Then, before starting a neural net experiment, one needs to only measure the tip parameters\n",
    "    k, Q, and omega, and feed these as inputs to the neural network and it will characterize the displacement data\n",
    "    appropriately based upon this tip state.\"\"\"\n",
    "    \n",
    "        \n",
    "    def load_experimental_train_data(self, *paths, sample_rate = 10, total_time = 3.2768, pre_trigger = 0.5):\n",
    "        \"\"\"This function loads experimental displacement training data generated by the Cypher microscope using\n",
    "        using my point scan script currently located both on the Cypher microscope and the flash drive.\n",
    "        \n",
    "        Inputs:\n",
    "        sample_rate = 10 or 100 (Gage card sampling rate in MS. \n",
    "                                Errors occur if this does not match the sampling rate used during the data collection)\n",
    "        total_time = total time of acquisition in ms (float)\n",
    "        pre_trigger = at what percentage of the total acquisition time does the trigger occur?  \n",
    "                      A parameter set during AFM data acquisition and historically has been 0.5 due to windowing function\n",
    "                      optimizations.  Could be reduced now to maximize data collection efficiency and throw away less\n",
    "                      pre-pulse data points.\n",
    "        *paths = e.g. (\"G:/2018_10_05 CNN voltage pulses/New tip 3/Run1\",\n",
    "                       \"G:/2018_10_05 CNN voltage pulses/New tip 4/Run1\",\n",
    "                       \"G:/2018_10_05 CNN voltage pulses/New tip 5/Run1\")\n",
    "            where each path is to a folder containing several items:\n",
    "            \n",
    "            if x = # of taus, then:\n",
    "            \n",
    "            1 to x) Subfolders named 0, 1, ... , x-1 where the training data for \n",
    "            each 1 of the x Taus in the training data is in its own subfolder\n",
    "            \n",
    "            x+1) Within the Run folder (so same folder as all the subfolders \"0\", \"1\", etc.) \n",
    "            there needs to be a .cfg file with parameters k, Q, omega named 'Parameters.cfg'\n",
    "        \n",
    "        So, for example, if Tau is {10,100,1000}, Run1 folder would contain:\n",
    "        Subfolder \"0\" with data from Tau = 10\n",
    "        Subfolder \"1\" with data from Tau = 100\n",
    "        Subfolder \"2\" with data from Tau = 1000\n",
    "        Parameters.cfg\n",
    "        ^It is imperative that the above naming convention is followed.\n",
    "        ^^If using my data collection script on the Cyper microscope this will naturally be the naming convention\n",
    "        that the data is saved in.\n",
    "        \n",
    "        \n",
    "        Outputs:\n",
    "        df_main = a Pandas DataFrame containing all of the z(t) data and Tau\n",
    "        #as well as k, Q, omega, and Tau for each point scan #many tips implementation\n",
    "        \"\"\"\n",
    "        \n",
    "        #taus array gives the values of tau for each Runx.  They need to be in ascending order \n",
    "        #(i.e. taus[0] = tau for Run0, taus[1] = tau for Run1, etc.)  \n",
    "        #The requirement for inputting ascending order can easily be fixed by a sort function \n",
    "        #but Taus is not a parameter I have ever changed yet so I haven't bothered with this\n",
    "        taus= np.array([10e-9, 31.62e-9, 100e-9, 316.2e-9, 1e-6, 3.162e-6, 10e-6, 31.62e-6, 100e-6, 316.2e-6, 1e-3])\n",
    "        \n",
    "        #Pre-calculate important indices\n",
    "        total_time = total_time * 1e-3\n",
    "        sample_rate = sample_rate * 1e6\n",
    "        total_points = total_time * sample_rate\n",
    "        self.trigger_index = int(pre_trigger * total_points)\n",
    "        #print('trigger index is ' + str(self.trigger_index))\n",
    "        \n",
    "        \n",
    "        #Initialize DataFrame for collecting each run\n",
    "        df_main = pd.DataFrame()\n",
    "        \n",
    "        for i in paths:\n",
    "            \n",
    "            #grab paths for upcoming data loading\n",
    "            params_path = i + \"/Parameters.cfg\"\n",
    "            tau_paths = [item for item in glob.glob(i + \"/*\") if not os.path.basename(item).startswith('Param')]\n",
    "            sorted_tau_paths = sorted(tau_paths, key = lambda x: int(os.path.basename(os.path.normpath(x))))\n",
    "            #Above line properly sorts so that they are ordered 0,1,2,3,4,5,6,7,8,9,10 rather than 0,1,10,2,3,4...\n",
    "    \n",
    "            #Initialize DataFrame for this run\n",
    "            df_run = pd.DataFrame()\n",
    "            \n",
    "            #Loop through every tau in that run and load it in\n",
    "            for j in range(4,len(tau_paths)): #CHANGE THIS RANGE IF YOU WANT TO TRAIN ON DIFFERENT TIME SCALES.  \n",
    "                                              #CURRENTLY IS TRAINING ON slower TIME SCALES\n",
    "                tau = taus[j]\n",
    "                displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "                #collect relevent parts of the data into df_run\n",
    "                for k in range(len(displacement_path)):\n",
    "                    #load displacement data from .ibw\n",
    "                    disp_array = loadibw(displacement_path[k])['wave']['wData'] \n",
    "                    #throw away all displacement before the trigger\n",
    "                    disp_array = disp_array[self.trigger_index:,:]\n",
    "                    disp_array = np.transpose(disp_array)\n",
    "        \n",
    "                    #Init columns for df\n",
    "                    columns=[]\n",
    "                    for l in range(disp_array.shape[1]):\n",
    "                        columns.append('t='+str(l))\n",
    "                    \n",
    "                    #init temporary DataFrame for appending this Tau's data into the main DataFrame for this run\n",
    "                    df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "                    \n",
    "                    #Commented out code below is for a feature still under development (implementing multiple tips)\n",
    "                    \"\"\"\n",
    "                    #load other parameters\n",
    "                    config = configparser.RawConfigParser()\n",
    "                    config.read(params_path)\n",
    "                    \n",
    "                    for (each_key,each_value) in config.items('Parameters'):\n",
    "                        setattr(self,each_key,config.getfloat('Parameters',each_key))\n",
    "                    \n",
    "                    df_temp['k'] = self.k\n",
    "                    df_temp['Q'] = self.q\n",
    "                    df_temp['omega'] = self.omega\"\"\"\n",
    "                    \n",
    "                    df_temp['Tau'] = pd.Series(index=df_temp.index) #create Tau column\n",
    "                    df_temp['Tau'] = tau #assign tau value to tau column \n",
    "                                         #(could probably be done in above step with data=tau flag?)\n",
    "                    \n",
    "                    df_run = df_run.append(df_temp,ignore_index=True) #append each tau value to this run          \n",
    "            \n",
    "            df_main = pd.concat([df_main,df_run], ignore_index=True) #append each run to the final collection DataFrame\n",
    "            \n",
    "        return df_main\n",
    "    \n",
    "        \n",
    "    \n",
    "    def preprocess_train_data(self,train_data, val_split = 0.2):\n",
    "        \"\"\"This function takes the raw training data (simulated or experimental) and prepares it \n",
    "        for the machine learning input.\n",
    "        It normalizes the data about zero.\n",
    "        It also separates training and validation data and preprocesses them separately \n",
    "        (training data first, then validation data with training data's settings to prevent data leakage).\n",
    "        \n",
    "        Inputs: \n",
    "        train_data = raw training data from load_experimental_train_data() or load_simulated_train_data()\n",
    "        val_split = fraction of input training data to randomly select and split off as validation data\n",
    "                    default val_split = 0.2\n",
    "        \n",
    "        Outputs:\n",
    "        train_x1_norm_reshaped = training x data (everything except the known, true Tau value)\n",
    "        val_x1_norm_reshaped = validation x data (everything except the known, true Tau value)\n",
    "        train_one_hot_tau = training data's known true Tau values in a one hot encoding\n",
    "        val_one_hot_tau = validation data's known true Tau values in a one hot encoding\n",
    "        \"\"\"\n",
    "        \n",
    "        #shuffle the training data since I load data sequentially.\n",
    "        #Shuffling makes the validation set a true representation of the data.\n",
    "        train_data = sklearn.utils.shuffle(train_data, random_state = 7)\n",
    "        \n",
    "        \n",
    "        #Split off the validation data\n",
    "        num_val_samples = int(val_split * len(train_data.index)) #calculate how many validation samples there are\n",
    "        val_x = train_data.tail(num_val_samples) #select the validation data from the total training pool\n",
    "        train_x = train_data[:int(-1*num_val_samples)] #select only the training split from the total training pool\n",
    "        \n",
    "        \n",
    "        #Grab the y labels\n",
    "        train_y = np.array(train_x['Tau']) #labeled, true Tau values for the CNN to learn from\n",
    "        val_y = np.array(val_x['Tau']) #labeled, true Tau values for the CNN to learn from\n",
    "        \n",
    "        #train_x1 = train_x.drop(['k','Q','omega','Tau'],axis=1) #Many tips implementation\n",
    "        train_x1 = train_x.drop(['Tau'],axis=1) #drop y values from training and validation data\n",
    "        val_x1 = val_x.drop(['Tau'],axis=1)\n",
    "        \n",
    "        #train_x2 = train_x[['k','Q','omega']] #Many tips implementation\n",
    "        \n",
    "        \n",
    "        #Prepare for normalization\n",
    "        self.mean_train_x1 = np.mean(train_x1) #saving the mean_train_x for preprocessing the \n",
    "                                               #validation/test data in the same manner as our training data\n",
    "        #self.mean_train_x2 = np.mean(train_x2) #Many tips implementation\n",
    "        \n",
    "        self.SD_train_x1 = np.std(train_x1) #saving the SD_train_x for preprocessing the validation/test data \n",
    "                                            #in the same manner as our training data\n",
    "        #self.SD_train_x2 = np.std(train_x2) #Many tips implementation\n",
    "        \n",
    "        #normalize and centralize the training data for best neural network performance\n",
    "        train_x1_norm = (train_x1 - self.mean_train_x1) /  (self.SD_train_x1) \n",
    "        train_x1_norm_reshaped = np.expand_dims(train_x1_norm,axis=2) #formatting for input into CNN\n",
    "        \n",
    "        #normalize and centralize the training data for best neural network performance\n",
    "        val_x1_norm = (val_x1 - self.mean_train_x1) /  (self.SD_train_x1) \n",
    "        val_x1_norm_reshaped = np.expand_dims(val_x1_norm,axis=2) #formatting for input into CNN\n",
    "        \n",
    "        #Many tips implementation\n",
    "        #normalize and centralize the training data for best neural network performance\n",
    "        #train_x2_norm = (train_x2 - self.mean_train_x2) /  (self.SD_train_x2) \n",
    "        #train_x2_norm_reshaped = np.expand_dims(train_x2_norm,axis=2) #formatting for input into CNN\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        #Label encode the y-data by one hot-encoding for proper format for classification based NN:\n",
    "\n",
    "        #tau_index is used to recover the original tau's from a one-hot encoded output.\n",
    "        #e.g. tau = [10, 100, 1000, 10, 10] then\n",
    "        #unique_tau = [10, 100, 1000]\n",
    "        #tau_index = [0,1,2,0,0] is index of tau to corresponding unique_tau so\n",
    "        #unique_tau[tau_index] == tau \n",
    "        unique_tau_train, tau_index_train = np.unique(train_y,return_inverse=True)\n",
    "        unique_tau_val, tau_index_val = np.unique(val_y,return_inverse=True)\n",
    "\n",
    "        #make one-hot encoded tau vectors\n",
    "        one_hot_tau_train = np_utils.to_categorical(tau_index_train)\n",
    "        one_hot_tau_val = np_utils.to_categorical(tau_index_val)\n",
    "\n",
    "        self.number_of_classes = one_hot_tau_train.shape[1] #used to match number of output Softmax layers in my NN\n",
    "        \n",
    "        return train_x1_norm_reshaped, one_hot_tau_train, val_x1_norm_reshaped, one_hot_tau_val #,\n",
    "               # train_x2_norm_reshaped\n",
    "    \n",
    "    \n",
    "    #Many tips implementation\n",
    "    #def train_CNN(self, train_x1, train_x2, train_y, num_epochs = 40, kernel1_size = 400, \n",
    "    #kernel2_size = 100, num_filter1 = 5, num_filter2 = 3):\n",
    "    \n",
    "    def train_CNN(self, train_x1, train_y, val_x, val_y, num_epochs = 20, kernel1_size = 400, \n",
    "                  kernel2_size = 100, num_filter1 = 5, num_filter2 = 3):\n",
    "        \"\"\"Build CNN and start training!\n",
    "        \n",
    "        Inputs:\n",
    "        train_x1 = training data (train_x1_norm_reshaped output from preprocess_train_data())\n",
    "        train_y = taus from training data (one_hot_tau_train from preprocess_train_data())\n",
    "        val_x = validation data (val_x1_norm_reshaped output from preprocess_train_data())\n",
    "        val_y = taus from training data (one_hot_tau_val from preprocess_train_data())\n",
    "        num_epochs = number of epochs to train for\n",
    "        kernel1_size = kernel length for first convolutional layer [int]\n",
    "        kernel2_size = kernel length for second convolutional layer [int]\n",
    "        num_filter1 = number of convolutional filters in first convolutional layer [int]\n",
    "        num_filter2 = number of convolutional filters in second convolutional layer [int]\n",
    "        \n",
    "        Outputs:\n",
    "        history = dictionary of model fitting metrics (accuracy, top2metric, etc.)\n",
    "        \n",
    "        Note that several other important variables from this function are not returned as function outputs,\n",
    "        but ARE saved as class variables for reuse later (e.g. self.model)\n",
    "        \"\"\"\n",
    "\n",
    "        #Save class variables for later use in visualize_weights()\n",
    "        self.filter_number1 = num_filter1\n",
    "        self.filter_number2 = num_filter2\n",
    "        self.kernel1_size = kernel1_size\n",
    "        self.kernel2_size = kernel2_size\n",
    "        \n",
    "        #Save class variable for later use in plot_training_statistics()\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        \n",
    "        #Initialize CNN branch 1 for main convolutional input data (displacement or instantaneous frequency)\n",
    "        #WHILE DEBUGGING I CHANGED \"BRANCH1 to MODEL\".  \n",
    "        #Change \"model\" to \"branch1\" for returning to many tips implementation!\n",
    "        model = Sequential()\n",
    "\n",
    "        #Add convolution layers\n",
    "        model.add(Conv1D(filters=num_filter1,kernel_size=kernel1_size,strides=2,padding='same',\n",
    "                         input_shape=(train_x1.shape[1],1)))\n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "        model.add(MaxPooling1D())\n",
    "\n",
    "        model.add(Conv1D(filters=num_filter2,kernel_size=kernel2_size,strides=2,padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "        model.add(MaxPooling1D())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        #Roughly 500 units length of branch 1 \n",
    "        #500 ~ (8000 displacement points / (2**4 because each strides = 2 and each maxpool halves data length))\n",
    "        \n",
    "        #Add fully connected layers (maybe remove for many tips implementation?  \n",
    "        #                           Check notes on arcitecture experiments I did)\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal',activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal',activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        #Many tips implementation\n",
    "        \"\"\"\n",
    "        #Initialize CNN branch 2 for supplementary data (Q, k, and omega)\n",
    "        branch2 = Sequential()\n",
    "\n",
    "        #Add supplementary data inputs\n",
    "        branch2.add(Dense(units=100, kernel_initializer='he_normal', activation='linear', \n",
    "        input_shape=(train_x2.shape[1],1)))\n",
    "        branch2.add(LeakyReLU(alpha=.01))\n",
    "        branch2.add(Dropout(0.3))\n",
    "        \n",
    "        branch2.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        branch2.add(LeakyReLU(alpha=.01))\n",
    "        branch2.add(Dropout(0.4))\n",
    "        \n",
    "        branch2.add(Flatten())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Merge branches 1 and 2\n",
    "        model = Sequential()\n",
    "        model.add(Merge([branch1,branch2], mode='concat'))\n",
    "\n",
    "        \n",
    "        #Add final fully connected layers\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.4))\n",
    "        \"\"\"\n",
    "\n",
    "        #Add classification layer\n",
    "        #If attempting regression modeling instead of tau bucketing, change here\n",
    "        model.add(Dense(units=self.number_of_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "        #Compile CNN and configure metrics/learning process\n",
    "        \"\"\"below functions are failure metrics that tell me if the true tau \n",
    "        was in the top 2, top 3, or top 5 guesses made by the neural network\"\"\"\n",
    "        def inTop2(k=2):\n",
    "            def top2metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=2)\n",
    "            return top2metric\n",
    "        \n",
    "        def inTop3(k=3):\n",
    "            def top3metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=3)\n",
    "            return top3metric\n",
    "        \n",
    "        def inTop5(k=5):\n",
    "            def top5metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=5)\n",
    "            return top5metric\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy', inTop2(), inTop3()])\n",
    "\n",
    "        #Prepare for visualization if using TensorBoard\n",
    "        #tbCallBack = callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "        #tbCallBack = callbacks.TensorBoard(log_dir=\"logs/{}\".format(time.time()))\n",
    "\n",
    "        #Train model\n",
    "        #Many tips implementation\n",
    "        #model.fit([train_x1, train_x2], train_y, batch_size=32, \n",
    "        #epochs=num_epochs,verbose=2, validation_split=0.05)#, callbacks=[tbCallBack])\n",
    "        history = model.fit(train_x1, train_y, batch_size=32, epochs=num_epochs,verbose=2, \n",
    "                            validation_data=(val_x,val_y))#, callbacks=[tbCallBack])\n",
    "        self.model = model #save model to self for calling from other functions later\n",
    "        \n",
    "        \n",
    "        #Save branches as class variables for visualize_weights()\n",
    "        self.branch1 = model\n",
    "        #Many tips implementation -- this has not been debugged and may be inaccurate\n",
    "        #self.branch1 = branch1\n",
    "        #self.branch2 = branch2\n",
    "        #self.model = model\n",
    "        return history\n",
    "    \n",
    "    \n",
    "    def plot_training_statistics(self, history):\n",
    "        \"\"\"Plot training data accuracy and validation data accuracy as a function of training epoch number\n",
    "        \n",
    "        Inputs: history (dictionary of model fitting metrics--it is the output variable from train_CNN())\n",
    "        Outputs: inline figure showing training vs. validation metrics of loss, accuracy, and Top2metric \"\"\"\n",
    "        \n",
    "        num_epochs = self.num_epochs\n",
    "\n",
    "        #extract relevent information from history\n",
    "        train_loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        train_acc = model.history['acc']\n",
    "        val_acc = model.history['val_acc']\n",
    "        \n",
    "        train_top_2 = model.history['top2metric']\n",
    "        val_top_2 = model.history['val_top2metric']\n",
    "\n",
    "        #x axis is num_epochs\n",
    "        x = range(1,self.num_epochs + 1)\n",
    "        xticks = range(1,self.num_epochs + 1)\n",
    "\n",
    "        #Set up plotting\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(nrows = 3)\n",
    "\n",
    "        ax1.plot(x, train_loss, label = 'Training')\n",
    "        ax1.plot(x, val_loss, label = 'Validation')\n",
    "        ax1.legend(loc = 'upper right')\n",
    "        #below if/else statement is to keep the x-axis formatted nicely regardless of number of epochs\n",
    "        if self.num_epochs < 6:\n",
    "            ax1.set_xticks(xticks)\n",
    "        else:\n",
    "            ax1.xaxis.set_major_locator(MaxNLocator(integer=True,steps=[1,2,5,10]))\n",
    "        ax1.set_ylabel('Loss')\n",
    "        \n",
    "\n",
    "        ax2.plot(x, train_acc, label = 'Training')\n",
    "        ax2.plot(x, val_acc, label = 'Validation')\n",
    "        ax2.legend(loc = 'lower right')\n",
    "        #below if/else statement is to keep the x-axis formatted nicely regardless of number of epochs\n",
    "        if self.num_epochs < 6:\n",
    "            ax2.set_xticks(xticks)\n",
    "        else:\n",
    "            ax2.xaxis.set_major_locator(MaxNLocator(integer=True,steps=[1,2,5,10]))\n",
    "        ax2.set_xlabel('Number of Epochs')\n",
    "        ax2.set_ylim(top=1)\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        \n",
    "        \n",
    "        ax3.plot(x, train_top_2, label = 'Training')\n",
    "        ax3.plot(x, val_top_2, label = 'Validation')\n",
    "        ax3.legend(loc = 'lower right')\n",
    "        #below if/else statement is to keep the x-axis formatted nicely regardless of number of epochs\n",
    "        if self.num_epochs < 6:\n",
    "            ax3.set_xticks(xticks)\n",
    "        else:\n",
    "            ax3.xaxis.set_major_locator(MaxNLocator(integer=True,steps=[1,2,5,10]))\n",
    "        ax3.set_xlabel('Number of Epochs')\n",
    "        ax3.set_ylim(top=1)\n",
    "        ax3.set_ylabel('Top-2 Accuracy')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_experimental_test_data(self, path):\n",
    "        \"\"\"This function loads test data and preprocesses it in the same way as the training data.\n",
    "        Note that the preprocessing uses parameters from the training data on the test data to prevent data leakage.\n",
    "        \n",
    "        Inputs:\n",
    "        path = string to file path folder.  Please see documentation load_experimental_train_data() for important\n",
    "                                            notes about this variable!\n",
    "                                            \n",
    "    \n",
    "        Outputs:\n",
    "        test_x1_norm_reshaped = preprocessed test x data ready for evaluation\n",
    "        one_hot_tau = test y data's corresponding one hot labels\n",
    "        \n",
    "        The outputs are properly formatted to be fed into model.evaluate()\n",
    "    \n",
    "        JAKE NOTE this function requires training functions to have been run (mean_train_x and SD_train_x must exist!!!)\n",
    "        This means for production, mean_train_x and SD_train_x must be saved variables \n",
    "        that I import along with my trained NN\"\"\"\n",
    "        \n",
    "        #taus = np.array([10e-9, 25e-9, 50e-9, 100e-9, 250e-9, 500e-9,\n",
    "        #                        1e-6, 5e-6, 10e-6, 100e-6, 1e-3])\n",
    "        taus= np.array([10e-9, 31.62e-9, 100e-9, 316.2e-9, 1e-6, 3.162e-6, 10e-6, 31.62e-6, 100e-6, 316.2e-6, 1e-3])\n",
    "        \n",
    "        params_path = path + \"/Parameters.cfg\"\n",
    "        tau_paths = [item for item in glob.glob(path + \"/*\") if not os.path.basename(item).startswith('Param')]\n",
    "        sorted_tau_paths = sorted(tau_paths, key = lambda x: int(os.path.basename(os.path.normpath(x))))\n",
    "        #Above line properly sorts so that they are ordered 0,1,2,3,4,5,6,7,8,9,10 rather than 0,1,10,2,3,4...\n",
    "        \n",
    "        #Init dataframe for this run.  This DataFrame will collect each array of data from each folder in for loop below\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        for j in range(4,len(tau_paths)):\n",
    "            tau = taus[j]\n",
    "            displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "            disp_array = loadibw(displacement_path[0])['wave']['wData'] #load displacement from .ibw file into python\n",
    "            disp_array = disp_array[self.trigger_index:,:] #throw away all displacement before the trigger\n",
    "            disp_array = np.transpose(disp_array)\n",
    "        \n",
    "            #Put loaded stuff into dataframe and label tau\n",
    "            columns=[]\n",
    "            for k in range(disp_array.shape[1]):\n",
    "                columns.append('t='+str(k))\n",
    "        \n",
    "            df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "            \n",
    "            #Many tips implementation\n",
    "            \"\"\"\n",
    "            #load other parameters\n",
    "            config = configparser.RawConfigParser()\n",
    "            config.read(params_path)\n",
    "                    \n",
    "            #Get k, Q, omega from .txt\n",
    "            for (each_key,each_value) in config.items('Parameters'):\n",
    "                setattr(self,each_key,config.getfloat('Parameters',each_key))\n",
    "                    \n",
    "            #JAKE INSERT df_temp['k'] = imported k value, etc for Q, omega\n",
    "            df_temp['k'] = self.k\n",
    "            df_temp['Q'] = self.q\n",
    "            df_temp['omega'] = self.omega\n",
    "            \"\"\"\n",
    "                \n",
    "            df_temp['Tau'] = pd.Series(index=df_temp.index)\n",
    "            df_temp['Tau'] = tau\n",
    "            df = df.append(df_temp,ignore_index=True)\n",
    "        \n",
    "     \n",
    "        test_y = np.array(df['Tau'])\n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "        unique_tau, tau_index = np.unique(test_y,return_inverse=True)\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "    \n",
    "        #preprocess test_x\n",
    "        #test_x1 = df.drop(['k','Q','omega','Tau'],axis=1) #x1 is just displacement #Many tips implementation\n",
    "        test_x1 = df.drop(['Tau'],axis=1) #x1 is just displacement\n",
    "        #test_x2 = df[['k','Q','omega']]  #Many tips implementation\n",
    "        \n",
    "        #important to preprocess my test data same as my train data to prevent data leakage!!!!\n",
    "        test_x1_norm = (test_x1 - self.mean_train_x1 ) /  (self.SD_train_x1) \n",
    "        #test_x2_norm = (test_x2 - self.mean_train_x2 ) /  (self.SD_train_x2)\n",
    "        \n",
    "        test_x1_norm_reshaped = np.expand_dims(test_x1_norm,axis=2)\n",
    "        #test_x2_norm_reshaped = np.expand_dims(test_x2_norm,axis=2) #Many tips implementation\n",
    "    \n",
    "        return test_x1_norm_reshaped, one_hot_tau #test_x2_norm_reshaped, one_hot_tau\n",
    "    \n",
    "    \n",
    "    #Many tips implementation\n",
    "    #def test_closeness(self, test_x1, test_x2, test_y):\n",
    "    def test_closeness(self, test_x1, test_y):\n",
    "        \"\"\"This function looks at the predicted tau values from model.predict(test_x) and compares them \n",
    "        to the true tau values from test_y.  \n",
    "        It then returns three values telling you what percentage of the incorrect predictions varied by spacing of\n",
    "        one tau value, two tau values, or three tau values.\n",
    "        \n",
    "        E.G.\n",
    "        Say possible taus = [1,2,3,4,5,6,7,8,9]\n",
    "        model.predict(test_x) = [2,2,2,3,3,3,4,4,5,6]\n",
    "        test_y = [2,2,2,2,2,2,2,2,2,2]\n",
    "        \n",
    "        test_closeness returns [0.3,0.2,0.1]\n",
    "        because 30% of the predictions varied by one tau value (tau = 2 but 3 times it guessed tau = 3)\n",
    "        because 20% of the predictions varied by two tau values (tau = 2 but 2 times it guessed tau = 4)\n",
    "        because 10% of the predictions varied by three tau values (tau = 2 but 1 time it guessed tau = 5)\n",
    "        \"\"\"\n",
    "        \n",
    "        #pred_tau = self.model.predict([test_x1,test_x2],verbose=0) #Many tips implementation\n",
    "        pred_tau = self.model.predict(test_x1,verbose=0)\n",
    "        \n",
    "        pred_tau_am = pred_tau.argmax(axis=-1) #pluck out actual prediction value!\n",
    "        test_y_am = test_y.argmax(axis=-1) #does not actually need the argmax function, \n",
    "                                           #but this makes it same format as pred_tau_am which IS necessary\n",
    "        \n",
    "        incorrect_indices = np.nonzero(pred_tau_am != test_y_am) #indices of incorrect predictions\n",
    "        \n",
    "        total_samples = len(pred_tau)\n",
    "        total_fails = len(incorrect_indices[0])\n",
    "        \n",
    "        #init diff collection variables (how many tau values away the true value was from the predicted value)\n",
    "        num_diff_1 = 0 #1 value away from true\n",
    "        num_diff_2 = 0 #2 values away from true\n",
    "        num_diff_3 = 0 #3 values away from true\n",
    "        num_greater = 0 #>3 values away from true\n",
    "        \n",
    "        #init array for seeing which taus it is bad at predicting\n",
    "        which_taus_failed = np.zeros(self.number_of_classes)\n",
    "        \n",
    "        for element in incorrect_indices[0]:\n",
    "            \n",
    "            #collect diff (how many tau values away the true value was from the predicted value)\n",
    "            diff = abs(pred_tau_am[element] - test_y_am[element])\n",
    "            if diff == 1:\n",
    "                num_diff_1 += 1\n",
    "            elif diff == 2:\n",
    "                num_diff_2 += 1\n",
    "            elif diff == 3:\n",
    "                num_diff_3 += 1\n",
    "            else:\n",
    "                num_greater += 1\n",
    "            \n",
    "            #collect how many of each tau failed\n",
    "            i=0\n",
    "            while True:\n",
    "                if test_y_am[element] == i:\n",
    "                    which_taus_failed[i] += 1\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "                    \n",
    "            #which_taus_failed_percent = np.round((which_taus_failed / total_fails),4) * 100\n",
    "\n",
    "        \n",
    "        percent_num_diff_1 = round((num_diff_1 / total_samples), 4) * 100\n",
    "        percent_num_diff_2 = round((num_diff_2 / total_samples), 4) * 100\n",
    "        percent_num_diff_3 = round((num_diff_3 / total_samples), 4) * 100\n",
    "        percent_num_diff_greater = round((num_greater / total_samples), 4) * 100\n",
    "            \n",
    "        #Next section is for debugging purposes\n",
    "        #percent_incorrect = (len(incorrect_indices[0])/total_samples)\n",
    "        #percent_incorrect_calculated = percent_num_diff_1 + percent_num_diff_2 + \n",
    "        #percent_num_diff_3 + percent_num_diff_greater\n",
    "        #print('percent incorrect should be ' + str(percent_incorrect))\n",
    "        #print('percent incorrect calculated is ' + str(percent_incorrect_calculated))\n",
    "        \n",
    "        return percent_num_diff_1, percent_num_diff_2, percent_num_diff_3, which_taus_failed#_percent\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def test_experimental_CNN(self, *paths):\n",
    "        \n",
    "        score_string = 'data order is testing against '\n",
    "        for element in paths:\n",
    "            score_string += (str(element) + \" , \\n\") \n",
    "\n",
    "        \n",
    "        score_collect = [score_string]\n",
    "        score_collect.append('column order is loss, accuracy, top2metric, top3metric')\n",
    "        score_collect.append('top2metric = % that the true tau was one of the top 2 predictions')\n",
    "        score_collect.append(' ')\n",
    "        \n",
    "        for element in paths:\n",
    "            #test_x1, test_x2, test_y = self.load_experimental_test_data(element) #Many tips implementation\n",
    "            test_x1, test_y = self.load_experimental_test_data(element)\n",
    "            #score = self.model.evaluate([test_x1,test_x2],test_y,batch_size = 32) #Many tips implementation\n",
    "            score = self.model.evaluate(test_x1,test_y,batch_size = 32)\n",
    "            percentage = str(round(score[1],5) * 100)\n",
    "            print('model scored ' + percentage + '% on ' + str(element))\n",
    "            score_collect.append(str(score))\n",
    "            \n",
    "            \n",
    "            #error1, error2, error3, which_taus_failed = self.test_closeness(test_x1,test_x2,test_y) #Many tips implementation\n",
    "            error1, error2, error3, which_taus_failed = self.test_closeness(test_x1,test_y)\n",
    "            score_collect.append('one_diff_error = ' + str(error1))\n",
    "            score_collect.append('two_diff_error = ' + str(error2))\n",
    "            score_collect.append('three_diff_error = ' + str(error3))\n",
    "            score_collect.append('which taus failed were: ' + str(which_taus_failed))\n",
    "            print('one_diff_error = ' + str(error1))\n",
    "            print('two_diff_error = ' + str(error2))\n",
    "            print('three_diff_error = ' + str(error3))\n",
    "            print('which taus failed were: ' + str(which_taus_failed))\n",
    "            print(' ')\n",
    "            \n",
    "            score_collect.append('above scores were for ' + str(element))\n",
    "            score_collect.append(' ')\n",
    "            \n",
    "        \n",
    "        self.score_collect = score_collect\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def save_CNN(self, save_str):\n",
    "        #save model and test evaluation outputs\n",
    "        #example save_str: save_str = 'displacement_10us_random_noise_10_2018_06_13_80epoch'\n",
    "        #requires test_CNN to have been run already\n",
    "        path = 'C:/Users/jakeprecht/DDHO/saved CNN models/'\n",
    "        save_str_h5 = path + save_str + '.h5'\n",
    "        save_str_txt = path + save_str + '_results.txt'\n",
    "        save_str_weights = path + save_str + '_weights.h5'\n",
    "        \n",
    "        self.model.save(save_str_h5)  # creates a HDF5 file 'save_str_h5.h5'\n",
    "        self.model.save_weights(save_str_weights)\n",
    "        \n",
    "        output_scores = open(save_str_txt, 'w')\n",
    "        for item in self.score_collect:\n",
    "            output_scores.write(\"%s\\n\" % item)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def visualize_weights(self, layer_number):\n",
    "        \"\"\"This function plots the weights at a given layer of the neural network\n",
    "        It is useful for 'un-blackboxing' the neural network and seeing where different layers of the network focus\"\"\"\n",
    "        #layer number 0 = conv layer 1\n",
    "        #layer number 1 = ReLU 1\"\"\"\n",
    "        weights, biases = self.branch1.layers[layer_number].get_weights()\n",
    "        \n",
    "        if layer_number == 0 or 1:\n",
    "            number_filters = self.filter_number1\n",
    "            kernel_length = self.kernel1_size         \n",
    "\n",
    "        #elif layer_number == 3:\n",
    "        #    number_filters = self.filter_number2\n",
    "        #    kernel_length = self.kernel2_size\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Input for layer_number must be 0 or 1 in current implementation (2019_04_30)\")\n",
    "            \n",
    "        fig = plt.figure()\n",
    "        for i in range(number_filters):\n",
    "            weight_plt = weights[:,:,i]\n",
    "            weight_plt2 = weight_plt.reshape((kernel_length,))\n",
    "            #ax = fig.add_subplot(number_filters,1,i+1)\n",
    "            plt.figure()\n",
    "            plt.plot(weight_plt2)\n",
    "            #ax.imshow(weight_plt2,cmap='gray')\n",
    "            \n",
    "        return\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100800 samples, validate on 25200 samples\n",
      "Epoch 1/20\n",
      " - 2160s - loss: 0.4352 - acc: 0.8114 - top2metric: 0.9539 - top3metric: 0.9835 - val_loss: 0.2371 - val_acc: 0.9024 - val_top2metric: 0.9929 - val_top3metric: 0.9998\n",
      "Epoch 2/20\n",
      " - 2155s - loss: 0.2712 - acc: 0.8929 - top2metric: 0.9899 - top3metric: 0.9995 - val_loss: 0.2236 - val_acc: 0.9131 - val_top2metric: 0.9930 - val_top3metric: 0.9998\n",
      "Epoch 3/20\n",
      " - 2155s - loss: 0.2380 - acc: 0.9080 - top2metric: 0.9922 - top3metric: 0.9997 - val_loss: 0.2056 - val_acc: 0.9169 - val_top2metric: 0.9946 - val_top3metric: 0.9997\n",
      "Epoch 4/20\n",
      " - 2156s - loss: 0.2204 - acc: 0.9158 - top2metric: 0.9933 - top3metric: 0.9998 - val_loss: 0.2165 - val_acc: 0.9192 - val_top2metric: 0.9940 - val_top3metric: 0.9998\n",
      "Epoch 5/20\n",
      " - 2155s - loss: 0.2013 - acc: 0.9233 - top2metric: 0.9942 - top3metric: 0.9997 - val_loss: 0.1809 - val_acc: 0.9299 - val_top2metric: 0.9954 - val_top3metric: 0.9998\n",
      "Epoch 6/20\n",
      " - 2155s - loss: 0.1920 - acc: 0.9277 - top2metric: 0.9949 - top3metric: 0.9998 - val_loss: 0.1806 - val_acc: 0.9326 - val_top2metric: 0.9958 - val_top3metric: 0.9998\n",
      "Epoch 7/20\n",
      " - 2155s - loss: 0.1831 - acc: 0.9302 - top2metric: 0.9956 - top3metric: 0.9999 - val_loss: 0.1852 - val_acc: 0.9229 - val_top2metric: 0.9959 - val_top3metric: 0.9998\n",
      "Epoch 8/20\n",
      " - 2155s - loss: 0.1814 - acc: 0.9326 - top2metric: 0.9954 - top3metric: 0.9999 - val_loss: 0.1778 - val_acc: 0.9314 - val_top2metric: 0.9962 - val_top3metric: 0.9998\n",
      "Epoch 9/20\n",
      " - 2155s - loss: 0.1776 - acc: 0.9337 - top2metric: 0.9959 - top3metric: 0.9998 - val_loss: 0.1949 - val_acc: 0.9316 - val_top2metric: 0.9968 - val_top3metric: 0.9999\n",
      "Epoch 10/20\n",
      " - 2155s - loss: 0.1670 - acc: 0.9375 - top2metric: 0.9963 - top3metric: 0.9999 - val_loss: 0.1527 - val_acc: 0.9419 - val_top2metric: 0.9960 - val_top3metric: 0.9999\n",
      "Epoch 11/20\n",
      " - 2155s - loss: 0.1704 - acc: 0.9368 - top2metric: 0.9959 - top3metric: 0.9999 - val_loss: 0.1570 - val_acc: 0.9409 - val_top2metric: 0.9966 - val_top3metric: 0.9999\n",
      "Epoch 12/20\n",
      " - 2155s - loss: 0.1677 - acc: 0.9382 - top2metric: 0.9963 - top3metric: 0.9999 - val_loss: 0.1736 - val_acc: 0.9326 - val_top2metric: 0.9960 - val_top3metric: 0.9999\n",
      "Epoch 13/20\n",
      " - 2155s - loss: 0.1648 - acc: 0.9396 - top2metric: 0.9964 - top3metric: 0.9998 - val_loss: 0.1526 - val_acc: 0.9412 - val_top2metric: 0.9968 - val_top3metric: 0.9999\n",
      "Epoch 14/20\n",
      " - 2155s - loss: 0.1661 - acc: 0.9399 - top2metric: 0.9962 - top3metric: 0.9999 - val_loss: 0.1780 - val_acc: 0.9346 - val_top2metric: 0.9969 - val_top3metric: 0.9999\n",
      "Epoch 15/20\n",
      " - 2155s - loss: 0.1554 - acc: 0.9427 - top2metric: 0.9966 - top3metric: 0.9999 - val_loss: 0.1579 - val_acc: 0.9383 - val_top2metric: 0.9972 - val_top3metric: 0.9999\n",
      "Epoch 16/20\n",
      " - 2155s - loss: 0.1621 - acc: 0.9401 - top2metric: 0.9964 - top3metric: 0.9998 - val_loss: 0.1436 - val_acc: 0.9466 - val_top2metric: 0.9974 - val_top3metric: 0.9999\n",
      "Epoch 17/20\n",
      " - 2155s - loss: 0.1675 - acc: 0.9404 - top2metric: 0.9961 - top3metric: 0.9997 - val_loss: 0.1553 - val_acc: 0.9407 - val_top2metric: 0.9972 - val_top3metric: 0.9999\n",
      "Epoch 18/20\n",
      " - 2155s - loss: 0.1562 - acc: 0.9437 - top2metric: 0.9967 - top3metric: 0.9999 - val_loss: 0.1579 - val_acc: 0.9419 - val_top2metric: 0.9965 - val_top3metric: 0.9998\n",
      "Epoch 19/20\n",
      " - 2155s - loss: 0.1593 - acc: 0.9421 - top2metric: 0.9967 - top3metric: 0.9999 - val_loss: 0.1655 - val_acc: 0.9388 - val_top2metric: 0.9961 - val_top3metric: 0.9999\n",
      "Epoch 20/20\n",
      " - 2155s - loss: 0.1505 - acc: 0.9460 - top2metric: 0.9969 - top3metric: 0.9999 - val_loss: 0.1698 - val_acc: 0.9388 - val_top2metric: 0.9965 - val_top3metric: 0.9999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x62b34f5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000/14000 [==============================] - 125s 9ms/step\n",
      "model scored 97.179% on G:/2018_11_02 PLL on gold/tip8/mono/Run1\n",
      "one_diff_error = 2.71\n",
      "two_diff_error = 0.11\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [167. 117.  66.  41.   4.   0.   0.]\n",
      " \n",
      "14000/14000 [==============================] - 124s 9ms/step\n",
      "model scored 93.429% on G:/2018_11_02 PLL on gold/tip8/mono/Run2\n",
      "one_diff_error = 6.510000000000001\n",
      "two_diff_error = 0.06\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [349. 443.  76.  47.   5.   0.   0.]\n",
      " \n",
      "14000/14000 [==============================] - 123s 9ms/step\n",
      "model scored 94.614% on G:/2018_11_02 PLL on gold/tip8/mono/Run3\n",
      "one_diff_error = 5.33\n",
      "two_diff_error = 0.06\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [266. 348.  80.  53.   7.   0.   0.]\n",
      " \n",
      "14000/14000 [==============================] - 124s 9ms/step\n",
      "model scored 94.943% on G:/2018_11_02 PLL on gold/tip8/mono/Run4\n",
      "one_diff_error = 4.99\n",
      "two_diff_error = 0.06\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [233. 350.  93.  32.   0.   0.   0.]\n",
      " \n",
      "14000/14000 [==============================] - 125s 9ms/step\n",
      "model scored 95.686% on G:/2018_11_02 PLL on gold/tip8/mono/Run6\n",
      "one_diff_error = 4.29\n",
      "two_diff_error = 0.02\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [128. 335.  91.  44.   6.   0.   0.]\n",
      " \n",
      "14000/14000 [==============================] - 126s 9ms/step\n",
      "model scored 95.536% on G:/2018_11_02 PLL on gold/tip8/mono/Run7\n",
      "one_diff_error = 4.41\n",
      "two_diff_error = 0.05\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [ 96. 376. 106.  39.   8.   0.   0.]\n",
      " \n",
      "14000/14000 [==============================] - 126s 9ms/step\n",
      "model scored 95.364% on G:/2018_11_02 PLL on gold/tip8/mono/Run8\n",
      "one_diff_error = 4.6\n",
      "two_diff_error = 0.04\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [ 94. 423.  81.  44.   3.   4.   0.]\n",
      " \n",
      "14000/14000 [==============================] - 127s 9ms/step\n",
      "model scored 95.757% on G:/2018_11_02 PLL on gold/tip8/mono/Run9\n",
      "one_diff_error = 4.2\n",
      "two_diff_error = 0.04\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [ 97. 357.  97.  38.   4.   1.   0.]\n",
      " \n",
      "14000/14000 [==============================] - 128s 9ms/step\n",
      "model scored 95.514% on G:/2018_11_02 PLL on gold/tip8/mono/Run10\n",
      "one_diff_error = 4.46\n",
      "two_diff_error = 0.03\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [ 75. 396. 114.  38.   5.   0.   0.]\n",
      " \n",
      "14000/14000 [==============================] - 129s 9ms/step\n",
      "model scored 93.07900000000001% on G:/2018_11_02 PLL on gold/tip8/mono/Run5\n",
      "one_diff_error = 6.79\n",
      "two_diff_error = 0.13999999999999999\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [366. 409. 133.  57.   3.   1.   0.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#init CNN class\n",
    "test = CNN()\n",
    "\n",
    "#load training data\n",
    "train_data = test.load_experimental_train_data(\"G:/2018_11_02 PLL on gold/tip8/mono/Run1\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run2\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run3\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run4\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run6\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run7\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run8\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run9\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run10\")\n",
    "\n",
    "#preprocess training data and split off validation data\n",
    "train_x1, train_y, val_x, val_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "#train the model\n",
    "model = test.train_CNN(train_x1, train_y, val_x, val_y, num_epochs = 20, kernel1_size = 400, \n",
    "                       kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model trained successfully!  Let's visualize some of the training statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.plot_training_statistics(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some insights into what exactly the neural network is learning to differentiate the data with.  The next function will plot the weights in the first convolution layer so we can see what patterns in the data the first convolution layer focuses on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.vizualize_weights(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test the CNN on both data it has not seen before (Run 5) as well as all of the training data it has already seen.  This way we can compare results and see if overfitting is a concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.test_experimental_CNN(\"G:/2018_11_02 PLL on gold/tip8/mono/Run1\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run2\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run3\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run4\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run6\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run7\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run8\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run9\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run10\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save all these results so I have a log of this experiment if I need to reference it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.save_CNN('2018_11_02 tip 8 train 1 thru 10 except 5 test 1 thru 10 KL 400 100 epochs 20 slow time scale only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell contains all the functions for the simulated data.\n",
    "I removed them from the main CNN_train() class to reduce clutter.\n",
    "They are mostly functional and can probably be utilized again with minimal changes by reinserting them into CNN_train()\"\"\"\n",
    "\n",
    "    def test_simulated_CNN(self, *paths):\n",
    "        \n",
    "        score_string = 'data order is testing against '\n",
    "        for element in paths:\n",
    "            score_string += (str(element) + \" , \\n\") \n",
    "\n",
    "        \n",
    "        score_collect = [score_string]\n",
    "        score_collect.append('column order is loss, accuracy, top2metric, top3metric')\n",
    "        score_collect.append('top2metric = % that the true tau was one of the top 2 predictions')\n",
    "        score_collect.append(' ')\n",
    "        \n",
    "        \n",
    "        #score_collect = ['data order is no_noise, 0pt1noise, 1noise, random_noise_1, random_noise_10']\n",
    "        #score_collect.append('first column is loss, second column is accuracy')\n",
    "        \n",
    "        for i in paths:\n",
    "            test_x1, test_x2, test_y = self.load_simulated_test_data(i)#,test_fraction)\n",
    "            score = self.model.evaluate([test_x1,test_x2],test_y, batch_size=32)\n",
    "            percentage = str(round(score[1],5) * 100)\n",
    "            print('model scored ' + percentage + '% on ' + str(i))\n",
    "            score_collect.append(str(score))\n",
    "            \n",
    "            error1, error2, error3, which_taus_failed = self.test_closeness(test_x1,test_x2,test_y)\n",
    "            score_collect.append('one_diff_error = ' + str(error1))\n",
    "            score_collect.append('two_diff_error = ' + str(error2))\n",
    "            score_collect.append('three_diff_error = ' + str(error3))\n",
    "            score_collect.append('which taus failed were: ' + str(which_taus_failed))\n",
    "            print('one_diff_error = ' + str(error1))\n",
    "            print('two_diff_error = ' + str(error2))\n",
    "            print('three_diff_error = ' + str(error3))\n",
    "            print('which taus failed were: ' + str(which_taus_failed))\n",
    "            print(' ')\n",
    "            \n",
    "            score_collect.append('above scores were for ' + str(element)) #new code on 7/2/18\n",
    "            score_collect.append(' ')\n",
    "        \n",
    "        self.score_collect = score_collect\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def load_simulated_test_data(self, file_path):#, test_fraction):\n",
    "        \"\"\"input string with file path (e.g. \"D:/jake/DDHO Data/inst_freq/25us/0pt1noise/*.npy\" )\n",
    "        and also input the fraction of the data you wish to test (e.g. testing 10% of data would be 0.1)\n",
    "    \n",
    "        outputs the requested percentage of the data (test_data_norm_reshaped) and their corresponding labels for evaluation (one_hot_tau)\n",
    "        these outputs are basically test_x and test_y that are formatted to be fed into model.evaluate()\n",
    "    \n",
    "        NOTE this function requires previous cells to have been run (train_x must exist!!!)\"\"\"\n",
    "        #This function is currently in \"many tips implementation\" form\n",
    "        \n",
    "        file_path1 = glob.glob(file_path)\n",
    "        test_pixel = np.load(file_path1[0])\n",
    "        #this will be used in for loop below\n",
    "        #it is just used to grab the length of any one inst. freq. curve\n",
    "    \n",
    "        columns2=[]\n",
    "        for i in range(len(test_pixel)-4):#-4 because k,Q,omega, tau is at end of pixel data\n",
    "        #for i in range(len(test_pixel)-2):#-2 because tfp and tau are at end of pixel data\n",
    "            columns2.append('t='+str(i))#most columns are just time points and I'm making them here\n",
    "    \n",
    "        #columns2.append('Tfp') #because tfps are appended onto the end of my inst_freq data\n",
    "        columns2.append('k')\n",
    "        columns2.append('Q')\n",
    "        columns2.append('omega')\n",
    "        columns2.append('Tau') #because taus are appended onto the end of my displacement data\n",
    "    \n",
    "        #alternate way to load only a fraction of the data to save memory and time\n",
    "        #num_samples = len(file_path1)\n",
    "        #num_buckets = self.number_of_classes\n",
    "        #bucket_range = int(num_samples/num_buckets)\n",
    "        #test_fraction_range = int(test_fraction * bucket_range)\n",
    "        #load_list = []\n",
    "\n",
    "        #for i in range(num_buckets):\n",
    "        #    for j in range(test_fraction_range):\n",
    "        #        load_list.append(int((i * bucket_range) + (j)))\n",
    "    \n",
    "        #data1 = [np.load(file_path1[i]) for i in load_list]\n",
    "        data1 = [np.load(file_path1[i]) for i in range(len(file_path1))]\n",
    "\n",
    "        #make df for output\n",
    "        test_data = pd.DataFrame(data=data1,columns=columns2,dtype=np.float64)\n",
    "        test_data = test_data.drop('t=0',axis=1)\n",
    "        #these t=0 points end up as just NaNs and are useless anyways because by definition freq shift is 0 at trigger\n",
    "\n",
    "        test_y = np.array(test_data['Tau'])\n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "        unique_tau, tau_index = np.unique(test_y,return_inverse=True)\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "    \n",
    "        #preprocess test_x\n",
    "        test_x1 = test_data.drop(['k','Q','omega','Tau'],axis=1) #x1 is just displacement\n",
    "        test_x2 = test_data[['k','Q','omega']]\n",
    "        \n",
    "        test_x1_norm = (test_x1 - self.mean_train_x1 ) /  (self.SD_train_x1) #important to preprocess my test data same as my train data!!!!\n",
    "        test_x2_norm = (test_x2 - self.mean_train_x2 ) /  (self.SD_train_x2) #important to preprocess my test data same as my train data!!!!\n",
    "        \n",
    "        test_x1_norm_reshaped = np.expand_dims(test_x1_norm,axis=2)\n",
    "        test_x2_norm_reshaped = np.expand_dims(test_x2_norm,axis=2)\n",
    "        \n",
    "        \n",
    "        return test_x1_norm_reshaped, test_x2_norm_reshaped, one_hot_tau\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        def load_simulated_train_data(self, *paths):\n",
    "        \"\"\"This function loads training data generated by my Python simulations \n",
    "        (use Biexp.py or generate_train_data.py to generate simulated data)\n",
    "        \n",
    "        Inputs:\n",
    "        *paths = e.g. (\"D:/jake/DDHO Data/displacement/10us/random_noise_10/*.npy\",\n",
    "        \"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\")\n",
    "        where each path is to all of the .npy files in those folders\n",
    "        ^ the .npy files are generated using my Biexp.py simulation code \n",
    "        (or a function utilizing Biexp.py such as generate_train_data.py)\n",
    "        \n",
    "        Outputs:\n",
    "        df_main = a Pandas DataFrame containing all of the z(t) data #as well as k, Q, omega, and Tau for each simulation\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialize DataFrame for collecting each run\n",
    "        df_main = pd.DataFrame()\n",
    "        \n",
    "        for i in paths:\n",
    "            \n",
    "            #grab a list of strings to every file path from the inputted folders\n",
    "            file_path = glob.glob(i)\n",
    "        \n",
    "            test_pixel = np.load(file_path[0])\n",
    "            #this will be used in for loop below to grab the length of any one simulated displacement curve\n",
    "        \n",
    "            #initialize DataFrame column names list (name for each feature point of NN)\n",
    "            columns=[]\n",
    "            for j in range(len(test_pixel)-4):#-4 because k,Q,omega,tau is at end of simulated pixel data\n",
    "                columns.append('t='+str(j))   #most columns are just time points and I'm making them here\n",
    "    \n",
    "            #name other columns that are not just time points (e.g. k, Q, omega)\n",
    "            #columns.append('Tfp') #because tfps are appended onto the 2nd to last column of my inst_freq data.  Uncomment for Inst_freq data!\n",
    "            columns.append('k')\n",
    "            columns.append('Q')\n",
    "            columns.append('omega')\n",
    "            columns.append('Tau') #because true taus are appended onto the end of my data\n",
    "    \n",
    "    \n",
    "            #load all of the data into an array for input into DataFrame\n",
    "            #each entry in the \"data1\" array is a numpy array of a displacement curve\n",
    "            data1 = [np.load(file_path[i]) for i in range(0,(len(file_path)))]\n",
    "    \n",
    "            #make df for output\n",
    "            train_data = pd.DataFrame(data=data1,columns=columns,dtype=np.float64)\n",
    "            train_data = train_data.drop('t=0',axis=1)\n",
    "            #these t=0 points end up as just NaNs after preprocessing and are useless anyways because by definition freq shift is 0 at trigger for all points\n",
    "            #dropping t=0 maybe unnecessary with displacement data?\n",
    "            \n",
    "            df_main = pd.concat([df_main,train_data], ignore_index=True) #append each run to the final collection DataFrame\n",
    "        \n",
    "        return df_main\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

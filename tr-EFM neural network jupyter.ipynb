{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakeprecht\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from igor.binarywave import load as loadibw\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Conv1D, MaxPooling1D, Flatten, Merge\n",
    "from keras.utils import np_utils\n",
    "from keras import callbacks\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class CNN_train():\n",
    "    def load_simulated_train_data(self, *paths):\n",
    "        #Example train_path: \"D:/jake/DDHO Data/displacement/10us/random_noise_10/*.npy\"\n",
    "        \n",
    "        #init df for collecting each run\n",
    "        df_main = pd.DataFrame()\n",
    "        \n",
    "        for i in paths:\n",
    "            \n",
    "            #grab a list of strings to every file path from the train_path folder\n",
    "            file_path = glob.glob(i)\n",
    "        \n",
    "            test_pixel = np.load(file_path[0])\n",
    "            #this will be used in for loop below\n",
    "            #it is just used to grab the length of any one displacement curve\n",
    "        \n",
    "            #initialize DataFrame column names list (name for each feature point of NN)\n",
    "            columns=[]\n",
    "            #for i in range(len(test_pixel)-2):#-2 because tfp and tau are at end of pixel data\n",
    "            #for j in range(len(test_pixel)-1):#-1 because tau is at end of pixel data\n",
    "            for j in range(len(test_pixel)-4):#-4 because k,Q,omega,tau is at end of pixel data\n",
    "                columns.append('t='+str(j))   #most columns are just time points and I'm making them here\n",
    "    \n",
    "            #columns.append('Tfp') #because tfps are appended onto the 2nd to last column of my inst_freq data.  Uncomment for Inst_freq data!\n",
    "            columns.append('k')\n",
    "            columns.append('Q')\n",
    "            columns.append('omega')\n",
    "            columns.append('Tau') #because taus are appended onto the end of my data\n",
    "    \n",
    "    \n",
    "            #load all of the data into an array for input into DataFrame\n",
    "            #each entry in the \"data1\" array is a numpy array of a displacement curve\n",
    "            data1 = [np.load(file_path[i]) for i in range(0,(len(file_path)))]\n",
    "    \n",
    "            #make df for output\n",
    "            train_data = pd.DataFrame(data=data1,columns=columns,dtype=np.float64)\n",
    "            train_data = train_data.drop('t=0',axis=1)\n",
    "            #these t=0 points end up as just NaNs after preprocessing and are useless anyways because by definition freq shift is 0 at trigger for all points\n",
    "            #dropping t=0 maybe unnecessary with displacement data?\n",
    "            \n",
    "            df_main = pd.concat([df_main,train_data], ignore_index=True) #append each run to the final collection DataFrame\n",
    "        \n",
    "        df_main = shuffle(df_main) #shuffle data because it is currently ordered and this could impact NN learning\n",
    "        \n",
    "        return df_main\n",
    "    \n",
    "    def load_experimental_train_data(self, *paths):\n",
    "        \"\"\"Path should be to Runx folder:\n",
    "        E.g.\n",
    "        path = \"C:/Users/Jake/Desktop/75khz RSI data/Run1\" for my pc or\n",
    "        path = \"D:/Jake/DDHO data/durmus_data/75khz RSI data/Run1\" for simulation computer\"\"\"\n",
    "        #taus = np.array([10e-9, 25e-9, 50e-9, 100e-9, 250e-9, 500e-9,\n",
    "        #                        1e-6, 5e-6, 10e-6, 100e-6, 1e-3])\n",
    "        taus= np.array([10e-9, 31.62e-9, 100e-9, 316.2e-9, 1e-6, 3.162e-6, 10e-6, 31.62e-6, 100e-6, 316.2e-6, 1e-3])\n",
    "        #below code assumes data array has trigger at 16384 and total points 16384*2 (this is what all my and Durmus' data is)\n",
    "        \n",
    "        #init df for collecting each run\n",
    "        df_main = pd.DataFrame()\n",
    "        \n",
    "        for i in paths:\n",
    "            \n",
    "            tau_paths = glob.glob(i + \"/*\")\n",
    "            sorted_tau_paths = sorted(tau_paths, key = lambda x: int(os.path.basename(os.path.normpath(x))))\n",
    "            #Above line properly sorts so that they are ordered 0,1,2,3,4,5,6,7,8,9,10 rather than 0,1,10,2,3,4...\n",
    "    \n",
    "            #Init dataframe for this run\n",
    "            df_run = pd.DataFrame()\n",
    "            \n",
    "            #Loop through every tau in that run\n",
    "            for j in range(4,len(tau_paths)):\n",
    "                tau = taus[j]\n",
    "                displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "                #collect relevent parts of the real data into df for that run\n",
    "                for k in range(len(displacement_path)):\n",
    "                    disp_array = loadibw(displacement_path[k])['wave']['wData'] #load displacement from .ibw\n",
    "                    #throw away all displacement before the trigger (16384 pre-trigger points)\n",
    "                    disp_array = disp_array[16384:,:]\n",
    "                    disp_array = np.transpose(disp_array)\n",
    "        \n",
    "                    #Put loaded stuff into dataframe and label tau\n",
    "                    columns=[]\n",
    "                    for l in range(disp_array.shape[1]):\n",
    "                        columns.append('t='+str(l))\n",
    "            \n",
    "                    df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "                    df_temp['Tau'] = pd.Series(index=df_temp.index) #create Tau column\n",
    "                    df_temp['Tau'] = tau #assign tau value to tau column (could probably be done in above step with data=tau?)\n",
    "                    \n",
    "                    df_run = df_run.append(df_temp,ignore_index=True) #append each tau value to this run\n",
    "                    #df_run = pd.concat([df_run,df_temp],ignore_index=True)\n",
    "            \n",
    "            \n",
    "            df_main = pd.concat([df_main,df_run], ignore_index=True) #append each run to the final collection DataFrame\n",
    "            \n",
    "        \n",
    "        df_main = shuffle(df_main) #shuffle data because it is currently ordered and this could impact NN learning\n",
    "        \n",
    "        return df_main\n",
    "    \n",
    "        \n",
    "    \n",
    "    def preprocess_train_data(self,train_data):\n",
    "        #Prep training data\n",
    "\n",
    "        num_samples = len(train_data)\n",
    "        train_x = train_data[0:num_samples+1] #this syntax is an artifact from when I was training with partial data sets but it doesn't really add much timing loss so I'm leaving it in case I need to change it again\n",
    "        \n",
    "        train_x1 = train_x.drop(['k','Q','omega','Tau'],axis=1)\n",
    "        \n",
    "        train_x2 = train_x[['k','Q','omega']]\n",
    "        \n",
    "        #train_x = train_x.drop('Tau',axis=1) #dropping Tau because we do not input Tau to the neural network (that's like giving it the solution and then asking for the solution--it cheats)\n",
    "        #train_x = train_x.drop('Tfp',axis=1) #for simulations only\n",
    "        \n",
    "        self.mean_train_x1 = np.mean(train_x1) #saving the mean_train_x for preprocessing the test data in the same manner as our training dat\n",
    "        self.mean_train_x2 = np.mean(train_x2)\n",
    "        \n",
    "        self.SD_train_x1 = np.std(train_x1) #saving the SD_train_x for preprocessing the test data in the same manner as our training data\n",
    "        self.SD_train_x2 = np.std(train_x2)\n",
    "        \n",
    "        train_x1_norm = (train_x1 - self.mean_train_x1) /  (self.SD_train_x1) #normalize and centralize the training data for best neural network performance\n",
    "        train_x1_norm_reshaped = np.expand_dims(train_x1_norm,axis=2) #formatting for input into CNN\n",
    "        \n",
    "        train_x2_norm = (train_x2 - self.mean_train_x2) /  (self.SD_train_x2) #normalize and centralize the training data for best neural network performance\n",
    "        train_x2_norm_reshaped = np.expand_dims(train_x2_norm,axis=2) #formatting for input into CNN\n",
    "        \n",
    "        train_y = np.array(train_data['Tau']) #labeled, true Tau values for the CNN to learn from\n",
    "        train_y = train_y[0:num_samples+1] #this syntax is an artifact from when I was training with partial data sets but it doesn't really add much timing loss so I'm leaving it in case I need to change it again\n",
    "        \n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "\n",
    "        #tau_index is used to recover the original tau's from a one-hot encoded output.\n",
    "        #e.g. tau = [10, 100, 1000, 10, 10] then\n",
    "        #unique_tau = [10, 100, 1000]\n",
    "        #tau_index = [0,1,2,0,0] is index of tau to corresponding unique_tau so\n",
    "        #unique_tau[tau_index] == tau \n",
    "        unique_tau, tau_index = np.unique(train_y,return_inverse=True)\n",
    "\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "\n",
    "        self.number_of_classes = one_hot_tau.shape[1] #used to match number of output Softmax layers in my NN\n",
    "        \n",
    "        return train_x1_norm_reshaped, train_x2_norm_reshaped, one_hot_tau\n",
    "    \n",
    "    def train_CNN(self, train_x1, train_x2, train_y, num_epochs = 40, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3):\n",
    "        #Build CNN and start training!\n",
    "\n",
    "        self.filter_number1 = num_filter1\n",
    "        self.filter_number2 = num_filter2\n",
    "        self.kernel1_size = kernel1_size\n",
    "        self.kernel2_size = kernel2_size\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Initialize CNN branch 1 for main convolutional input data (displacement or instantaneous frequency)\n",
    "        branch1 = Sequential()\n",
    "\n",
    "        #Add convolution layers\n",
    "        branch1.add(Conv1D(filters=num_filter1,kernel_size=kernel1_size,strides=2,padding='same',input_shape=(train_x1.shape[1],1)))\n",
    "        branch1.add(LeakyReLU(alpha=0.01))\n",
    "        branch1.add(MaxPooling1D())\n",
    "\n",
    "        branch1.add(Conv1D(filters=num_filter2,kernel_size=kernel2_size,strides=2,padding='same'))\n",
    "        branch1.add(LeakyReLU(alpha=0.01))\n",
    "        branch1.add(MaxPooling1D())\n",
    "\n",
    "        branch1.add(Flatten())\n",
    "        #Roughly 500 units length of branch 1 (8000 displacement points / (2**4 because each strides = 2 and each maxpool halves data length))\n",
    "        \n",
    "        branch1.add(Dense(units=100, kernel_initializer='he_normal',activation='linear'))\n",
    "        branch1.add(LeakyReLU(alpha=.01))\n",
    "        branch1.add(Dropout(0.3))\n",
    "\n",
    "        branch1.add(Dense(units=100, kernel_initializer='he_normal',activation='linear'))\n",
    "        branch1.add(LeakyReLU(alpha=.01))\n",
    "        branch1.add(Dropout(0.4))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Initialize CNN branch 2 for supplementary data (Q, k, and omega)\n",
    "        branch2 = Sequential()\n",
    "\n",
    "        #Add supplementary data inputs\n",
    "        branch2.add(Dense(units=100, kernel_initializer='he_normal', activation='linear', input_shape=(train_x2.shape[1],1)))\n",
    "        branch2.add(LeakyReLU(alpha=.01))\n",
    "        branch2.add(Dropout(0.3))\n",
    "        \n",
    "        branch2.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        branch2.add(LeakyReLU(alpha=.01))\n",
    "        branch2.add(Dropout(0.4))\n",
    "        \n",
    "        branch2.add(Flatten())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Merge branches 1 and 2\n",
    "        model = Sequential()\n",
    "        model.add(Merge([branch1,branch2], mode='concat'))\n",
    "\n",
    "        \n",
    "        #Add final fully connected layers\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        #Add classification layer\n",
    "        model.add(Dense(units=self.number_of_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "        #Compile CNN and configure metrics/learning process\n",
    "        \n",
    "        \n",
    "        \"\"\"below functions are failure metrics that tell me if the true tau was in the top 2, top 3, or top 5 guesses made by the neural network\"\"\"\n",
    "        def inTop2(k=2):\n",
    "            def top2metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=2)\n",
    "            return top2metric\n",
    "        \n",
    "        def inTop3(k=3):\n",
    "            def top3metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=3)\n",
    "            return top3metric\n",
    "        \n",
    "        def inTop5(k=5):\n",
    "            def top5metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=5)\n",
    "            return top5metric\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy', inTop2(), inTop3()])\n",
    "\n",
    "        #Prepare for visualization\n",
    "        #tbCallBack = callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "        #tbCallBack = callbacks.TensorBoard(log_dir=\"logs/{}\".format(time.time()))\n",
    "\n",
    "        #Train model\n",
    "        model.fit([train_x1, train_x2], train_y, batch_size=32, epochs=num_epochs,verbose=2, validation_split=0.05)#, callbacks=[tbCallBack])\n",
    "        self.model = model #save model to self for calling from other functions later\n",
    "        return\n",
    "    \n",
    "    def load_simulated_test_data(self, file_path):#, test_fraction):\n",
    "        \"\"\"input string with file path (e.g. \"D:/jake/DDHO Data/inst_freq/25us/0pt1noise/*.npy\" )\n",
    "        and also input the fraction of the data you wish to test (e.g. testing 10% of data would be 0.1)\n",
    "    \n",
    "        outputs the requested percentage of the data (test_data_norm_reshaped) and their corresponding labels for evaluation (one_hot_tau)\n",
    "        these outputs are basically test_x and test_y that are formatted to be fed into model.evaluate()\n",
    "    \n",
    "        NOTE this function requires previous cells to have been run (train_x must exist!!!)\"\"\"\n",
    "        file_path1 = glob.glob(file_path)\n",
    "        test_pixel = np.load(file_path1[0])\n",
    "        #this will be used in for loop below\n",
    "        #it is just used to grab the length of any one inst. freq. curve\n",
    "    \n",
    "        columns2=[]\n",
    "        for i in range(len(test_pixel)-4):#-4 because k,Q,omega, tau is at end of pixel data\n",
    "        #for i in range(len(test_pixel)-2):#-2 because tfp and tau are at end of pixel data\n",
    "            columns2.append('t='+str(i))#most columns are just time points and I'm making them here\n",
    "    \n",
    "        #columns2.append('Tfp') #because tfps are appended onto the end of my inst_freq data\n",
    "        columns2.append('k')\n",
    "        columns2.append('Q')\n",
    "        columns2.append('omega')\n",
    "        columns2.append('Tau') #because taus are appended onto the end of my inst_freq data\n",
    "    \n",
    "        #alternate way to load only a fraction of the data to save memory and time\n",
    "        #num_samples = len(file_path1)\n",
    "        #num_buckets = self.number_of_classes\n",
    "        #bucket_range = int(num_samples/num_buckets)\n",
    "        #test_fraction_range = int(test_fraction * bucket_range)\n",
    "        #load_list = []\n",
    "\n",
    "        #for i in range(num_buckets):\n",
    "        #    for j in range(test_fraction_range):\n",
    "        #        load_list.append(int((i * bucket_range) + (j)))\n",
    "    \n",
    "        #data1 = [np.load(file_path1[i]) for i in load_list]\n",
    "        data1 = [np.load(file_path1[i]) for i in range(len(file_path1))]\n",
    "\n",
    "        #make df for output\n",
    "        test_data = pd.DataFrame(data=data1,columns=columns2,dtype=np.float64)\n",
    "        test_data = test_data.drop('t=0',axis=1)\n",
    "        #these t=0 points end up as just NaNs and are useless anyways because by definition freq shift is 0 at trigger\n",
    "        test_data = shuffle(test_data)\n",
    "        #shuffle data because it is currently ordered and this could impact NN learning\n",
    "\n",
    "        test_y = np.array(test_data['Tau'])\n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "        unique_tau, tau_index = np.unique(test_y,return_inverse=True)\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "    \n",
    "        #preprocess test_x\n",
    "        test_x1 = test_data.drop(['k','Q','omega','Tau'],axis=1)\n",
    "        test_x2 = test_data[['k','Q','omega']]\n",
    "        \n",
    "        #test_data_norm = (test_data - self.mean_train_x ) /  (self.SD_train_x) #important to preprocess my test data same as my train data!!!!\n",
    "        #test_data_norm_reshaped = np.expand_dims(test_data_norm,axis=2)\n",
    "        \n",
    "        test_x1_norm = (test_x1 - self.mean_train_x1 ) /  (self.SD_train_x1) #important to preprocess my test data same as my train data!!!!\n",
    "        test_x2_norm = (test_x2 - self.mean_train_x2 ) /  (self.SD_train_x2) #important to preprocess my test data same as my train data!!!!\n",
    "        \n",
    "        test_x1_norm_reshaped = np.expand_dims(test_x1_norm,axis=2)\n",
    "        test_x2_norm_reshaped = np.expand_dims(test_x2_norm,axis=2)\n",
    "        \n",
    "        \n",
    "        return test_x1_norm_reshaped, test_x2_norm_reshaped, one_hot_tau\n",
    "    \n",
    "    \n",
    "    def load_experimental_test_data(self, path):\n",
    "        \"\"\"input string with file path\n",
    "    \n",
    "        outputs the requested percentage of the data (test_data_norm_reshaped) and their corresponding labels for evaluation (one_hot_tau)\n",
    "        these outputs are basically test_x and test_y that are formatted to be fed into model.evaluate()\n",
    "    \n",
    "        NOTE this function requires previous cells to have been run (train_x must exist!!!)\"\"\"\n",
    "        #taus = np.array([10e-9, 25e-9, 50e-9, 100e-9, 250e-9, 500e-9,\n",
    "        #                        1e-6, 5e-6, 10e-6, 100e-6, 1e-3])\n",
    "        taus= np.array([10e-9, 31.62e-9, 100e-9, 316.2e-9, 1e-6, 3.162e-6, 10e-6, 31.62e-6, 100e-6, 316.2e-6, 1e-3])\n",
    "        \n",
    "        \n",
    "        tau_paths = glob.glob(path + \"/*\")\n",
    "        sorted_tau_paths = sorted(tau_paths, key = lambda x: int(os.path.basename(os.path.normpath(x))))\n",
    "        #Above line properly sorts so that they are ordered 0,1,2,3,4,5,6,7,8,9,10 rather than 0,1,10,2,3,4...\n",
    "    \n",
    "        #Init dataframe for this run\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        for j in range(4,len(tau_paths)):\n",
    "            tau = taus[j]\n",
    "            displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "            #print(displacement_path)\n",
    "            #print(j)\n",
    "            #print('the above two should be matching indices')\n",
    "            \n",
    "            disp_array = loadibw(displacement_path[0])['wave']['wData']\n",
    "            #throw away all displacement before the trigger\n",
    "            disp_array = disp_array[16384:,:]\n",
    "            disp_array = np.transpose(disp_array)\n",
    "        \n",
    "            #Put loaded stuff into dataframe and label tau\n",
    "            columns=[]\n",
    "            for k in range(disp_array.shape[1]):\n",
    "                columns.append('t='+str(k))\n",
    "        \n",
    "            df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "            df_temp['Tau'] = pd.Series(index=df_temp.index)\n",
    "            df_temp['Tau'] = tau\n",
    "            df = df.append(df_temp,ignore_index=True)\n",
    "       \n",
    "        \"\"\"for j in range(len(tau_paths)):\n",
    "            tau = taus[j]\n",
    "            displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "            print(displacement_path)\n",
    "            #print(j)\n",
    "            #print('the above two should be matching indices')\n",
    "            \n",
    "            for i in range(len(displacement_path)):\n",
    "                disp_array = loadibw(displacement_path[i])['wave']['wData']\n",
    "                #throw away all displacement before the trigger\n",
    "                disp_array = disp_array[16384:,:]\n",
    "                disp_array = np.transpose(disp_array)\n",
    "        \n",
    "                #Put loaded stuff into dataframe and label tau\n",
    "                columns=[]\n",
    "                for k in range(disp_array.shape[1]):\n",
    "                    columns.append('t='+str(k))\n",
    "        \n",
    "                df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "                df_temp['Tau'] = pd.Series(index=df_temp.index)\n",
    "                df_temp['Tau'] = tau\n",
    "                df = df.append(df_temp,ignore_index=True)\n",
    "            \n",
    "        df = shuffle(df) #shuffle data because it is currently ordered and this could impact NN learning\n",
    "        \"\"\"\n",
    "        \n",
    "        df = shuffle(df) #shuffle data because it is currently ordered and this could impact NN learning\n",
    "        \n",
    "        test_y = np.array(df['Tau'])\n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "        unique_tau, tau_index = np.unique(test_y,return_inverse=True)\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "    \n",
    "        #preprocess test_x\n",
    "        df = df.drop('Tau',axis=1)\n",
    "        #test_data = test_data.drop('Tfp',axis=1)\n",
    "        df_norm = (df - self.mean_train_x ) /  (self.SD_train_x) #important to preprocess my test data same as my train data!!!!\n",
    "        df_norm_reshaped = np.expand_dims(df_norm,axis=2)\n",
    "    \n",
    "        return df_norm_reshaped, one_hot_tau\n",
    "    \n",
    "    \n",
    "    def load_experimental_test_data_averaged(self, path):\n",
    "        \"\"\"input string with file path\n",
    "    \n",
    "        outputs the requested percentage of the data (test_data_norm_reshaped) and their corresponding labels for evaluation (one_hot_tau)\n",
    "        these outputs are basically test_x and test_y that are formatted to be fed into model.evaluate()\n",
    "    \n",
    "        NOTE this function requires previous cells to have been run (train_x must exist!!!)\"\"\"\n",
    "        #This function is unused and not useful\n",
    "        #taus = np.array([10e-9, 25e-9, 50e-9, 100e-9, 250e-9, 500e-9,\n",
    "        #                        1e-6, 5e-6, 10e-6, 100e-6, 1e-3])\n",
    "        taus= np.array([10e-9, 31.62e-9, 100e-9, 316.2e-9, 1e-6, 3.162e-6, 10e-6, 31.62e-6, 100e-6, 316.2e-6, 1e-3])\n",
    "        \n",
    "        \n",
    "        tau_paths = glob.glob(path + \"/*\")\n",
    "        sorted_tau_paths = sorted(tau_paths, key = lambda x: int(os.path.basename(os.path.normpath(x))))\n",
    "        #Above line properly sorts so that they are ordered 0,1,2,3,4,5,6,7,8,9,10 rather than 0,1,10,2,3,4...\n",
    "    \n",
    "        #Init dataframe for this run\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        for j in range(len(tau_paths)):\n",
    "            tau = taus[j]\n",
    "            displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "            #print(displacement_path)\n",
    "            #print(j)\n",
    "            #print('the above two should be matching indices')\n",
    "            \n",
    "            disp_array = loadibw(displacement_path[0])['wave']['wData']\n",
    "            #throw away all displacement before the trigger\n",
    "            disp_array = disp_array[16384:,:]\n",
    "            disp_array = np.transpose(disp_array)\n",
    "        \n",
    "            #Put loaded stuff into dataframe and label tau\n",
    "            columns=[]\n",
    "            for k in range(disp_array.shape[1]):\n",
    "                columns.append('t='+str(k))\n",
    "        \n",
    "            df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "            df_temp['Tau'] = pd.Series(index=df_temp.index)\n",
    "            df_temp['Tau'] = tau\n",
    "            \n",
    "            df_temp = df_temp.mean(axis=0)\n",
    "            \n",
    "            df = df.append(df_temp,ignore_index=True)\n",
    "        \n",
    "        #df = shuffle(df) #shuffle data because it is currently ordered and this could impact NN learning\n",
    "        \n",
    "        test_y = np.array(df['Tau'])\n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "        unique_tau, tau_index = np.unique(test_y,return_inverse=True)\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "    \n",
    "        #preprocess test_x\n",
    "        df = df.drop('Tau',axis=1)\n",
    "        #test_data = test_data.drop('Tfp',axis=1)\n",
    "        df_norm = (df - self.mean_train_x ) /  (self.SD_train_x) #important to preprocess my test data same as my train data!!!!\n",
    "        df_norm_reshaped = np.expand_dims(df_norm,axis=2)\n",
    "    \n",
    "        return df_norm_reshaped, one_hot_tau\n",
    "    \n",
    "    \n",
    "    def test_closeness(self, test_x1, test_x2, test_y):\n",
    "        \"\"\"This function looks at the predicted tau values from model.predict(test_x) and compares them \n",
    "        to the true tau values from test_y.  \n",
    "        It then returns three values telling you what percentage of the incorrect predictions varied by spacing of\n",
    "        one tau value, two tau values, or three tau values.\n",
    "        \n",
    "        E.G.\n",
    "        Say possible taus = [1,2,3,4,5,6,7,8,9]\n",
    "        model.predict(test_x) = [2,2,2,3,3,3,4,4,5,6]\n",
    "        test_y = [2,2,2,2,2,2,2,2,2,2]\n",
    "        \n",
    "        test_closeness returns [0.3,0.2,0.1]\n",
    "        because 30% of the predictions varied by one tau value (tau = 2 but 3 times it guessed tau = 3)\n",
    "        because 20% of the predictions varied by two tau values (tau = 2 but 2 times it guessed tau = 4)\n",
    "        because 10% of the predictions varied by three tau values (tau = 2 but 1 time it guessed tau = 5)\n",
    "        \"\"\"\n",
    "        \n",
    "        pred_tau = self.model.predict([test_x1,test_x2],verbose=0)\n",
    "        \n",
    "        pred_tau_am = pred_tau.argmax(axis=-1) #pluck out actual prediction value!\n",
    "        test_y_am = test_y.argmax(axis=-1) #does not actually need argmax, but this makes it same format as pred_tau_am\n",
    "        \n",
    "        incorrect_indices = np.nonzero(pred_tau_am != test_y_am) #indices of incorrect predictions\n",
    "        \n",
    "        total_samples = len(pred_tau)\n",
    "        total_fails = len(incorrect_indices[0])\n",
    "        \n",
    "        #init diff collection variables (how many tau values away the true value was from the predicted value)\n",
    "        num_diff_1 = 0\n",
    "        num_diff_2 = 0\n",
    "        num_diff_3 = 0\n",
    "        num_greater = 0\n",
    "        \n",
    "        #init array for seeing which taus it is bad at predicting\n",
    "        #which_taus_failed = np.zeros(11) #CHANGE THIS HARD-CODED 11 TO WHATEVER THE NUMBER OF CLASSES IS!!!\n",
    "        which_taus_failed = np.zeros(self.number_of_classes) #CHANGE THIS HARD-CODED 11 TO WHATEVER THE NUMBER OF CLASSES IS!!!\n",
    "        \n",
    "        for element in incorrect_indices[0]:\n",
    "            \n",
    "            #collect diff (how many tau values away the true value was from the predicted value)\n",
    "            diff = abs(pred_tau_am[element] - test_y_am[element])\n",
    "            if diff == 1:\n",
    "                num_diff_1 += 1\n",
    "            elif diff == 2:\n",
    "                num_diff_2 += 1\n",
    "            elif diff == 3:\n",
    "                num_diff_3 += 1\n",
    "            else:\n",
    "                num_greater += 1\n",
    "            \n",
    "            #collect how many of each tau failed\n",
    "            i=0\n",
    "            while True:\n",
    "                if test_y_am[element] == i:\n",
    "                    which_taus_failed[i] += 1\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "                    \n",
    "            which_taus_failed_percent = np.round((which_taus_failed / total_fails),4) * 100\n",
    "\n",
    "        \n",
    "        percent_num_diff_1 = round((num_diff_1 / total_samples), 4) * 100\n",
    "        percent_num_diff_2 = round((num_diff_2 / total_samples), 4) * 100\n",
    "        percent_num_diff_3 = round((num_diff_3 / total_samples), 4) * 100\n",
    "        percent_num_diff_greater = round((num_greater / total_samples), 4) * 100\n",
    "            \n",
    "        #Next section is for debugging purposes\n",
    "        #percent_incorrect = (len(incorrect_indices[0])/total_samples)\n",
    "        #percent_incorrect_calculated = percent_num_diff_1 + percent_num_diff_2 + percent_num_diff_3 + percent_num_diff_greater\n",
    "        #print('percent incorrect should be ' + str(percent_incorrect))\n",
    "        #print('percent incorrect calculated is ' + str(percent_incorrect_calculated))\n",
    "        \n",
    "        return percent_num_diff_1, percent_num_diff_2, percent_num_diff_3, which_taus_failed#_percent\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def test_simulated_CNN(self, *paths):\n",
    "        \n",
    "        score_string = 'data order is testing against '\n",
    "        for element in paths:\n",
    "            score_string += (str(element) + \" , \\n\") \n",
    "\n",
    "        \n",
    "        score_collect = [score_string]\n",
    "        score_collect.append('column order is loss, accuracy, top2metric, top3metric')\n",
    "        score_collect.append('top2metric = % that the true tau was one of the top 2 predictions')\n",
    "        score_collect.append(' ')\n",
    "        \n",
    "        \n",
    "        #score_collect = ['data order is no_noise, 0pt1noise, 1noise, random_noise_1, random_noise_10']\n",
    "        #score_collect.append('first column is loss, second column is accuracy')\n",
    "        \n",
    "        for i in paths:\n",
    "            test_x1, test_x2, test_y = self.load_simulated_test_data(i)#,test_fraction)\n",
    "            score = self.model.evaluate([test_x1,test_x2],test_y, batch_size=32)\n",
    "            percentage = str(round(score[1],5) * 100)\n",
    "            print('model scored ' + percentage + '% on ' + str(i))\n",
    "            score_collect.append(str(score))\n",
    "            \n",
    "            error1, error2, error3, which_taus_failed = self.test_closeness(test_x1,test_x2,test_y)\n",
    "            score_collect.append('one_diff_error = ' + str(error1))\n",
    "            score_collect.append('two_diff_error = ' + str(error2))\n",
    "            score_collect.append('three_diff_error = ' + str(error3))\n",
    "            score_collect.append('which taus failed were: ' + str(which_taus_failed))\n",
    "            print('one_diff_error = ' + str(error1))\n",
    "            print('two_diff_error = ' + str(error2))\n",
    "            print('three_diff_error = ' + str(error3))\n",
    "            print('which taus failed were: ' + str(which_taus_failed))\n",
    "            print(' ')\n",
    "            \n",
    "            score_collect.append('above scores were for ' + str(element)) #new code on 7/2/18\n",
    "            score_collect.append(' ')\n",
    "        \n",
    "        self.score_collect = score_collect\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def test_experimental_CNN(self, *paths):\n",
    "        \n",
    "        score_string = 'data order is testing against '\n",
    "        for element in paths:\n",
    "            score_string += (str(element) + \" , \\n\") \n",
    "\n",
    "        \n",
    "        score_collect = [score_string]\n",
    "        score_collect.append('column order is loss, accuracy, top2metric, top3metric')\n",
    "        score_collect.append('top2metric = % that the true tau was one of the top 2 predictions')\n",
    "        score_collect.append(' ')\n",
    "        \n",
    "        for element in paths:\n",
    "            test_x1, test_x2, test_y = self.load_experimental_test_data(element)\n",
    "            score = self.model.evaluate([test_x1,test_x2],test_y,batch_size = 32)\n",
    "            percentage = str(round(score[1],5) * 100)\n",
    "            print('model scored ' + percentage + '% on ' + str(element))\n",
    "            score_collect.append(str(score))\n",
    "            #score_collect.append('above score was for ' + str(element)) #new code on 7/2/18\n",
    "            \n",
    "            error1, error2, error3, which_taus_failed = self.test_closeness(test_x,test_y)\n",
    "            score_collect.append('one_diff_error = ' + str(error1))\n",
    "            score_collect.append('two_diff_error = ' + str(error2))\n",
    "            score_collect.append('three_diff_error = ' + str(error3))\n",
    "            score_collect.append('which taus failed were: ' + str(which_taus_failed))\n",
    "            print('one_diff_error = ' + str(error1))\n",
    "            print('two_diff_error = ' + str(error2))\n",
    "            print('three_diff_error = ' + str(error3))\n",
    "            print('which taus failed were: ' + str(which_taus_failed))\n",
    "            print(' ')\n",
    "            \n",
    "            score_collect.append('above scores were for ' + str(element)) #new code on 7/2/18\n",
    "            score_collect.append(' ')\n",
    "            \n",
    "        \n",
    "        self.score_collect = score_collect\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def test_experimental_CNN_averaged(self, *paths):\n",
    "        \n",
    "        score_string = 'data order is testing against '\n",
    "        for element in paths:\n",
    "            score_string += (str(element) + \" , \\n\") \n",
    "\n",
    "        \n",
    "        score_collect = [score_string]\n",
    "        score_collect.append('column order is loss, accuracy, top2metric, top3metric')\n",
    "        score_collect.append('top2metric = % that the true tau was one of the top 2 predictions')\n",
    "        score_collect.append(' ')\n",
    "        \n",
    "        for element in paths:\n",
    "            test_x, test_y = self.load_experimental_test_data_averaged(element)\n",
    "            score = self.model.evaluate(test_x,test_y,batch_size = 32)\n",
    "            percentage = str(round(score[1],5) * 100)\n",
    "            print('model scored ' + percentage + '% on ' + str(element))\n",
    "            score_collect.append(str(score))\n",
    "            #score_collect.append('above score was for ' + str(element)) #new code on 7/2/18\n",
    "            \n",
    "            #more new code currently testing\n",
    "            error1, error2, error3, which_taus_failed = self.test_closeness(test_x,test_y)\n",
    "            score_collect.append('one_diff_error = ' + str(error1))\n",
    "            score_collect.append('two_diff_error = ' + str(error2))\n",
    "            score_collect.append('three_diff_error = ' + str(error3))\n",
    "            score_collect.append('which taus failed were: ' + str(which_taus_failed))\n",
    "            print('one_diff_error = ' + str(error1))\n",
    "            print('two_diff_error = ' + str(error2))\n",
    "            print('three_diff_error = ' + str(error3))\n",
    "            print('which taus failed were: ' + str(which_taus_failed))\n",
    "            print(' ')\n",
    "            \n",
    "            score_collect.append('above scores were for ' + str(element)) #new code on 7/2/18\n",
    "            score_collect.append(' ')\n",
    "            \n",
    "        \n",
    "        self.score_collect = score_collect\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def save_CNN(self, save_str):\n",
    "        #save model and test evaluation outputs\n",
    "        #example save_str: save_str = 'displacement_10us_random_noise_10_2018_06_13_80epoch'\n",
    "        #requires test_CNN to have been run already\n",
    "        path = 'C:/Users/jakeprecht/DDHO/saved CNN models/'\n",
    "        save_str_h5 = path + save_str + '.h5'\n",
    "        save_str_txt = path + save_str + '_results.txt'\n",
    "        save_str_weights = path + save_str + '_weights.h5'\n",
    "        \n",
    "        self.model.save(save_str_h5)  # creates a HDF5 file 'my_model.h5'\n",
    "        self.model.save_weights(save_str_weights)\n",
    "        \n",
    "        output_scores = open(save_str_txt, 'w')\n",
    "        for item in self.score_collect:\n",
    "            output_scores.write(\"%s\\n\" % item)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def visualize_weights(self, layer_number):\n",
    "        #layer number 0 = conv layer 1\n",
    "        #layer number 1 = ReLU 1\"\"\"\n",
    "        weights, biases = self.model.layers[layer_number].get_weights()\n",
    "        \n",
    "        if layer_number == 0 or 1:\n",
    "            number_filters = self.filter_number1\n",
    "            kernel_length = self.kernel1_size         \n",
    "\n",
    "        #elif layer_number == 3:\n",
    "        #    number_filters = self.filter_number2\n",
    "        #    kernel_length = self.kernel2_size\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Input for layer_number must be 0 or 3 in current implementation (2018_08_08)\")\n",
    "            \n",
    "        fig = plt.figure()\n",
    "        for i in range(number_filters):\n",
    "            weight_plt = weights[:,:,i]\n",
    "            weight_plt2 = weight_plt.reshape((kernel_length,))\n",
    "            #ax = fig.add_subplot(number_filters,1,i+1)\n",
    "            plt.figure()\n",
    "            plt.plot(weight_plt2)\n",
    "            #ax.imshow(weight_plt2,cmap='gray')\n",
    "            \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def layer_to_visualize(self, layer, img_to_visualize):\n",
    "        \"\"\"img_to_visualize = train_x[image_number]\n",
    "        this code does not work yet\n",
    "        \"\"\"\n",
    "        layer = self.model.layers[layer]\n",
    "        img_to_visualize = np.expand_dims(img_to_visualize, axis=0)\n",
    "        \n",
    "        inputs = [K.learning_phase()] + self.model.inputs\n",
    "\n",
    "        _convout1_f = K.function(inputs, [layer.output])\n",
    "        def convout1_f(X):\n",
    "            # The [0] is to disable the training phase flag\n",
    "            return _convout1_f([0] + [X])\n",
    "\n",
    "        convolutions = convout1_f(img_to_visualize)\n",
    "        convolutions = np.squeeze(convolutions)\n",
    "\n",
    "        print ('Shape of conv:', convolutions.shape)\n",
    "    \n",
    "        n = convolutions.shape[0]\n",
    "        n = int(np.ceil(np.sqrt(n)))\n",
    "    \n",
    "        # Visualization of each filter of the layer\n",
    "        fig = plt.figure(figsize=(12,8))\n",
    "        for i in range(len(convolutions)):\n",
    "            ax = fig.add_subplot(n,n,i+1)\n",
    "            ax.imshow(convolutions[i], cmap='gray')\n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "#number_of_classes = one_hot_tau.shape[1] #used to match number of output Softmax layers in my NN\n",
    "#train_x_norm_reshaped = np.expand_dims(train_x_norm,axis=2) #formatting for input into CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below three cells are for quick testing of code (load one run, train one epoch, test two runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2018_09_17/0noise/*.npy\")\n",
    "#train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x1, train_x2, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakeprecht\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:217: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36575 samples, validate on 1925 samples\n",
      "Epoch 1/20\n",
      " - 214s - loss: 2.4235 - acc: 0.0907 - top2metric: 0.1803 - top3metric: 0.2706 - val_loss: 2.3977 - val_acc: 0.0945 - val_top2metric: 0.1912 - val_top3metric: 0.2857\n",
      "Epoch 2/20\n",
      " - 211s - loss: 2.3995 - acc: 0.0902 - top2metric: 0.1821 - top3metric: 0.2712 - val_loss: 2.3985 - val_acc: 0.0966 - val_top2metric: 0.1865 - val_top3metric: 0.2758\n",
      "Epoch 3/20\n",
      " - 210s - loss: 2.3987 - acc: 0.0904 - top2metric: 0.1794 - top3metric: 0.2704 - val_loss: 2.3985 - val_acc: 0.0925 - val_top2metric: 0.1813 - val_top3metric: 0.2644\n",
      "Epoch 4/20\n",
      " - 210s - loss: 2.3985 - acc: 0.0921 - top2metric: 0.1783 - top3metric: 0.2672 - val_loss: 2.3984 - val_acc: 0.0919 - val_top2metric: 0.1740 - val_top3metric: 0.2660\n",
      "Epoch 5/20\n",
      " - 210s - loss: 2.3731 - acc: 0.0990 - top2metric: 0.1979 - top3metric: 0.2929 - val_loss: 1.7517 - val_acc: 0.1922 - val_top2metric: 0.3818 - val_top3metric: 0.5590\n",
      "Epoch 6/20\n",
      " - 210s - loss: 1.2018 - acc: 0.4500 - top2metric: 0.7080 - top3metric: 0.8597 - val_loss: 0.6579 - val_acc: 0.7023 - val_top2metric: 0.8488 - val_top3metric: 0.9455\n",
      "Epoch 7/20\n",
      " - 210s - loss: 0.7520 - acc: 0.6585 - top2metric: 0.8595 - top3metric: 0.9545 - val_loss: 0.4531 - val_acc: 0.8000 - val_top2metric: 0.8945 - val_top3metric: 1.0000\n",
      "Epoch 8/20\n",
      " - 211s - loss: 0.5083 - acc: 0.7697 - top2metric: 0.9148 - top3metric: 0.9868 - val_loss: 0.3055 - val_acc: 0.8353 - val_top2metric: 0.9590 - val_top3metric: 1.0000\n",
      "Epoch 9/20\n",
      " - 210s - loss: 0.4412 - acc: 0.7978 - top2metric: 0.9271 - top3metric: 0.9920 - val_loss: 0.3098 - val_acc: 0.8395 - val_top2metric: 0.9429 - val_top3metric: 1.0000\n",
      "Epoch 10/20\n",
      " - 211s - loss: 0.4068 - acc: 0.8089 - top2metric: 0.9306 - top3metric: 0.9938 - val_loss: 0.2557 - val_acc: 0.9034 - val_top2metric: 0.9200 - val_top3metric: 1.0000\n",
      "Epoch 11/20\n",
      " - 211s - loss: 0.3878 - acc: 0.8200 - top2metric: 0.9359 - top3metric: 0.9948 - val_loss: 0.3291 - val_acc: 0.8099 - val_top2metric: 0.9247 - val_top3metric: 0.9912\n",
      "Epoch 12/20\n",
      " - 211s - loss: 0.3760 - acc: 0.8267 - top2metric: 0.9389 - top3metric: 0.9951 - val_loss: 0.2985 - val_acc: 0.8852 - val_top2metric: 0.9616 - val_top3metric: 1.0000\n",
      "Epoch 13/20\n",
      " - 211s - loss: 0.3327 - acc: 0.8425 - top2metric: 0.9515 - top3metric: 0.9977 - val_loss: 0.3277 - val_acc: 0.7958 - val_top2metric: 0.9361 - val_top3metric: 1.0000\n",
      "Epoch 14/20\n",
      " - 211s - loss: 0.3294 - acc: 0.8432 - top2metric: 0.9531 - top3metric: 0.9979 - val_loss: 0.2989 - val_acc: 0.8535 - val_top2metric: 0.9745 - val_top3metric: 1.0000\n",
      "Epoch 15/20\n",
      " - 210s - loss: 0.3455 - acc: 0.8436 - top2metric: 0.9517 - top3metric: 0.9975 - val_loss: 0.2692 - val_acc: 0.8442 - val_top2metric: 0.9356 - val_top3metric: 1.0000\n",
      "Epoch 16/20\n",
      " - 210s - loss: 0.3169 - acc: 0.8544 - top2metric: 0.9609 - top3metric: 0.9985 - val_loss: 0.2884 - val_acc: 0.8987 - val_top2metric: 0.9704 - val_top3metric: 1.0000\n",
      "Epoch 17/20\n",
      " - 210s - loss: 0.3036 - acc: 0.8618 - top2metric: 0.9650 - top3metric: 0.9990 - val_loss: 0.2050 - val_acc: 0.9101 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 18/20\n",
      " - 210s - loss: 0.2809 - acc: 0.8718 - top2metric: 0.9703 - top3metric: 0.9991 - val_loss: 0.3039 - val_acc: 0.8088 - val_top2metric: 0.9055 - val_top3metric: 1.0000\n",
      "Epoch 19/20\n",
      " - 210s - loss: 0.2909 - acc: 0.8674 - top2metric: 0.9688 - top3metric: 0.9989 - val_loss: 0.1617 - val_acc: 0.9361 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 20/20\n",
      " - 210s - loss: 0.2845 - acc: 0.8690 - top2metric: 0.9698 - top3metric: 0.9990 - val_loss: 0.2199 - val_acc: 0.9138 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = test.train_CNN(train_x1,train_x2,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load and test synthetic test data.  also save model and results\n",
    "test.test_CNN(test_fraction=0.1)\n",
    "test.save_CNN('modular_CNN_Durmus_data_2018_06_21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500/5500 [==============================] - 15s 3ms/step\n",
      "model scored 72.727% on D:/jake/DDHO Data/displacement/2018_09_17/test2/0noise/*.npy\n",
      "one_diff_error = 9.09\n",
      "two_diff_error = 18.18\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [  0. 500. 500. 500.   0.   0.   0.   0.   0.   0.   0.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#load and test test data.  also save model and results\n",
    "\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2018_09_17/test2/0noise/*.npy\")\n",
    "#test.test_experimental_CNN(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\")\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run3\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run8\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run22\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run28\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "test.save_CNN('2018_09_17 simulated displacement train 0noise more complex architecture test 2018_09_12 tip1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500/5500 [==============================] - 15s 3ms/step\n",
      "model scored 27.272999999999996% on D:/jake/DDHO Data/displacement/2018_09_17/test/0noise/*.npy\n",
      "one_diff_error = 27.27\n",
      "two_diff_error = 9.09\n",
      "three_diff_error = 9.09\n",
      "which taus failed were: [  0. 500. 500. 500. 500. 500. 500. 500.   0.   0. 500.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#load and test test data.  also save model and results\n",
    "\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2018_09_17/test/0noise/*.npy\")\n",
    "#test.test_experimental_CNN(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\")\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run3\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run8\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run22\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run28\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "test.save_CNN('2018_09_17 simulated displacement train 0noise more complex architecture test 2018_09_17 tip6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4860565a683d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-3649c386eae0>\u001b[0m in \u001b[0;36mvisualize_weights\u001b[1;34m(self, layer_number)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;31m#layer number 0 = conv layer 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;31m#layer number 1 = ReLU 1\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m         \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_number\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlayer_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "test.visualize_weights(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/random_noise_1/*.npy\")\n",
    "#train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and test test data.  also save model and results\n",
    "\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/no_noise/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/0pt1noise/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/1noise/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/random_noise_1/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/random_noise_10/*.npy\")\n",
    "#test.test_experimental_CNN(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\")\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run3\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run8\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run22\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run28\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "test.save_CNN('2018_08_23 simulated displacement train rand noise 1 slow time only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.visualize_weights(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/no_noise/*.npy\")\n",
    "#train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load and test test data.  also save model and results\n",
    "\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/0pt1noise/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/1noise/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/random_noise_1/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/random_noise_10/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/no_noise/*.npy\")\n",
    "#test.test_experimental_CNN(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\")\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run3\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run8\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run22\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run28\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "test.save_CNN('2018_08_29 simulated inst_freq train no_noise 300 kHz 400 100 KL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.visualize_weights(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_vis = train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_to_vis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.layer_to_visualize(1, img_to_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.visualize_weights(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "\n",
    "train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         \"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and test experimental test data.  also save model and results\n",
    "test.test_experimental_CNN(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                          \"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                          \"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                          \"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\")\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run3\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run8\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run22\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run28\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "test.save_CNN('test_save_with_which_taus_failed_metric_2_20_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "#train_data = test.load_train_data()\n",
    "train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load and test synthetic test data.  also save model and results\n",
    "test.test_CNN(test_fraction=0.1)\n",
    "test.save_CNN('modular_CNN_Durmus_data_2018_06_21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and test experimental test data.  also save model and results\n",
    "test.test_experimental_CNN(\"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run29\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run30\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run31\")\n",
    "test.save_CNN('2018_08_14 train 30 test 29 30 31 slow timescale only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "#train_data = test.load_train_data()\n",
    "train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run1\",\n",
    "                                              \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run2\",\n",
    "                                              \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run29\",\n",
    "                                              \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load and test synthetic test data.  also save model and results\n",
    "test.test_CNN(test_fraction=0.1)\n",
    "test.save_CNN('modular_CNN_Durmus_data_2018_06_21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and test experimental test data.  also save model and results\n",
    "test.test_experimental_CNN(\"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run1\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run2\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run3\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run5\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run10\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run15\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run20\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run25\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run28\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run29\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run30\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run31\")\n",
    "test.save_CNN('2018_08_15 train 1 2 29 30 test many slow timescales only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and test experimental test data.  also save model and results\n",
    "test.test_experimental_CNN_averaged(\"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run5\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run6\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run7\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run8\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run9\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run10\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run20\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run30\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run40\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run43\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run44\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run45\")\n",
    "test.save_CNN('2018_07_19 train 5_6_7_43_44_45 test averaged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "#train_data = test.load_train_data()\n",
    "train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run1\",\n",
    "                                              \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run2\",\n",
    "                                              \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run3\")\n",
    "                                         #\"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run6\",\n",
    "                                         #\"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run7\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load and test synthetic test data.  also save model and results\n",
    "test.test_CNN(test_fraction=0.1)\n",
    "test.save_CNN('modular_CNN_Durmus_data_2018_06_21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load and test experimental test data.  also save model and results\n",
    "test.test_experimental_CNN(\"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run1\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run2\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run3\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run4\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run5\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run6\")\n",
    "test.save_CNN('2018_08_01 first test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = test.model.layers[3].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_plt = weights[:,:,0]\n",
    "weight_plt.shape\n",
    "#weight_plt2 = weight_plt.reshape((400,))\n",
    "#plt.plot(weight_plt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = test.model.layers[1].get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

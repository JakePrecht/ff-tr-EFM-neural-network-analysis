{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from igor.binarywave import load as loadibw\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Conv1D, MaxPooling1D, Flatten, Merge\n",
    "from keras.utils import np_utils\n",
    "from keras import callbacks\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class CNN_train():\n",
    "    def load_simulated_train_data(self, *paths):\n",
    "        #Example train_path: \"D:/jake/DDHO Data/displacement/10us/random_noise_10/*.npy\"\n",
    "        #Example train_path: \"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\"\n",
    "        \n",
    "        #init df for collecting each run\n",
    "        df_main = pd.DataFrame()\n",
    "        \n",
    "        for i in paths:\n",
    "            \n",
    "            #grab a list of strings to every file path from the train_path folder\n",
    "            file_path = glob.glob(i)\n",
    "        \n",
    "            test_pixel = np.load(file_path[0])\n",
    "            #this will be used in for loop below\n",
    "            #it is just used to grab the length of any one displacement curve\n",
    "        \n",
    "            #initialize DataFrame column names list (name for each feature point of NN)\n",
    "            columns=[]\n",
    "            for j in range(len(test_pixel)-4):#-4 because k,Q,omega,tau is at end of pixel data\n",
    "                columns.append('t='+str(j))   #most columns are just time points and I'm making them here\n",
    "    \n",
    "            #columns.append('Tfp') #because tfps are appended onto the 2nd to last column of my inst_freq data.  Uncomment for Inst_freq data!\n",
    "            columns.append('k')\n",
    "            columns.append('Q')\n",
    "            columns.append('omega')\n",
    "            columns.append('Tau') #because taus are appended onto the end of my data\n",
    "    \n",
    "    \n",
    "            #load all of the data into an array for input into DataFrame\n",
    "            #each entry in the \"data1\" array is a numpy array of a displacement curve\n",
    "            data1 = [np.load(file_path[i]) for i in range(0,(len(file_path)))]\n",
    "    \n",
    "            #make df for output\n",
    "            train_data = pd.DataFrame(data=data1,columns=columns,dtype=np.float64)\n",
    "            train_data = train_data.drop('t=0',axis=1)\n",
    "            #these t=0 points end up as just NaNs after preprocessing and are useless anyways because by definition freq shift is 0 at trigger for all points\n",
    "            #dropping t=0 maybe unnecessary with displacement data?\n",
    "            \n",
    "            df_main = pd.concat([df_main,train_data], ignore_index=True) #append each run to the final collection DataFrame\n",
    "        \n",
    "        df_main = shuffle(df_main) #shuffle data because it is currently ordered and this could impact NN learning\n",
    "        \n",
    "        return df_main\n",
    "    \n",
    "    def load_experimental_train_data(self, *paths):\n",
    "        \"\"\"Path should be to Runx folder:\n",
    "        E.g.\n",
    "        path = \"C:/Users/Jake/Desktop/75khz RSI data/Run1\" for my pc or\n",
    "        path = \"D:/Jake/DDHO data/durmus_data/75khz RSI data/Run1\" for simulation computer\"\"\"\n",
    "        #taus = np.array([10e-9, 25e-9, 50e-9, 100e-9, 250e-9, 500e-9,\n",
    "        #                        1e-6, 5e-6, 10e-6, 100e-6, 1e-3])\n",
    "        taus= np.array([10e-9, 31.62e-9, 100e-9, 316.2e-9, 1e-6, 3.162e-6, 10e-6, 31.62e-6, 100e-6, 316.2e-6, 1e-3])\n",
    "        #below code assumes data array has trigger at 16384 and total points 16384*2 (this is what all my and Durmus' data is)\n",
    "        \n",
    "        #init df for collecting each run\n",
    "        df_main = pd.DataFrame()\n",
    "        \n",
    "        for i in paths:\n",
    "            \n",
    "            tau_paths = glob.glob(i + \"/*\")\n",
    "            sorted_tau_paths = sorted(tau_paths, key = lambda x: int(os.path.basename(os.path.normpath(x))))\n",
    "            #Above line properly sorts so that they are ordered 0,1,2,3,4,5,6,7,8,9,10 rather than 0,1,10,2,3,4...\n",
    "    \n",
    "            #Init dataframe for this run\n",
    "            df_run = pd.DataFrame()\n",
    "            \n",
    "            #Loop through every tau in that run\n",
    "            for j in range(4,len(tau_paths)):\n",
    "                tau = taus[j]\n",
    "                displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "                #collect relevent parts of the real data into df for that run\n",
    "                for k in range(len(displacement_path)):\n",
    "                    disp_array = loadibw(displacement_path[k])['wave']['wData'] #load displacement from .ibw\n",
    "                    #throw away all displacement before the trigger (16384 pre-trigger points)\n",
    "                    disp_array = disp_array[16384:,:]\n",
    "                    disp_array = np.transpose(disp_array)\n",
    "        \n",
    "                    #Put loaded stuff into dataframe and label tau\n",
    "                    columns=[]\n",
    "                    for l in range(disp_array.shape[1]):\n",
    "                        columns.append('t='+str(l))\n",
    "            \n",
    "                    df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "                    df_temp['Tau'] = pd.Series(index=df_temp.index) #create Tau column\n",
    "                    df_temp['Tau'] = tau #assign tau value to tau column (could probably be done in above step with data=tau?)\n",
    "                    \n",
    "                    df_run = df_run.append(df_temp,ignore_index=True) #append each tau value to this run\n",
    "                    #df_run = pd.concat([df_run,df_temp],ignore_index=True)\n",
    "            \n",
    "            \n",
    "            df_main = pd.concat([df_main,df_run], ignore_index=True) #append each run to the final collection DataFrame\n",
    "            \n",
    "        \n",
    "        df_main = shuffle(df_main) #shuffle data because it is currently ordered and this could impact NN learning\n",
    "        \n",
    "        return df_main\n",
    "    \n",
    "        \n",
    "    \n",
    "    def preprocess_train_data(self,train_data):\n",
    "        #Prep training data\n",
    "\n",
    "        num_samples = len(train_data)\n",
    "        train_x = train_data[0:num_samples+1] #this syntax is an artifact from when I was training with partial data sets but it doesn't really add much timing loss so I'm leaving it in case I need to change it again\n",
    "        \n",
    "        train_x1 = train_x.drop(['k','Q','omega','Tau'],axis=1)\n",
    "        \n",
    "        train_x2 = train_x[['k','Q','omega']]\n",
    "        \n",
    "        #train_x = train_x.drop('Tau',axis=1) #dropping Tau because we do not input Tau to the neural network (that's like giving it the solution and then asking for the solution--it cheats)\n",
    "        #train_x = train_x.drop('Tfp',axis=1) #for simulations only\n",
    "        \n",
    "        self.mean_train_x1 = np.mean(train_x1) #saving the mean_train_x for preprocessing the test data in the same manner as our training dat\n",
    "        self.mean_train_x2 = np.mean(train_x2)\n",
    "        \n",
    "        self.SD_train_x1 = np.std(train_x1) #saving the SD_train_x for preprocessing the test data in the same manner as our training data\n",
    "        self.SD_train_x2 = np.std(train_x2)\n",
    "        \n",
    "        train_x1_norm = (train_x1 - self.mean_train_x1) /  (self.SD_train_x1) #normalize and centralize the training data for best neural network performance\n",
    "        train_x1_norm_reshaped = np.expand_dims(train_x1_norm,axis=2) #formatting for input into CNN\n",
    "        \n",
    "        train_x2_norm = (train_x2 - self.mean_train_x2) /  (self.SD_train_x2) #normalize and centralize the training data for best neural network performance\n",
    "        train_x2_norm_reshaped = np.expand_dims(train_x2_norm,axis=2) #formatting for input into CNN\n",
    "        \n",
    "        train_y = np.array(train_data['Tau']) #labeled, true Tau values for the CNN to learn from\n",
    "        train_y = train_y[0:num_samples+1] #this syntax is an artifact from when I was training with partial data sets but it doesn't really add much timing loss so I'm leaving it in case I need to change it again\n",
    "        \n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "\n",
    "        #tau_index is used to recover the original tau's from a one-hot encoded output.\n",
    "        #e.g. tau = [10, 100, 1000, 10, 10] then\n",
    "        #unique_tau = [10, 100, 1000]\n",
    "        #tau_index = [0,1,2,0,0] is index of tau to corresponding unique_tau so\n",
    "        #unique_tau[tau_index] == tau \n",
    "        unique_tau, tau_index = np.unique(train_y,return_inverse=True)\n",
    "\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "\n",
    "        self.number_of_classes = one_hot_tau.shape[1] #used to match number of output Softmax layers in my NN\n",
    "        \n",
    "        return train_x1_norm_reshaped, train_x2_norm_reshaped, one_hot_tau\n",
    "    \n",
    "    def train_CNN(self, train_x1, train_x2, train_y, num_epochs = 40, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3):\n",
    "        #Build CNN and start training!\n",
    "\n",
    "        self.filter_number1 = num_filter1\n",
    "        self.filter_number2 = num_filter2\n",
    "        self.kernel1_size = kernel1_size\n",
    "        self.kernel2_size = kernel2_size\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Initialize CNN branch 1 for main convolutional input data (displacement or instantaneous frequency)\n",
    "        branch1 = Sequential()\n",
    "\n",
    "        #Add convolution layers\n",
    "        branch1.add(Conv1D(filters=num_filter1,kernel_size=kernel1_size,strides=2,padding='same',input_shape=(train_x1.shape[1],1)))\n",
    "        branch1.add(LeakyReLU(alpha=0.01))\n",
    "        branch1.add(MaxPooling1D())\n",
    "\n",
    "        branch1.add(Conv1D(filters=num_filter2,kernel_size=kernel2_size,strides=2,padding='same'))\n",
    "        branch1.add(LeakyReLU(alpha=0.01))\n",
    "        branch1.add(MaxPooling1D())\n",
    "\n",
    "        branch1.add(Flatten())\n",
    "        #Roughly 500 units length of branch 1 (8000 displacement points / (2**4 because each strides = 2 and each maxpool halves data length))\n",
    "        \n",
    "        branch1.add(Dense(units=100, kernel_initializer='he_normal',activation='linear'))\n",
    "        branch1.add(LeakyReLU(alpha=.01))\n",
    "        branch1.add(Dropout(0.3))\n",
    "\n",
    "        branch1.add(Dense(units=100, kernel_initializer='he_normal',activation='linear'))\n",
    "        branch1.add(LeakyReLU(alpha=.01))\n",
    "        branch1.add(Dropout(0.4))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Initialize CNN branch 2 for supplementary data (Q, k, and omega)\n",
    "        branch2 = Sequential()\n",
    "\n",
    "        #Add supplementary data inputs\n",
    "        branch2.add(Dense(units=100, kernel_initializer='he_normal', activation='linear', input_shape=(train_x2.shape[1],1)))\n",
    "        branch2.add(LeakyReLU(alpha=.01))\n",
    "        branch2.add(Dropout(0.3))\n",
    "        \n",
    "        branch2.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        branch2.add(LeakyReLU(alpha=.01))\n",
    "        branch2.add(Dropout(0.4))\n",
    "        \n",
    "        branch2.add(Flatten())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Merge branches 1 and 2\n",
    "        model = Sequential()\n",
    "        model.add(Merge([branch1,branch2], mode='concat'))\n",
    "\n",
    "        \n",
    "        #Add final fully connected layers\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        #Add classification layer\n",
    "        model.add(Dense(units=self.number_of_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "        #Compile CNN and configure metrics/learning process\n",
    "        \n",
    "        \n",
    "        \"\"\"below functions are failure metrics that tell me if the true tau was in the top 2, top 3, or top 5 guesses made by the neural network\"\"\"\n",
    "        def inTop2(k=2):\n",
    "            def top2metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=2)\n",
    "            return top2metric\n",
    "        \n",
    "        def inTop3(k=3):\n",
    "            def top3metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=3)\n",
    "            return top3metric\n",
    "        \n",
    "        def inTop5(k=5):\n",
    "            def top5metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=5)\n",
    "            return top5metric\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy', inTop2(), inTop3()])\n",
    "\n",
    "        #Prepare for visualization\n",
    "        #tbCallBack = callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "        #tbCallBack = callbacks.TensorBoard(log_dir=\"logs/{}\".format(time.time()))\n",
    "\n",
    "        #Train model\n",
    "        model.fit([train_x1, train_x2], train_y, batch_size=32, epochs=num_epochs,verbose=2, validation_split=0.05)#, callbacks=[tbCallBack])\n",
    "        self.model = model #save model to self for calling from other functions later\n",
    "        self.branch1 = branch1\n",
    "        self.branch2 = branch2\n",
    "        return\n",
    "    \n",
    "    def load_simulated_test_data(self, file_path):#, test_fraction):\n",
    "        \"\"\"input string with file path (e.g. \"D:/jake/DDHO Data/inst_freq/25us/0pt1noise/*.npy\" )\n",
    "        and also input the fraction of the data you wish to test (e.g. testing 10% of data would be 0.1)\n",
    "    \n",
    "        outputs the requested percentage of the data (test_data_norm_reshaped) and their corresponding labels for evaluation (one_hot_tau)\n",
    "        these outputs are basically test_x and test_y that are formatted to be fed into model.evaluate()\n",
    "    \n",
    "        NOTE this function requires previous cells to have been run (train_x must exist!!!)\"\"\"\n",
    "        file_path1 = glob.glob(file_path)\n",
    "        test_pixel = np.load(file_path1[0])\n",
    "        #this will be used in for loop below\n",
    "        #it is just used to grab the length of any one inst. freq. curve\n",
    "    \n",
    "        columns2=[]\n",
    "        for i in range(len(test_pixel)-4):#-4 because k,Q,omega, tau is at end of pixel data\n",
    "        #for i in range(len(test_pixel)-2):#-2 because tfp and tau are at end of pixel data\n",
    "            columns2.append('t='+str(i))#most columns are just time points and I'm making them here\n",
    "    \n",
    "        #columns2.append('Tfp') #because tfps are appended onto the end of my inst_freq data\n",
    "        columns2.append('k')\n",
    "        columns2.append('Q')\n",
    "        columns2.append('omega')\n",
    "        columns2.append('Tau') #because taus are appended onto the end of my inst_freq data\n",
    "    \n",
    "        #alternate way to load only a fraction of the data to save memory and time\n",
    "        #num_samples = len(file_path1)\n",
    "        #num_buckets = self.number_of_classes\n",
    "        #bucket_range = int(num_samples/num_buckets)\n",
    "        #test_fraction_range = int(test_fraction * bucket_range)\n",
    "        #load_list = []\n",
    "\n",
    "        #for i in range(num_buckets):\n",
    "        #    for j in range(test_fraction_range):\n",
    "        #        load_list.append(int((i * bucket_range) + (j)))\n",
    "    \n",
    "        #data1 = [np.load(file_path1[i]) for i in load_list]\n",
    "        data1 = [np.load(file_path1[i]) for i in range(len(file_path1))]\n",
    "\n",
    "        #make df for output\n",
    "        test_data = pd.DataFrame(data=data1,columns=columns2,dtype=np.float64)\n",
    "        test_data = test_data.drop('t=0',axis=1)\n",
    "        #these t=0 points end up as just NaNs and are useless anyways because by definition freq shift is 0 at trigger\n",
    "        test_data = shuffle(test_data)\n",
    "        #shuffle data because it is currently ordered and this could impact NN learning\n",
    "\n",
    "        test_y = np.array(test_data['Tau'])\n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "        unique_tau, tau_index = np.unique(test_y,return_inverse=True)\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "    \n",
    "        #preprocess test_x\n",
    "        test_x1 = test_data.drop(['k','Q','omega','Tau'],axis=1) #x1 is just displacement\n",
    "        test_x2 = test_data[['k','Q','omega']]\n",
    "        \n",
    "        test_x1_norm = (test_x1 - self.mean_train_x1 ) /  (self.SD_train_x1) #important to preprocess my test data same as my train data!!!!\n",
    "        test_x2_norm = (test_x2 - self.mean_train_x2 ) /  (self.SD_train_x2) #important to preprocess my test data same as my train data!!!!\n",
    "        \n",
    "        test_x1_norm_reshaped = np.expand_dims(test_x1_norm,axis=2)\n",
    "        test_x2_norm_reshaped = np.expand_dims(test_x2_norm,axis=2)\n",
    "        \n",
    "        \n",
    "        return test_x1_norm_reshaped, test_x2_norm_reshaped, one_hot_tau\n",
    "    \n",
    "    \n",
    "    def load_experimental_test_data(self, path):\n",
    "        \"\"\"input string with file path\n",
    "    \n",
    "        outputs the requested percentage of the data (test_data_norm_reshaped) and their corresponding labels for evaluation (one_hot_tau)\n",
    "        these outputs are basically test_x and test_y that are formatted to be fed into model.evaluate()\n",
    "    \n",
    "        NOTE this function requires previous cells to have been run (train_x must exist!!!)\"\"\"\n",
    "        #taus = np.array([10e-9, 25e-9, 50e-9, 100e-9, 250e-9, 500e-9,\n",
    "        #                        1e-6, 5e-6, 10e-6, 100e-6, 1e-3])\n",
    "        taus= np.array([10e-9, 31.62e-9, 100e-9, 316.2e-9, 1e-6, 3.162e-6, 10e-6, 31.62e-6, 100e-6, 316.2e-6, 1e-3])\n",
    "        \n",
    "        \n",
    "        tau_paths = glob.glob(path + \"/*\")\n",
    "        sorted_tau_paths = sorted(tau_paths, key = lambda x: int(os.path.basename(os.path.normpath(x))))\n",
    "        #Above line properly sorts so that they are ordered 0,1,2,3,4,5,6,7,8,9,10 rather than 0,1,10,2,3,4...\n",
    "    \n",
    "        #Init dataframe for this run\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        for j in range(4,len(tau_paths)):\n",
    "            tau = taus[j]\n",
    "            displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "            #print(displacement_path)\n",
    "            #print(j)\n",
    "            #print('the above two should be matching indices')\n",
    "            \n",
    "            disp_array = loadibw(displacement_path[0])['wave']['wData']\n",
    "            #throw away all displacement before the trigger\n",
    "            disp_array = disp_array[16384:,:]\n",
    "            disp_array = np.transpose(disp_array)\n",
    "        \n",
    "            #Put loaded stuff into dataframe and label tau\n",
    "            columns=[]\n",
    "            for k in range(disp_array.shape[1]):\n",
    "                columns.append('t='+str(k))\n",
    "        \n",
    "            df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "            df_temp['Tau'] = pd.Series(index=df_temp.index)\n",
    "            df_temp['Tau'] = tau\n",
    "            df = df.append(df_temp,ignore_index=True)\n",
    "       \n",
    "        \"\"\"for j in range(len(tau_paths)):\n",
    "            tau = taus[j]\n",
    "            displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "            print(displacement_path)\n",
    "            #print(j)\n",
    "            #print('the above two should be matching indices')\n",
    "            \n",
    "            for i in range(len(displacement_path)):\n",
    "                disp_array = loadibw(displacement_path[i])['wave']['wData']\n",
    "                #throw away all displacement before the trigger\n",
    "                disp_array = disp_array[16384:,:]\n",
    "                disp_array = np.transpose(disp_array)\n",
    "        \n",
    "                #Put loaded stuff into dataframe and label tau\n",
    "                columns=[]\n",
    "                for k in range(disp_array.shape[1]):\n",
    "                    columns.append('t='+str(k))\n",
    "        \n",
    "                df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "                df_temp['Tau'] = pd.Series(index=df_temp.index)\n",
    "                df_temp['Tau'] = tau\n",
    "                df = df.append(df_temp,ignore_index=True)\n",
    "            \n",
    "        df = shuffle(df) #shuffle data because it is currently ordered and this could impact NN learning\n",
    "        \"\"\"\n",
    "        \n",
    "        df = shuffle(df) #shuffle data because it is currently ordered and this could impact NN learning\n",
    "        \n",
    "        test_y = np.array(df['Tau'])\n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "        unique_tau, tau_index = np.unique(test_y,return_inverse=True)\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "    \n",
    "        #preprocess test_x\n",
    "        df = df.drop('Tau',axis=1)\n",
    "        #test_data = test_data.drop('Tfp',axis=1)\n",
    "        df_norm = (df - self.mean_train_x ) /  (self.SD_train_x) #important to preprocess my test data same as my train data!!!!\n",
    "        df_norm_reshaped = np.expand_dims(df_norm,axis=2)\n",
    "    \n",
    "        return df_norm_reshaped, one_hot_tau\n",
    "    \n",
    "    \n",
    "    def load_experimental_test_data_averaged(self, path):\n",
    "        \"\"\"input string with file path\n",
    "    \n",
    "        outputs the requested percentage of the data (test_data_norm_reshaped) and their corresponding labels for evaluation (one_hot_tau)\n",
    "        these outputs are basically test_x and test_y that are formatted to be fed into model.evaluate()\n",
    "    \n",
    "        NOTE this function requires previous cells to have been run (train_x must exist!!!)\"\"\"\n",
    "        #This function is unused and not useful\n",
    "        #taus = np.array([10e-9, 25e-9, 50e-9, 100e-9, 250e-9, 500e-9,\n",
    "        #                        1e-6, 5e-6, 10e-6, 100e-6, 1e-3])\n",
    "        taus= np.array([10e-9, 31.62e-9, 100e-9, 316.2e-9, 1e-6, 3.162e-6, 10e-6, 31.62e-6, 100e-6, 316.2e-6, 1e-3])\n",
    "        \n",
    "        \n",
    "        tau_paths = glob.glob(path + \"/*\")\n",
    "        sorted_tau_paths = sorted(tau_paths, key = lambda x: int(os.path.basename(os.path.normpath(x))))\n",
    "        #Above line properly sorts so that they are ordered 0,1,2,3,4,5,6,7,8,9,10 rather than 0,1,10,2,3,4...\n",
    "    \n",
    "        #Init dataframe for this run\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        for j in range(len(tau_paths)):\n",
    "            tau = taus[j]\n",
    "            displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "            #print(displacement_path)\n",
    "            #print(j)\n",
    "            #print('the above two should be matching indices')\n",
    "            \n",
    "            disp_array = loadibw(displacement_path[0])['wave']['wData']\n",
    "            #throw away all displacement before the trigger\n",
    "            disp_array = disp_array[16384:,:]\n",
    "            disp_array = np.transpose(disp_array)\n",
    "        \n",
    "            #Put loaded stuff into dataframe and label tau\n",
    "            columns=[]\n",
    "            for k in range(disp_array.shape[1]):\n",
    "                columns.append('t='+str(k))\n",
    "        \n",
    "            df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "            df_temp['Tau'] = pd.Series(index=df_temp.index)\n",
    "            df_temp['Tau'] = tau\n",
    "            \n",
    "            df_temp = df_temp.mean(axis=0)\n",
    "            \n",
    "            df = df.append(df_temp,ignore_index=True)\n",
    "        \n",
    "        #df = shuffle(df) #shuffle data because it is currently ordered and this could impact NN learning\n",
    "        \n",
    "        test_y = np.array(df['Tau'])\n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "        unique_tau, tau_index = np.unique(test_y,return_inverse=True)\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "    \n",
    "        #preprocess test_x\n",
    "        df = df.drop('Tau',axis=1)\n",
    "        #test_data = test_data.drop('Tfp',axis=1)\n",
    "        df_norm = (df - self.mean_train_x ) /  (self.SD_train_x) #important to preprocess my test data same as my train data!!!!\n",
    "        df_norm_reshaped = np.expand_dims(df_norm,axis=2)\n",
    "    \n",
    "        return df_norm_reshaped, one_hot_tau\n",
    "    \n",
    "    \n",
    "    def test_closeness(self, test_x1, test_x2, test_y):\n",
    "        \"\"\"This function looks at the predicted tau values from model.predict(test_x) and compares them \n",
    "        to the true tau values from test_y.  \n",
    "        It then returns three values telling you what percentage of the incorrect predictions varied by spacing of\n",
    "        one tau value, two tau values, or three tau values.\n",
    "        \n",
    "        E.G.\n",
    "        Say possible taus = [1,2,3,4,5,6,7,8,9]\n",
    "        model.predict(test_x) = [2,2,2,3,3,3,4,4,5,6]\n",
    "        test_y = [2,2,2,2,2,2,2,2,2,2]\n",
    "        \n",
    "        test_closeness returns [0.3,0.2,0.1]\n",
    "        because 30% of the predictions varied by one tau value (tau = 2 but 3 times it guessed tau = 3)\n",
    "        because 20% of the predictions varied by two tau values (tau = 2 but 2 times it guessed tau = 4)\n",
    "        because 10% of the predictions varied by three tau values (tau = 2 but 1 time it guessed tau = 5)\n",
    "        \"\"\"\n",
    "        \n",
    "        pred_tau = self.model.predict([test_x1,test_x2],verbose=0)\n",
    "        \n",
    "        pred_tau_am = pred_tau.argmax(axis=-1) #pluck out actual prediction value!\n",
    "        test_y_am = test_y.argmax(axis=-1) #does not actually need argmax, but this makes it same format as pred_tau_am\n",
    "        \n",
    "        incorrect_indices = np.nonzero(pred_tau_am != test_y_am) #indices of incorrect predictions\n",
    "        \n",
    "        total_samples = len(pred_tau)\n",
    "        total_fails = len(incorrect_indices[0])\n",
    "        \n",
    "        #init diff collection variables (how many tau values away the true value was from the predicted value)\n",
    "        num_diff_1 = 0\n",
    "        num_diff_2 = 0\n",
    "        num_diff_3 = 0\n",
    "        num_greater = 0\n",
    "        \n",
    "        #init array for seeing which taus it is bad at predicting\n",
    "        #which_taus_failed = np.zeros(11) #CHANGE THIS HARD-CODED 11 TO WHATEVER THE NUMBER OF CLASSES IS!!!\n",
    "        which_taus_failed = np.zeros(self.number_of_classes) #CHANGE THIS HARD-CODED 11 TO WHATEVER THE NUMBER OF CLASSES IS!!!\n",
    "        \n",
    "        for element in incorrect_indices[0]:\n",
    "            \n",
    "            #collect diff (how many tau values away the true value was from the predicted value)\n",
    "            diff = abs(pred_tau_am[element] - test_y_am[element])\n",
    "            if diff == 1:\n",
    "                num_diff_1 += 1\n",
    "            elif diff == 2:\n",
    "                num_diff_2 += 1\n",
    "            elif diff == 3:\n",
    "                num_diff_3 += 1\n",
    "            else:\n",
    "                num_greater += 1\n",
    "            \n",
    "            #collect how many of each tau failed\n",
    "            i=0\n",
    "            while True:\n",
    "                if test_y_am[element] == i:\n",
    "                    which_taus_failed[i] += 1\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "                    \n",
    "            which_taus_failed_percent = np.round((which_taus_failed / total_fails),4) * 100\n",
    "\n",
    "        \n",
    "        percent_num_diff_1 = round((num_diff_1 / total_samples), 4) * 100\n",
    "        percent_num_diff_2 = round((num_diff_2 / total_samples), 4) * 100\n",
    "        percent_num_diff_3 = round((num_diff_3 / total_samples), 4) * 100\n",
    "        percent_num_diff_greater = round((num_greater / total_samples), 4) * 100\n",
    "            \n",
    "        #Next section is for debugging purposes\n",
    "        #percent_incorrect = (len(incorrect_indices[0])/total_samples)\n",
    "        #percent_incorrect_calculated = percent_num_diff_1 + percent_num_diff_2 + percent_num_diff_3 + percent_num_diff_greater\n",
    "        #print('percent incorrect should be ' + str(percent_incorrect))\n",
    "        #print('percent incorrect calculated is ' + str(percent_incorrect_calculated))\n",
    "        \n",
    "        return percent_num_diff_1, percent_num_diff_2, percent_num_diff_3, which_taus_failed#_percent\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def test_simulated_CNN(self, *paths):\n",
    "        \n",
    "        score_string = 'data order is testing against '\n",
    "        for element in paths:\n",
    "            score_string += (str(element) + \" , \\n\") \n",
    "\n",
    "        \n",
    "        score_collect = [score_string]\n",
    "        score_collect.append('column order is loss, accuracy, top2metric, top3metric')\n",
    "        score_collect.append('top2metric = % that the true tau was one of the top 2 predictions')\n",
    "        score_collect.append(' ')\n",
    "        \n",
    "        \n",
    "        #score_collect = ['data order is no_noise, 0pt1noise, 1noise, random_noise_1, random_noise_10']\n",
    "        #score_collect.append('first column is loss, second column is accuracy')\n",
    "        \n",
    "        for i in paths:\n",
    "            test_x1, test_x2, test_y = self.load_simulated_test_data(i)#,test_fraction)\n",
    "            score = self.model.evaluate([test_x1,test_x2],test_y, batch_size=32)\n",
    "            percentage = str(round(score[1],5) * 100)\n",
    "            print('model scored ' + percentage + '% on ' + str(i))\n",
    "            score_collect.append(str(score))\n",
    "            \n",
    "            error1, error2, error3, which_taus_failed = self.test_closeness(test_x1,test_x2,test_y)\n",
    "            score_collect.append('one_diff_error = ' + str(error1))\n",
    "            score_collect.append('two_diff_error = ' + str(error2))\n",
    "            score_collect.append('three_diff_error = ' + str(error3))\n",
    "            score_collect.append('which taus failed were: ' + str(which_taus_failed))\n",
    "            print('one_diff_error = ' + str(error1))\n",
    "            print('two_diff_error = ' + str(error2))\n",
    "            print('three_diff_error = ' + str(error3))\n",
    "            print('which taus failed were: ' + str(which_taus_failed))\n",
    "            print(' ')\n",
    "            \n",
    "            score_collect.append('above scores were for ' + str(element)) #new code on 7/2/18\n",
    "            score_collect.append(' ')\n",
    "        \n",
    "        self.score_collect = score_collect\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def test_experimental_CNN(self, *paths):\n",
    "        \n",
    "        score_string = 'data order is testing against '\n",
    "        for element in paths:\n",
    "            score_string += (str(element) + \" , \\n\") \n",
    "\n",
    "        \n",
    "        score_collect = [score_string]\n",
    "        score_collect.append('column order is loss, accuracy, top2metric, top3metric')\n",
    "        score_collect.append('top2metric = % that the true tau was one of the top 2 predictions')\n",
    "        score_collect.append(' ')\n",
    "        \n",
    "        for element in paths:\n",
    "            test_x1, test_x2, test_y = self.load_experimental_test_data(element)\n",
    "            score = self.model.evaluate([test_x1,test_x2],test_y,batch_size = 32)\n",
    "            percentage = str(round(score[1],5) * 100)\n",
    "            print('model scored ' + percentage + '% on ' + str(element))\n",
    "            score_collect.append(str(score))\n",
    "            #score_collect.append('above score was for ' + str(element)) #new code on 7/2/18\n",
    "            \n",
    "            error1, error2, error3, which_taus_failed = self.test_closeness(test_x,test_y)\n",
    "            score_collect.append('one_diff_error = ' + str(error1))\n",
    "            score_collect.append('two_diff_error = ' + str(error2))\n",
    "            score_collect.append('three_diff_error = ' + str(error3))\n",
    "            score_collect.append('which taus failed were: ' + str(which_taus_failed))\n",
    "            print('one_diff_error = ' + str(error1))\n",
    "            print('two_diff_error = ' + str(error2))\n",
    "            print('three_diff_error = ' + str(error3))\n",
    "            print('which taus failed were: ' + str(which_taus_failed))\n",
    "            print(' ')\n",
    "            \n",
    "            score_collect.append('above scores were for ' + str(element)) #new code on 7/2/18\n",
    "            score_collect.append(' ')\n",
    "            \n",
    "        \n",
    "        self.score_collect = score_collect\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def test_experimental_CNN_averaged(self, *paths):\n",
    "        \n",
    "        score_string = 'data order is testing against '\n",
    "        for element in paths:\n",
    "            score_string += (str(element) + \" , \\n\") \n",
    "\n",
    "        \n",
    "        score_collect = [score_string]\n",
    "        score_collect.append('column order is loss, accuracy, top2metric, top3metric')\n",
    "        score_collect.append('top2metric = % that the true tau was one of the top 2 predictions')\n",
    "        score_collect.append(' ')\n",
    "        \n",
    "        for element in paths:\n",
    "            test_x, test_y = self.load_experimental_test_data_averaged(element)\n",
    "            score = self.model.evaluate(test_x,test_y,batch_size = 32)\n",
    "            percentage = str(round(score[1],5) * 100)\n",
    "            print('model scored ' + percentage + '% on ' + str(element))\n",
    "            score_collect.append(str(score))\n",
    "            #score_collect.append('above score was for ' + str(element)) #new code on 7/2/18\n",
    "            \n",
    "            #more new code currently testing\n",
    "            error1, error2, error3, which_taus_failed = self.test_closeness(test_x,test_y)\n",
    "            score_collect.append('one_diff_error = ' + str(error1))\n",
    "            score_collect.append('two_diff_error = ' + str(error2))\n",
    "            score_collect.append('three_diff_error = ' + str(error3))\n",
    "            score_collect.append('which taus failed were: ' + str(which_taus_failed))\n",
    "            print('one_diff_error = ' + str(error1))\n",
    "            print('two_diff_error = ' + str(error2))\n",
    "            print('three_diff_error = ' + str(error3))\n",
    "            print('which taus failed were: ' + str(which_taus_failed))\n",
    "            print(' ')\n",
    "            \n",
    "            score_collect.append('above scores were for ' + str(element)) #new code on 7/2/18\n",
    "            score_collect.append(' ')\n",
    "            \n",
    "        \n",
    "        self.score_collect = score_collect\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def save_CNN(self, save_str):\n",
    "        #save model and test evaluation outputs\n",
    "        #example save_str: save_str = 'displacement_10us_random_noise_10_2018_06_13_80epoch'\n",
    "        #requires test_CNN to have been run already\n",
    "        path = 'C:/Users/jakeprecht/DDHO/saved CNN models/'\n",
    "        save_str_h5 = path + save_str + '.h5'\n",
    "        save_str_txt = path + save_str + '_results.txt'\n",
    "        save_str_weights = path + save_str + '_weights.h5'\n",
    "        \n",
    "        self.model.save(save_str_h5)  # creates a HDF5 file 'save_str_h5.h5'\n",
    "        self.model.save_weights(save_str_weights)\n",
    "        \n",
    "        output_scores = open(save_str_txt, 'w')\n",
    "        for item in self.score_collect:\n",
    "            output_scores.write(\"%s\\n\" % item)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def visualize_weights(self, layer_number):\n",
    "        #layer number 0 = conv layer 1\n",
    "        #layer number 1 = ReLU 1\"\"\"\n",
    "        weights, biases = self.branch1.layers[layer_number].get_weights()\n",
    "        \n",
    "        if layer_number == 0 or 1:\n",
    "            number_filters = self.filter_number1\n",
    "            kernel_length = self.kernel1_size         \n",
    "\n",
    "        #elif layer_number == 3:\n",
    "        #    number_filters = self.filter_number2\n",
    "        #    kernel_length = self.kernel2_size\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Input for layer_number must be 0 or 3 in current implementation (2018_08_08)\")\n",
    "            \n",
    "        fig = plt.figure()\n",
    "        for i in range(number_filters):\n",
    "            weight_plt = weights[:,:,i]\n",
    "            weight_plt2 = weight_plt.reshape((kernel_length,))\n",
    "            #ax = fig.add_subplot(number_filters,1,i+1)\n",
    "            plt.figure()\n",
    "            plt.plot(weight_plt2)\n",
    "            #ax.imshow(weight_plt2,cmap='gray')\n",
    "            \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def layer_to_visualize(self, layer, img_to_visualize):\n",
    "        \"\"\"img_to_visualize = train_x[image_number]\n",
    "        this code does not work yet\n",
    "        \"\"\"\n",
    "        layer = self.model.layers[layer]\n",
    "        img_to_visualize = np.expand_dims(img_to_visualize, axis=0)\n",
    "        \n",
    "        inputs = [K.learning_phase()] + self.model.inputs\n",
    "\n",
    "        _convout1_f = K.function(inputs, [layer.output])\n",
    "        def convout1_f(X):\n",
    "            # The [0] is to disable the training phase flag\n",
    "            return _convout1_f([0] + [X])\n",
    "\n",
    "        convolutions = convout1_f(img_to_visualize)\n",
    "        convolutions = np.squeeze(convolutions)\n",
    "\n",
    "        print ('Shape of conv:', convolutions.shape)\n",
    "    \n",
    "        n = convolutions.shape[0]\n",
    "        n = int(np.ceil(np.sqrt(n)))\n",
    "    \n",
    "        # Visualization of each filter of the layer\n",
    "        fig = plt.figure(figsize=(12,8))\n",
    "        for i in range(len(convolutions)):\n",
    "            ax = fig.add_subplot(n,n,i+1)\n",
    "            ax.imshow(convolutions[i], cmap='gray')\n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "#number_of_classes = one_hot_tau.shape[1] #used to match number of output Softmax layers in my NN\n",
    "#train_x_norm_reshaped = np.expand_dims(train_x_norm,axis=2) #formatting for input into CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below three cells are for quick testing of code (load one run, train one epoch, test two runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2018_09_18/tip6/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip2/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip3/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip4/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip5/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip7/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip8/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip9/0noise/*.npy\")\n",
    "#train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x1, train_x2, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakeprecht\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:215: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41800 samples, validate on 2200 samples\n",
      "Epoch 1/20\n",
      " - 242s - loss: 2.4178 - acc: 0.0920 - top2metric: 0.1834 - top3metric: 0.2749 - val_loss: 2.3974 - val_acc: 0.1036 - val_top2metric: 0.1941 - val_top3metric: 0.2905\n",
      "Epoch 2/20\n",
      " - 239s - loss: 1.8128 - acc: 0.2523 - top2metric: 0.4362 - top3metric: 0.5799 - val_loss: 0.8552 - val_acc: 0.6036 - val_top2metric: 0.7850 - val_top3metric: 0.9200\n",
      "Epoch 3/20\n",
      " - 240s - loss: 0.7355 - acc: 0.6614 - top2metric: 0.8486 - top3metric: 0.9441 - val_loss: 0.5026 - val_acc: 0.7782 - val_top2metric: 0.9041 - val_top3metric: 1.0000\n",
      "Epoch 4/20\n",
      " - 239s - loss: 0.4962 - acc: 0.7748 - top2metric: 0.9068 - top3metric: 0.9844 - val_loss: 0.3932 - val_acc: 0.8005 - val_top2metric: 0.8791 - val_top3metric: 0.9655\n",
      "Epoch 5/20\n",
      " - 239s - loss: 0.4103 - acc: 0.8083 - top2metric: 0.9257 - top3metric: 0.9931 - val_loss: 0.2718 - val_acc: 0.8673 - val_top2metric: 0.9877 - val_top3metric: 1.0000\n",
      "Epoch 6/20\n",
      " - 238s - loss: 0.3814 - acc: 0.8230 - top2metric: 0.9367 - top3metric: 0.9956 - val_loss: 0.5259 - val_acc: 0.7509 - val_top2metric: 0.9286 - val_top3metric: 1.0000\n",
      "Epoch 7/20\n",
      " - 238s - loss: 0.3477 - acc: 0.8394 - top2metric: 0.9497 - top3metric: 0.9974 - val_loss: 0.3289 - val_acc: 0.8591 - val_top2metric: 0.9332 - val_top3metric: 0.9905\n",
      "Epoch 8/20\n",
      " - 238s - loss: 0.3305 - acc: 0.8484 - top2metric: 0.9570 - top3metric: 0.9984 - val_loss: 0.2128 - val_acc: 0.9132 - val_top2metric: 0.9841 - val_top3metric: 1.0000\n",
      "Epoch 9/20\n",
      " - 238s - loss: 0.2940 - acc: 0.8653 - top2metric: 0.9684 - top3metric: 0.9989 - val_loss: 0.2288 - val_acc: 0.8868 - val_top2metric: 0.9877 - val_top3metric: 1.0000\n",
      "Epoch 10/20\n",
      " - 238s - loss: 0.2905 - acc: 0.8683 - top2metric: 0.9693 - top3metric: 0.9987 - val_loss: 0.1677 - val_acc: 0.9136 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 11/20\n",
      " - 239s - loss: 0.2717 - acc: 0.8788 - top2metric: 0.9758 - top3metric: 0.9994 - val_loss: 0.2443 - val_acc: 0.8591 - val_top2metric: 0.9532 - val_top3metric: 1.0000\n",
      "Epoch 12/20\n",
      " - 238s - loss: 0.2370 - acc: 0.8913 - top2metric: 0.9819 - top3metric: 0.9995 - val_loss: 0.1455 - val_acc: 0.9136 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 13/20\n",
      " - 239s - loss: 0.2409 - acc: 0.8917 - top2metric: 0.9831 - top3metric: 0.9995 - val_loss: 0.3189 - val_acc: 0.8405 - val_top2metric: 0.9450 - val_top3metric: 1.0000\n",
      "Epoch 14/20\n",
      " - 239s - loss: 0.2408 - acc: 0.8926 - top2metric: 0.9838 - top3metric: 0.9996 - val_loss: 0.1713 - val_acc: 0.9159 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 15/20\n",
      " - 238s - loss: 0.2246 - acc: 0.8983 - top2metric: 0.9867 - top3metric: 0.9996 - val_loss: 0.1379 - val_acc: 0.9473 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 16/20\n",
      " - 238s - loss: 0.2191 - acc: 0.9025 - top2metric: 0.9878 - top3metric: 0.9995 - val_loss: 0.1898 - val_acc: 0.9259 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 17/20\n",
      " - 238s - loss: 0.2131 - acc: 0.9037 - top2metric: 0.9888 - top3metric: 0.9997 - val_loss: 0.1488 - val_acc: 0.9205 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 18/20\n",
      " - 239s - loss: 0.2017 - acc: 0.9101 - top2metric: 0.9901 - top3metric: 0.9997 - val_loss: 0.1541 - val_acc: 0.9118 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 19/20\n",
      " - 238s - loss: 0.2040 - acc: 0.9089 - top2metric: 0.9909 - top3metric: 0.9996 - val_loss: 0.1361 - val_acc: 0.8923 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 20/20\n",
      " - 238s - loss: 0.1916 - acc: 0.9137 - top2metric: 0.9913 - top3metric: 0.9998 - val_loss: 0.1956 - val_acc: 0.8795 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = test.train_CNN(train_x1,train_x2,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load and test synthetic test data.  also save model and results\n",
    "test.test_CNN(test_fraction=0.1)\n",
    "test.save_CNN('modular_CNN_Durmus_data_2018_06_21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500/5500 [==============================] - 15s 3ms/step\n",
      "model scored 54.545% on D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\n",
      "one_diff_error = 27.27\n",
      "two_diff_error = 9.09\n",
      "three_diff_error = 9.09\n",
      "which taus failed were: [  0. 500. 500. 500. 500.   0.   0.   0. 500.   0.   0.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#load and test test data.  also save model and results\n",
    "\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\")\n",
    "#test.test_experimental_CNN(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\")\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run3\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run8\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run22\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run28\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "test.save_CNN('2018_09_19_expt_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakeprecht\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:215: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41800 samples, validate on 2200 samples\n",
      "Epoch 1/20\n",
      " - 248s - loss: 2.2648 - acc: 0.1238 - top2metric: 0.2440 - top3metric: 0.3625 - val_loss: 1.4962 - val_acc: 0.2909 - val_top2metric: 0.5564 - val_top3metric: 0.7168\n",
      "Epoch 2/20\n",
      " - 246s - loss: 0.9506 - acc: 0.5712 - top2metric: 0.7865 - top3metric: 0.9057 - val_loss: 0.6551 - val_acc: 0.7641 - val_top2metric: 0.8618 - val_top3metric: 0.9668\n",
      "Epoch 3/20\n",
      " - 246s - loss: 0.5297 - acc: 0.7620 - top2metric: 0.8991 - top3metric: 0.9801 - val_loss: 0.3867 - val_acc: 0.8259 - val_top2metric: 0.9400 - val_top3metric: 1.0000\n",
      "Epoch 4/20\n",
      " - 246s - loss: 0.4616 - acc: 0.7914 - top2metric: 0.9176 - top3metric: 0.9907 - val_loss: 0.3527 - val_acc: 0.8205 - val_top2metric: 0.9536 - val_top3metric: 1.0000\n",
      "Epoch 5/20\n",
      " - 246s - loss: 0.4157 - acc: 0.8101 - top2metric: 0.9273 - top3metric: 0.9926 - val_loss: 0.3463 - val_acc: 0.8209 - val_top2metric: 0.9405 - val_top3metric: 1.0000\n",
      "Epoch 6/20\n",
      " - 246s - loss: 0.3615 - acc: 0.8313 - top2metric: 0.9426 - top3metric: 0.9969 - val_loss: 0.2821 - val_acc: 0.8477 - val_top2metric: 0.9523 - val_top3metric: 1.0000\n",
      "Epoch 7/20\n",
      " - 246s - loss: 0.3558 - acc: 0.8373 - top2metric: 0.9477 - top3metric: 0.9963 - val_loss: 0.5588 - val_acc: 0.8218 - val_top2metric: 0.9264 - val_top3metric: 0.9782\n",
      "Epoch 8/20\n",
      " - 246s - loss: 0.3471 - acc: 0.8421 - top2metric: 0.9500 - top3metric: 0.9966 - val_loss: 0.2295 - val_acc: 0.9177 - val_top2metric: 0.9909 - val_top3metric: 1.0000\n",
      "Epoch 9/20\n",
      " - 246s - loss: 0.3164 - acc: 0.8522 - top2metric: 0.9572 - top3metric: 0.9975 - val_loss: 0.2518 - val_acc: 0.8868 - val_top2metric: 0.9677 - val_top3metric: 1.0000\n",
      "Epoch 10/20\n",
      " - 246s - loss: 0.3075 - acc: 0.8595 - top2metric: 0.9634 - top3metric: 0.9985 - val_loss: 0.2437 - val_acc: 0.8586 - val_top2metric: 0.9632 - val_top3metric: 1.0000\n",
      "Epoch 11/20\n",
      " - 246s - loss: 0.2977 - acc: 0.8664 - top2metric: 0.9688 - top3metric: 0.9983 - val_loss: 0.3748 - val_acc: 0.8564 - val_top2metric: 0.9427 - val_top3metric: 1.0000\n",
      "Epoch 12/20\n",
      " - 246s - loss: 0.2851 - acc: 0.8681 - top2metric: 0.9706 - top3metric: 0.9989 - val_loss: 0.2393 - val_acc: 0.9255 - val_top2metric: 0.9882 - val_top3metric: 1.0000\n",
      "Epoch 13/20\n",
      " - 246s - loss: 0.2728 - acc: 0.8785 - top2metric: 0.9741 - top3metric: 0.9990 - val_loss: 0.3060 - val_acc: 0.8355 - val_top2metric: 0.9650 - val_top3metric: 1.0000\n",
      "Epoch 14/20\n",
      " - 246s - loss: 0.2815 - acc: 0.8723 - top2metric: 0.9713 - top3metric: 0.9985 - val_loss: 0.2084 - val_acc: 0.8955 - val_top2metric: 0.9818 - val_top3metric: 1.0000\n",
      "Epoch 15/20\n",
      " - 246s - loss: 0.2664 - acc: 0.8784 - top2metric: 0.9755 - top3metric: 0.9989 - val_loss: 0.2245 - val_acc: 0.8982 - val_top2metric: 0.9909 - val_top3metric: 1.0000\n",
      "Epoch 16/20\n",
      " - 246s - loss: 0.2317 - acc: 0.8921 - top2metric: 0.9822 - top3metric: 0.9996 - val_loss: 0.1811 - val_acc: 0.8782 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 17/20\n",
      " - 246s - loss: 0.2438 - acc: 0.8868 - top2metric: 0.9795 - top3metric: 0.9995 - val_loss: 0.1574 - val_acc: 0.9291 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 18/20\n",
      " - 246s - loss: 0.2704 - acc: 0.8850 - top2metric: 0.9790 - top3metric: 0.9986 - val_loss: 0.2534 - val_acc: 0.8536 - val_top2metric: 0.9768 - val_top3metric: 1.0000\n",
      "Epoch 19/20\n",
      " - 246s - loss: 0.2512 - acc: 0.8871 - top2metric: 0.9798 - top3metric: 0.9995 - val_loss: 0.1686 - val_acc: 0.9414 - val_top2metric: 0.9909 - val_top3metric: 1.0000\n",
      "Epoch 20/20\n",
      " - 246s - loss: 0.2467 - acc: 0.8941 - top2metric: 0.9832 - top3metric: 0.9994 - val_loss: 0.1363 - val_acc: 0.9155 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "5500/5500 [==============================] - 15s 3ms/step\n",
      "model scored 63.636% on D:/jake/DDHO Data/displacement/2018_09_18/tip2/0noise/*.npy\n",
      "one_diff_error = 18.18\n",
      "two_diff_error = 9.09\n",
      "three_diff_error = 9.09\n",
      "which taus failed were: [500. 500. 500. 500.   0.   0.   0.   0.   0.   0.   0.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2018_09_18/tip9/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip3/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip4/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip5/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip6/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip7/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip8/0noise/*.npy\")\n",
    "train_x1, train_x2, train_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "#train model\n",
    "model = test.train_CNN(train_x1,train_x2,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "#load and test test data.  also save model and results\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2018_09_18/tip2/0noise/*.npy\")\n",
    "test.save_CNN('2018_09_19_expt_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakeprecht\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:215: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41800 samples, validate on 2200 samples\n",
      "Epoch 1/20\n",
      " - 245s - loss: 2.4229 - acc: 0.0892 - top2metric: 0.1819 - top3metric: 0.2731 - val_loss: 2.3992 - val_acc: 0.0736 - val_top2metric: 0.1605 - val_top3metric: 0.2491\n",
      "Epoch 2/20\n",
      " - 243s - loss: 2.3992 - acc: 0.0887 - top2metric: 0.1797 - top3metric: 0.2719 - val_loss: 2.3984 - val_acc: 0.0736 - val_top2metric: 0.1655 - val_top3metric: 0.2559\n",
      "Epoch 3/20\n",
      " - 243s - loss: 1.5649 - acc: 0.3360 - top2metric: 0.5484 - top3metric: 0.7056 - val_loss: 0.8108 - val_acc: 0.6182 - val_top2metric: 0.7964 - val_top3metric: 0.9436\n",
      "Epoch 4/20\n",
      " - 243s - loss: 0.7561 - acc: 0.6529 - top2metric: 0.8544 - top3metric: 0.9557 - val_loss: 0.3928 - val_acc: 0.8127 - val_top2metric: 0.9264 - val_top3metric: 1.0000\n",
      "Epoch 5/20\n",
      " - 243s - loss: 0.4843 - acc: 0.7810 - top2metric: 0.9182 - top3metric: 0.9892 - val_loss: 0.3295 - val_acc: 0.8323 - val_top2metric: 0.9373 - val_top3metric: 0.9891\n",
      "Epoch 6/20\n",
      " - 243s - loss: 0.4080 - acc: 0.8128 - top2metric: 0.9332 - top3metric: 0.9942 - val_loss: 0.2955 - val_acc: 0.8677 - val_top2metric: 0.9673 - val_top3metric: 1.0000\n",
      "Epoch 7/20\n",
      " - 243s - loss: 0.3897 - acc: 0.8176 - top2metric: 0.9337 - top3metric: 0.9934 - val_loss: 0.2445 - val_acc: 0.9055 - val_top2metric: 0.9932 - val_top3metric: 1.0000\n",
      "Epoch 8/20\n",
      " - 243s - loss: 0.3737 - acc: 0.8309 - top2metric: 0.9476 - top3metric: 0.9967 - val_loss: 0.2142 - val_acc: 0.9014 - val_top2metric: 0.9850 - val_top3metric: 1.0000\n",
      "Epoch 9/20\n",
      " - 243s - loss: 0.3305 - acc: 0.8470 - top2metric: 0.9519 - top3metric: 0.9970 - val_loss: 0.2482 - val_acc: 0.8773 - val_top2metric: 0.9850 - val_top3metric: 1.0000\n",
      "Epoch 10/20\n",
      " - 243s - loss: 0.3184 - acc: 0.8543 - top2metric: 0.9594 - top3metric: 0.9986 - val_loss: 0.2843 - val_acc: 0.8327 - val_top2metric: 0.9264 - val_top3metric: 1.0000\n",
      "Epoch 11/20\n",
      " - 243s - loss: 0.3235 - acc: 0.8551 - top2metric: 0.9617 - top3metric: 0.9980 - val_loss: 0.2755 - val_acc: 0.8691 - val_top2metric: 0.9545 - val_top3metric: 1.0000\n",
      "Epoch 12/20\n",
      " - 243s - loss: 0.3312 - acc: 0.8482 - top2metric: 0.9523 - top3metric: 0.9960 - val_loss: 0.2592 - val_acc: 0.8427 - val_top2metric: 0.9555 - val_top3metric: 1.0000\n",
      "Epoch 13/20\n",
      " - 243s - loss: 0.3009 - acc: 0.8653 - top2metric: 0.9683 - top3metric: 0.9984 - val_loss: 0.2255 - val_acc: 0.9005 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 14/20\n",
      " - 243s - loss: 0.2861 - acc: 0.8690 - top2metric: 0.9701 - top3metric: 0.9988 - val_loss: 0.1828 - val_acc: 0.8823 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 15/20\n",
      " - 243s - loss: 0.2851 - acc: 0.8716 - top2metric: 0.9728 - top3metric: 0.9983 - val_loss: 0.1537 - val_acc: 0.9109 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 16/20\n",
      " - 243s - loss: 0.3457 - acc: 0.8727 - top2metric: 0.9715 - top3metric: 0.9973 - val_loss: 0.1477 - val_acc: 0.9368 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 17/20\n",
      " - 243s - loss: 0.2579 - acc: 0.8847 - top2metric: 0.9798 - top3metric: 0.9992 - val_loss: 0.1772 - val_acc: 0.9123 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 18/20\n",
      " - 243s - loss: 0.2428 - acc: 0.8885 - top2metric: 0.9808 - top3metric: 0.9992 - val_loss: 0.1649 - val_acc: 0.9286 - val_top2metric: 0.9941 - val_top3metric: 1.0000\n",
      "Epoch 19/20\n",
      " - 243s - loss: 0.2353 - acc: 0.8967 - top2metric: 0.9841 - top3metric: 0.9995 - val_loss: 0.1540 - val_acc: 0.9468 - val_top2metric: 0.9941 - val_top3metric: 1.0000\n",
      "Epoch 20/20\n",
      " - 243s - loss: 0.2469 - acc: 0.8923 - top2metric: 0.9820 - top3metric: 0.9991 - val_loss: 0.1455 - val_acc: 0.9359 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "5500/5500 [==============================] - 15s 3ms/step\n",
      "model scored 63.636% on D:/jake/DDHO Data/displacement/2018_09_18/tip3/0noise/*.npy\n",
      "one_diff_error = 18.18\n",
      "two_diff_error = 18.18\n",
      "three_diff_error = 0.0\n",
      "which taus failed were: [  0. 500. 500. 500. 500.   0.   0.   0.   0.   0.   0.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2018_09_18/tip9/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip2/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip4/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip5/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip6/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip7/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip8/0noise/*.npy\")\n",
    "train_x1, train_x2, train_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "#train model\n",
    "model = test.train_CNN(train_x1,train_x2,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "#load and test test data.  also save model and results\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2018_09_18/tip3/0noise/*.npy\")\n",
    "test.save_CNN('2018_09_19_expt_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakeprecht\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:215: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41800 samples, validate on 2200 samples\n",
      "Epoch 1/20\n",
      " - 248s - loss: 2.4214 - acc: 0.0923 - top2metric: 0.1828 - top3metric: 0.2717 - val_loss: 2.3982 - val_acc: 0.0927 - val_top2metric: 0.1882 - val_top3metric: 0.2695\n",
      "Epoch 2/20\n",
      " - 245s - loss: 2.3990 - acc: 0.0863 - top2metric: 0.1765 - top3metric: 0.2689 - val_loss: 2.3982 - val_acc: 0.0941 - val_top2metric: 0.1845 - val_top3metric: 0.2741\n",
      "Epoch 3/20\n",
      " - 245s - loss: 2.3989 - acc: 0.0927 - top2metric: 0.1813 - top3metric: 0.2712 - val_loss: 2.3983 - val_acc: 0.0859 - val_top2metric: 0.1691 - val_top3metric: 0.2609\n",
      "Epoch 4/20\n",
      " - 245s - loss: 2.3993 - acc: 0.0902 - top2metric: 0.1835 - top3metric: 0.2736 - val_loss: 2.3979 - val_acc: 0.0932 - val_top2metric: 0.1877 - val_top3metric: 0.2795\n",
      "Epoch 5/20\n",
      " - 245s - loss: 2.3986 - acc: 0.0908 - top2metric: 0.1794 - top3metric: 0.2721 - val_loss: 2.3985 - val_acc: 0.0882 - val_top2metric: 0.1818 - val_top3metric: 0.2727\n",
      "Epoch 6/20\n",
      " - 245s - loss: 2.2377 - acc: 0.1400 - top2metric: 0.2625 - top3metric: 0.3690 - val_loss: 1.2926 - val_acc: 0.4500 - val_top2metric: 0.7073 - val_top3metric: 0.8373\n",
      "Epoch 7/20\n",
      " - 245s - loss: 1.1028 - acc: 0.5205 - top2metric: 0.7343 - top3metric: 0.8483 - val_loss: 0.6803 - val_acc: 0.6795 - val_top2metric: 0.8023 - val_top3metric: 0.9636\n",
      "Epoch 8/20\n",
      " - 245s - loss: 0.7174 - acc: 0.6839 - top2metric: 0.8430 - top3metric: 0.9362 - val_loss: 0.4593 - val_acc: 0.8245 - val_top2metric: 0.9518 - val_top3metric: 1.0000\n",
      "Epoch 9/20\n",
      " - 245s - loss: 0.5554 - acc: 0.7525 - top2metric: 0.8917 - top3metric: 0.9745 - val_loss: 0.3780 - val_acc: 0.7995 - val_top2metric: 0.9041 - val_top3metric: 1.0000\n",
      "Epoch 10/20\n",
      " - 245s - loss: 0.4769 - acc: 0.7830 - top2metric: 0.9129 - top3metric: 0.9865 - val_loss: 0.3367 - val_acc: 0.8000 - val_top2metric: 0.9473 - val_top3metric: 1.0000\n",
      "Epoch 11/20\n",
      " - 245s - loss: 0.4520 - acc: 0.7952 - top2metric: 0.9173 - top3metric: 0.9889 - val_loss: 0.3459 - val_acc: 0.8105 - val_top2metric: 0.9236 - val_top3metric: 1.0000\n",
      "Epoch 12/20\n",
      " - 245s - loss: 0.4110 - acc: 0.8091 - top2metric: 0.9268 - top3metric: 0.9935 - val_loss: 0.3355 - val_acc: 0.8295 - val_top2metric: 0.9432 - val_top3metric: 1.0000\n",
      "Epoch 13/20\n",
      " - 245s - loss: 0.4048 - acc: 0.8136 - top2metric: 0.9301 - top3metric: 0.9924 - val_loss: 0.3199 - val_acc: 0.8591 - val_top2metric: 0.9477 - val_top3metric: 1.0000\n",
      "Epoch 14/20\n",
      " - 245s - loss: 0.3882 - acc: 0.8241 - top2metric: 0.9389 - top3metric: 0.9944 - val_loss: 0.3212 - val_acc: 0.8359 - val_top2metric: 0.9555 - val_top3metric: 0.9859\n",
      "Epoch 15/20\n",
      " - 245s - loss: 0.3542 - acc: 0.8344 - top2metric: 0.9461 - top3metric: 0.9951 - val_loss: 0.2798 - val_acc: 0.8709 - val_top2metric: 0.9573 - val_top3metric: 1.0000\n",
      "Epoch 16/20\n",
      " - 245s - loss: 0.3607 - acc: 0.8347 - top2metric: 0.9486 - top3metric: 0.9961 - val_loss: 0.2499 - val_acc: 0.9100 - val_top2metric: 0.9636 - val_top3metric: 1.0000\n",
      "Epoch 17/20\n",
      " - 245s - loss: 0.3241 - acc: 0.8500 - top2metric: 0.9570 - top3metric: 0.9982 - val_loss: 0.2734 - val_acc: 0.8441 - val_top2metric: 0.9782 - val_top3metric: 1.0000\n",
      "Epoch 18/20\n",
      " - 245s - loss: 0.3176 - acc: 0.8557 - top2metric: 0.9633 - top3metric: 0.9979 - val_loss: 0.2956 - val_acc: 0.8150 - val_top2metric: 0.9623 - val_top3metric: 1.0000\n",
      "Epoch 19/20\n",
      " - 245s - loss: 0.3160 - acc: 0.8563 - top2metric: 0.9621 - top3metric: 0.9973 - val_loss: 0.2274 - val_acc: 0.8891 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 20/20\n",
      " - 245s - loss: 0.3134 - acc: 0.8595 - top2metric: 0.9644 - top3metric: 0.9980 - val_loss: 0.1910 - val_acc: 0.9295 - val_top2metric: 0.9900 - val_top3metric: 1.0000\n",
      "5500/5500 [==============================] - 15s 3ms/step\n",
      "model scored 45.455% on D:/jake/DDHO Data/displacement/2018_09_18/tip4/0noise/*.npy\n",
      "one_diff_error = 27.27\n",
      "two_diff_error = 9.09\n",
      "three_diff_error = 9.09\n",
      "which taus failed were: [500. 500. 500. 500. 500. 500.   0.   0.   0.   0.   0.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2018_09_18/tip9/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip2/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip3/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip5/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip6/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip7/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip8/0noise/*.npy\")\n",
    "train_x1, train_x2, train_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "#train model\n",
    "model = test.train_CNN(train_x1,train_x2,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "#load and test test data.  also save model and results\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2018_09_18/tip4/0noise/*.npy\")\n",
    "test.save_CNN('2018_09_19_expt_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakeprecht\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:215: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41800 samples, validate on 2200 samples\n",
      "Epoch 1/20\n",
      " - 253s - loss: 2.4232 - acc: 0.0876 - top2metric: 0.1801 - top3metric: 0.2699 - val_loss: 2.3979 - val_acc: 0.0918 - val_top2metric: 0.1859 - val_top3metric: 0.2805\n",
      "Epoch 2/20\n",
      " - 251s - loss: 2.3991 - acc: 0.0890 - top2metric: 0.1793 - top3metric: 0.2684 - val_loss: 2.3997 - val_acc: 0.0868 - val_top2metric: 0.1855 - val_top3metric: 0.2545\n",
      "Epoch 3/20\n",
      " - 251s - loss: 2.3989 - acc: 0.0890 - top2metric: 0.1783 - top3metric: 0.2690 - val_loss: 2.3982 - val_acc: 0.0877 - val_top2metric: 0.1695 - val_top3metric: 0.2627\n",
      "Epoch 4/20\n",
      " - 251s - loss: 2.3986 - acc: 0.0897 - top2metric: 0.1769 - top3metric: 0.2676 - val_loss: 2.3982 - val_acc: 0.0823 - val_top2metric: 0.1577 - val_top3metric: 0.2591\n",
      "Epoch 5/20\n",
      " - 251s - loss: 1.9928 - acc: 0.2113 - top2metric: 0.3600 - top3metric: 0.4749 - val_loss: 0.8755 - val_acc: 0.6277 - val_top2metric: 0.7786 - val_top3metric: 0.8736\n",
      "Epoch 6/20\n",
      " - 251s - loss: 0.7213 - acc: 0.6776 - top2metric: 0.8282 - top3metric: 0.9212 - val_loss: 0.4495 - val_acc: 0.7886 - val_top2metric: 0.8827 - val_top3metric: 0.9905\n",
      "Epoch 7/20\n",
      " - 251s - loss: 0.5214 - acc: 0.7611 - top2metric: 0.8882 - top3metric: 0.9717 - val_loss: 0.4824 - val_acc: 0.7832 - val_top2metric: 0.8782 - val_top3metric: 0.9750\n",
      "Epoch 8/20\n",
      " - 251s - loss: 0.4181 - acc: 0.8018 - top2metric: 0.9194 - top3metric: 0.9906 - val_loss: 0.4217 - val_acc: 0.8450 - val_top2metric: 0.9386 - val_top3metric: 0.9750\n",
      "Epoch 9/20\n",
      " - 251s - loss: 0.4051 - acc: 0.8120 - top2metric: 0.9263 - top3metric: 0.9935 - val_loss: 0.8051 - val_acc: 0.6977 - val_top2metric: 0.8282 - val_top3metric: 0.9205\n",
      "Epoch 10/20\n",
      " - 251s - loss: 0.4004 - acc: 0.8180 - top2metric: 0.9303 - top3metric: 0.9924 - val_loss: 0.2729 - val_acc: 0.8709 - val_top2metric: 0.9659 - val_top3metric: 1.0000\n",
      "Epoch 11/20\n",
      " - 251s - loss: 0.3566 - acc: 0.8301 - top2metric: 0.9396 - top3metric: 0.9971 - val_loss: 0.2683 - val_acc: 0.8641 - val_top2metric: 0.9491 - val_top3metric: 1.0000\n",
      "Epoch 12/20\n",
      " - 251s - loss: 0.3258 - acc: 0.8483 - top2metric: 0.9525 - top3metric: 0.9986 - val_loss: 0.2306 - val_acc: 0.8814 - val_top2metric: 0.9886 - val_top3metric: 1.0000\n",
      "Epoch 13/20\n",
      " - 251s - loss: 0.3138 - acc: 0.8548 - top2metric: 0.9591 - top3metric: 0.9989 - val_loss: 0.2760 - val_acc: 0.8436 - val_top2metric: 0.9536 - val_top3metric: 1.0000\n",
      "Epoch 14/20\n",
      " - 252s - loss: 0.3055 - acc: 0.8597 - top2metric: 0.9646 - top3metric: 0.9990 - val_loss: 0.1943 - val_acc: 0.9250 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 15/20\n",
      " - 252s - loss: 0.3024 - acc: 0.8581 - top2metric: 0.9604 - top3metric: 0.9989 - val_loss: 0.2185 - val_acc: 0.9150 - val_top2metric: 0.9741 - val_top3metric: 1.0000\n",
      "Epoch 16/20\n",
      " - 251s - loss: 0.2969 - acc: 0.8649 - top2metric: 0.9671 - top3metric: 0.9991 - val_loss: 0.3344 - val_acc: 0.8586 - val_top2metric: 0.9564 - val_top3metric: 1.0000\n",
      "Epoch 17/20\n",
      " - 251s - loss: 0.2812 - acc: 0.8706 - top2metric: 0.9715 - top3metric: 0.9992 - val_loss: 0.2737 - val_acc: 0.8532 - val_top2metric: 0.9541 - val_top3metric: 1.0000\n",
      "Epoch 18/20\n",
      " - 251s - loss: 0.2797 - acc: 0.8741 - top2metric: 0.9723 - top3metric: 0.9986 - val_loss: 0.4712 - val_acc: 0.7791 - val_top2metric: 0.9164 - val_top3metric: 1.0000\n",
      "Epoch 19/20\n",
      " - 251s - loss: 0.2564 - acc: 0.8817 - top2metric: 0.9769 - top3metric: 0.9994 - val_loss: 0.2049 - val_acc: 0.9050 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 20/20\n",
      " - 251s - loss: 0.2566 - acc: 0.8819 - top2metric: 0.9790 - top3metric: 0.9994 - val_loss: 0.2835 - val_acc: 0.8864 - val_top2metric: 0.9709 - val_top3metric: 1.0000\n",
      "5500/5500 [==============================] - 16s 3ms/step\n",
      "model scored 9.091000000000001% on D:/jake/DDHO Data/displacement/2018_09_18/tip5/0noise/*.npy\n",
      "one_diff_error = 9.09\n",
      "two_diff_error = 9.09\n",
      "three_diff_error = 9.09\n",
      "which taus failed were: [500. 500. 500. 500. 500. 500. 500. 500. 500. 500.   0.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2018_09_18/tip9/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip2/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip3/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip4/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip6/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip7/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip8/0noise/*.npy\")\n",
    "train_x1, train_x2, train_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "#train model\n",
    "model = test.train_CNN(train_x1,train_x2,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "#load and test test data.  also save model and results\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2018_09_18/tip5/0noise/*.npy\")\n",
    "test.save_CNN('2018_09_19_expt_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakeprecht\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:215: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41800 samples, validate on 2200 samples\n",
      "Epoch 1/20\n",
      " - 252s - loss: 2.4227 - acc: 0.0924 - top2metric: 0.1822 - top3metric: 0.2734 - val_loss: 2.3984 - val_acc: 0.0845 - val_top2metric: 0.1527 - val_top3metric: 0.2527\n",
      "Epoch 2/20\n",
      " - 250s - loss: 2.3996 - acc: 0.0901 - top2metric: 0.1800 - top3metric: 0.2707 - val_loss: 2.3977 - val_acc: 0.0959 - val_top2metric: 0.1923 - val_top3metric: 0.2805\n",
      "Epoch 3/20\n",
      " - 250s - loss: 2.3990 - acc: 0.0890 - top2metric: 0.1782 - top3metric: 0.2706 - val_loss: 2.3980 - val_acc: 0.0895 - val_top2metric: 0.1764 - val_top3metric: 0.2759\n",
      "Epoch 4/20\n",
      " - 250s - loss: 2.3987 - acc: 0.0900 - top2metric: 0.1783 - top3metric: 0.2690 - val_loss: 2.3977 - val_acc: 0.0877 - val_top2metric: 0.1645 - val_top3metric: 0.2727\n",
      "Epoch 5/20\n",
      " - 250s - loss: 2.3989 - acc: 0.0879 - top2metric: 0.1778 - top3metric: 0.2700 - val_loss: 2.3986 - val_acc: 0.0877 - val_top2metric: 0.1736 - val_top3metric: 0.2636\n",
      "Epoch 6/20\n",
      " - 250s - loss: 2.3991 - acc: 0.0887 - top2metric: 0.1779 - top3metric: 0.2681 - val_loss: 2.3984 - val_acc: 0.0859 - val_top2metric: 0.1795 - val_top3metric: 0.2659\n",
      "Epoch 7/20\n",
      " - 250s - loss: 2.3983 - acc: 0.0944 - top2metric: 0.1835 - top3metric: 0.2734 - val_loss: 2.3976 - val_acc: 0.1091 - val_top2metric: 0.1768 - val_top3metric: 0.2727\n",
      "Epoch 8/20\n",
      " - 250s - loss: 1.7710 - acc: 0.2815 - top2metric: 0.4619 - top3metric: 0.5870 - val_loss: 0.9449 - val_acc: 0.5782 - val_top2metric: 0.7905 - val_top3metric: 0.8668\n",
      "Epoch 9/20\n",
      " - 250s - loss: 0.8308 - acc: 0.6373 - top2metric: 0.8138 - top3metric: 0.9102 - val_loss: 0.5307 - val_acc: 0.7345 - val_top2metric: 0.8364 - val_top3metric: 0.9341\n",
      "Epoch 10/20\n",
      " - 250s - loss: 0.6275 - acc: 0.7203 - top2metric: 0.8591 - top3metric: 0.9486 - val_loss: 0.4007 - val_acc: 0.8255 - val_top2metric: 0.9277 - val_top3metric: 1.0000\n",
      "Epoch 11/20\n",
      " - 250s - loss: 0.5507 - acc: 0.7522 - top2metric: 0.8827 - top3metric: 0.9644 - val_loss: 0.4534 - val_acc: 0.8182 - val_top2metric: 0.9300 - val_top3metric: 1.0000\n",
      "Epoch 12/20\n",
      " - 250s - loss: 0.5121 - acc: 0.7660 - top2metric: 0.8969 - top3metric: 0.9745 - val_loss: 0.4225 - val_acc: 0.7632 - val_top2metric: 0.8800 - val_top3metric: 0.9786\n",
      "Epoch 13/20\n",
      " - 250s - loss: 0.5053 - acc: 0.7756 - top2metric: 0.9012 - top3metric: 0.9783 - val_loss: 0.3113 - val_acc: 0.8818 - val_top2metric: 0.9791 - val_top3metric: 1.0000\n",
      "Epoch 14/20\n",
      " - 250s - loss: 0.4321 - acc: 0.8000 - top2metric: 0.9152 - top3metric: 0.9854 - val_loss: 0.3975 - val_acc: 0.8009 - val_top2metric: 0.9264 - val_top3metric: 1.0000\n",
      "Epoch 15/20\n",
      " - 250s - loss: 0.4286 - acc: 0.7982 - top2metric: 0.9160 - top3metric: 0.9862 - val_loss: 0.3089 - val_acc: 0.8500 - val_top2metric: 0.9414 - val_top3metric: 1.0000\n",
      "Epoch 16/20\n",
      " - 250s - loss: 0.4081 - acc: 0.8121 - top2metric: 0.9262 - top3metric: 0.9903 - val_loss: 0.4291 - val_acc: 0.8032 - val_top2metric: 0.9195 - val_top3metric: 0.9673\n",
      "Epoch 17/20\n",
      " - 250s - loss: 0.3722 - acc: 0.8245 - top2metric: 0.9379 - top3metric: 0.9933 - val_loss: 0.2885 - val_acc: 0.8309 - val_top2metric: 0.9486 - val_top3metric: 1.0000\n",
      "Epoch 18/20\n",
      " - 250s - loss: 0.3763 - acc: 0.8246 - top2metric: 0.9375 - top3metric: 0.9933 - val_loss: 0.3229 - val_acc: 0.8491 - val_top2metric: 0.9291 - val_top3metric: 0.9795\n",
      "Epoch 19/20\n",
      " - 250s - loss: 0.3631 - acc: 0.8312 - top2metric: 0.9418 - top3metric: 0.9933 - val_loss: 0.2587 - val_acc: 0.8455 - val_top2metric: 0.9755 - val_top3metric: 1.0000\n",
      "Epoch 20/20\n",
      " - 250s - loss: 0.3967 - acc: 0.8213 - top2metric: 0.9416 - top3metric: 0.9917 - val_loss: 0.3819 - val_acc: 0.8186 - val_top2metric: 0.9205 - val_top3metric: 0.9809\n",
      "5500/5500 [==============================] - 16s 3ms/step\n",
      "model scored 9.091000000000001% on D:/jake/DDHO Data/displacement/2018_09_18/tip6/0noise/*.npy\n",
      "one_diff_error = 9.09\n",
      "two_diff_error = 9.09\n",
      "three_diff_error = 9.09\n",
      "which taus failed were: [500. 500. 500. 500. 500. 500. 500. 500. 500. 500.   0.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2018_09_18/tip9/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip2/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip3/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip4/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip5/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip7/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip8/0noise/*.npy\")\n",
    "train_x1, train_x2, train_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "#train model\n",
    "model = test.train_CNN(train_x1,train_x2,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "#load and test test data.  also save model and results\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2018_09_18/tip6/0noise/*.npy\")\n",
    "test.save_CNN('2018_09_19_expt_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakeprecht\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:215: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41800 samples, validate on 2200 samples\n",
      "Epoch 1/20\n",
      " - 249s - loss: 2.4212 - acc: 0.0899 - top2metric: 0.1785 - top3metric: 0.2682 - val_loss: 2.3980 - val_acc: 0.0991 - val_top2metric: 0.1845 - val_top3metric: 0.2636\n",
      "Epoch 2/20\n",
      " - 247s - loss: 2.3965 - acc: 0.0959 - top2metric: 0.1870 - top3metric: 0.2774 - val_loss: 2.3071 - val_acc: 0.1168 - val_top2metric: 0.2391 - val_top3metric: 0.3705\n",
      "Epoch 3/20\n",
      " - 247s - loss: 1.2820 - acc: 0.4290 - top2metric: 0.6711 - top3metric: 0.8233 - val_loss: 0.6952 - val_acc: 0.7086 - val_top2metric: 0.8691 - val_top3metric: 0.9545\n",
      "Epoch 4/20\n",
      " - 247s - loss: 0.6931 - acc: 0.6826 - top2metric: 0.8678 - top3metric: 0.9584 - val_loss: 0.3706 - val_acc: 0.8050 - val_top2metric: 0.8982 - val_top3metric: 0.9909\n",
      "Epoch 5/20\n",
      " - 247s - loss: 0.4731 - acc: 0.7837 - top2metric: 0.9214 - top3metric: 0.9911 - val_loss: 0.2894 - val_acc: 0.8250 - val_top2metric: 0.9182 - val_top3metric: 1.0000\n",
      "Epoch 6/20\n",
      " - 247s - loss: 0.4178 - acc: 0.8094 - top2metric: 0.9301 - top3metric: 0.9930 - val_loss: 0.2825 - val_acc: 0.8627 - val_top2metric: 0.9864 - val_top3metric: 1.0000\n",
      "Epoch 7/20\n",
      " - 247s - loss: 0.3595 - acc: 0.8317 - top2metric: 0.9451 - top3metric: 0.9969 - val_loss: 0.3151 - val_acc: 0.8536 - val_top2metric: 0.9414 - val_top3metric: 1.0000\n",
      "Epoch 8/20\n",
      " - 247s - loss: 0.3523 - acc: 0.8366 - top2metric: 0.9468 - top3metric: 0.9971 - val_loss: 0.4165 - val_acc: 0.7886 - val_top2metric: 0.9127 - val_top3metric: 0.9909\n",
      "Epoch 9/20\n",
      " - 247s - loss: 0.3407 - acc: 0.8437 - top2metric: 0.9495 - top3metric: 0.9970 - val_loss: 0.2781 - val_acc: 0.8518 - val_top2metric: 0.9418 - val_top3metric: 1.0000\n",
      "Epoch 10/20\n",
      " - 247s - loss: 0.3290 - acc: 0.8500 - top2metric: 0.9584 - top3metric: 0.9978 - val_loss: 0.3764 - val_acc: 0.7891 - val_top2metric: 0.9191 - val_top3metric: 0.9868\n",
      "Epoch 11/20\n",
      " - 247s - loss: 0.2873 - acc: 0.8678 - top2metric: 0.9709 - top3metric: 0.9989 - val_loss: 0.2000 - val_acc: 0.8723 - val_top2metric: 0.9655 - val_top3metric: 1.0000\n",
      "Epoch 12/20\n",
      " - 247s - loss: 0.2878 - acc: 0.8709 - top2metric: 0.9734 - top3metric: 0.9987 - val_loss: 0.2081 - val_acc: 0.8695 - val_top2metric: 0.9736 - val_top3metric: 1.0000\n",
      "Epoch 13/20\n",
      " - 247s - loss: 0.2622 - acc: 0.8790 - top2metric: 0.9762 - top3metric: 0.9995 - val_loss: 0.1951 - val_acc: 0.8800 - val_top2metric: 0.9773 - val_top3metric: 1.0000\n",
      "Epoch 14/20\n",
      " - 247s - loss: 0.2536 - acc: 0.8864 - top2metric: 0.9811 - top3metric: 0.9993 - val_loss: 0.1605 - val_acc: 0.9209 - val_top2metric: 0.9868 - val_top3metric: 1.0000\n",
      "Epoch 15/20\n",
      " - 247s - loss: 0.2428 - acc: 0.8914 - top2metric: 0.9832 - top3metric: 0.9994 - val_loss: 0.1690 - val_acc: 0.9277 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 16/20\n",
      " - 247s - loss: 0.2563 - acc: 0.8838 - top2metric: 0.9793 - top3metric: 0.9995 - val_loss: 0.1688 - val_acc: 0.9236 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 17/20\n",
      " - 247s - loss: 0.2444 - acc: 0.8923 - top2metric: 0.9849 - top3metric: 0.9994 - val_loss: 0.1690 - val_acc: 0.9073 - val_top2metric: 0.9886 - val_top3metric: 1.0000\n",
      "Epoch 18/20\n",
      " - 247s - loss: 0.2115 - acc: 0.9013 - top2metric: 0.9885 - top3metric: 0.9998 - val_loss: 0.1299 - val_acc: 0.9773 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 19/20\n",
      " - 260s - loss: 0.2250 - acc: 0.9006 - top2metric: 0.9867 - top3metric: 0.9993 - val_loss: 0.1426 - val_acc: 0.9368 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 20/20\n",
      " - 258s - loss: 0.2201 - acc: 0.9003 - top2metric: 0.9882 - top3metric: 0.9998 - val_loss: 0.1256 - val_acc: 0.9536 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "5500/5500 [==============================] - 16s 3ms/step\n",
      "model scored 27.272999999999996% on D:/jake/DDHO Data/displacement/2018_09_18/tip7/0noise/*.npy\n",
      "one_diff_error = 36.36\n",
      "two_diff_error = 27.27\n",
      "three_diff_error = 9.09\n",
      "which taus failed were: [  0. 500. 500. 500.   0. 500. 500. 500. 500.   0. 500.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2018_09_18/tip9/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip2/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip3/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip4/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip5/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip6/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip8/0noise/*.npy\")\n",
    "train_x1, train_x2, train_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "#train model\n",
    "model = test.train_CNN(train_x1,train_x2,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "#load and test test data.  also save model and results\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2018_09_18/tip7/0noise/*.npy\")\n",
    "test.save_CNN('2018_09_19_expt_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakeprecht\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:215: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41800 samples, validate on 2200 samples\n",
      "Epoch 1/20\n",
      " - 264s - loss: 2.0419 - acc: 0.1935 - top2metric: 0.3452 - top3metric: 0.4741 - val_loss: 0.8308 - val_acc: 0.6264 - val_top2metric: 0.8455 - val_top3metric: 0.9005\n",
      "Epoch 2/20\n",
      " - 261s - loss: 0.7647 - acc: 0.6453 - top2metric: 0.8495 - top3metric: 0.9515 - val_loss: 0.5282 - val_acc: 0.7591 - val_top2metric: 0.9100 - val_top3metric: 0.9527\n",
      "Epoch 3/20\n",
      " - 261s - loss: 0.4594 - acc: 0.7911 - top2metric: 0.9290 - top3metric: 0.9942 - val_loss: 0.2409 - val_acc: 0.9123 - val_top2metric: 0.9923 - val_top3metric: 1.0000\n",
      "Epoch 4/20\n",
      " - 260s - loss: 0.3778 - acc: 0.8289 - top2metric: 0.9503 - top3metric: 0.9979 - val_loss: 0.2705 - val_acc: 0.9027 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 5/20\n",
      " - 260s - loss: 0.3249 - acc: 0.8505 - top2metric: 0.9600 - top3metric: 0.9991 - val_loss: 0.2390 - val_acc: 0.9123 - val_top2metric: 0.9923 - val_top3metric: 1.0000\n",
      "Epoch 6/20\n",
      " - 259s - loss: 0.2955 - acc: 0.8630 - top2metric: 0.9691 - top3metric: 0.9994 - val_loss: 0.2080 - val_acc: 0.9459 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 7/20\n",
      " - 259s - loss: 0.2825 - acc: 0.8688 - top2metric: 0.9730 - top3metric: 0.9994 - val_loss: 0.1566 - val_acc: 0.9582 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 8/20\n",
      " - 259s - loss: 0.2316 - acc: 0.8921 - top2metric: 0.9873 - top3metric: 0.9998 - val_loss: 0.2920 - val_acc: 0.8873 - val_top2metric: 0.9759 - val_top3metric: 1.0000\n",
      "Epoch 9/20\n",
      " - 259s - loss: 0.2571 - acc: 0.8850 - top2metric: 0.9808 - top3metric: 0.9993 - val_loss: 0.2421 - val_acc: 0.8855 - val_top2metric: 0.9845 - val_top3metric: 1.0000\n",
      "Epoch 10/20\n",
      " - 255s - loss: 0.2512 - acc: 0.8884 - top2metric: 0.9837 - top3metric: 0.9994 - val_loss: 0.1395 - val_acc: 0.9145 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 11/20\n",
      " - 250s - loss: 0.2267 - acc: 0.8956 - top2metric: 0.9867 - top3metric: 0.9994 - val_loss: 0.1561 - val_acc: 0.9532 - val_top2metric: 0.9891 - val_top3metric: 1.0000\n",
      "Epoch 12/20\n",
      " - 250s - loss: 0.2237 - acc: 0.8957 - top2metric: 0.9869 - top3metric: 0.9996 - val_loss: 0.1513 - val_acc: 0.9545 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 13/20\n",
      " - 250s - loss: 0.2078 - acc: 0.9018 - top2metric: 0.9890 - top3metric: 0.9999 - val_loss: 0.1224 - val_acc: 0.9250 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 14/20\n",
      " - 251s - loss: 0.2153 - acc: 0.9020 - top2metric: 0.9888 - top3metric: 0.9994 - val_loss: 0.1752 - val_acc: 0.8909 - val_top2metric: 0.9900 - val_top3metric: 1.0000\n",
      "Epoch 15/20\n",
      " - 251s - loss: 0.2107 - acc: 0.9074 - top2metric: 0.9898 - top3metric: 0.9997 - val_loss: 0.1463 - val_acc: 0.9045 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 16/20\n",
      " - 251s - loss: 0.1905 - acc: 0.9107 - top2metric: 0.9927 - top3metric: 0.9999 - val_loss: 0.1369 - val_acc: 0.9214 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 17/20\n",
      " - 251s - loss: 0.1922 - acc: 0.9143 - top2metric: 0.9935 - top3metric: 0.9998 - val_loss: 0.1158 - val_acc: 0.9168 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 18/20\n",
      " - 251s - loss: 0.2141 - acc: 0.9096 - top2metric: 0.9908 - top3metric: 0.9992 - val_loss: 0.1131 - val_acc: 0.9427 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 19/20\n",
      " - 251s - loss: 0.1875 - acc: 0.9152 - top2metric: 0.9936 - top3metric: 0.9998 - val_loss: 0.1074 - val_acc: 0.9545 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 20/20\n",
      " - 251s - loss: 0.1649 - acc: 0.9248 - top2metric: 0.9946 - top3metric: 0.9999 - val_loss: 0.1150 - val_acc: 0.9005 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "5500/5500 [==============================] - 16s 3ms/step\n",
      "model scored 9.091000000000001% on D:/jake/DDHO Data/displacement/2018_09_18/tip8/0noise/*.npy\n",
      "one_diff_error = 9.09\n",
      "two_diff_error = 27.27\n",
      "three_diff_error = 9.09\n",
      "which taus failed were: [  0. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2018_09_18/tip9/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip2/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip3/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip4/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip5/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip6/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip7/0noise/*.npy\")\n",
    "train_x1, train_x2, train_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "#train model\n",
    "model = test.train_CNN(train_x1,train_x2,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "#load and test test data.  also save model and results\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2018_09_18/tip8/0noise/*.npy\")\n",
    "test.save_CNN('2018_09_19_expt_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakeprecht\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:215: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41800 samples, validate on 2200 samples\n",
      "Epoch 1/20\n",
      " - 256s - loss: 2.4225 - acc: 0.0904 - top2metric: 0.1797 - top3metric: 0.2711 - val_loss: 2.3986 - val_acc: 0.0905 - val_top2metric: 0.1836 - val_top3metric: 0.2705\n",
      "Epoch 2/20\n",
      " - 255s - loss: 2.3992 - acc: 0.0894 - top2metric: 0.1818 - top3metric: 0.2706 - val_loss: 2.3980 - val_acc: 0.0845 - val_top2metric: 0.1864 - val_top3metric: 0.2814\n",
      "Epoch 3/20\n",
      " - 255s - loss: 2.3987 - acc: 0.0919 - top2metric: 0.1805 - top3metric: 0.2679 - val_loss: 2.3979 - val_acc: 0.0895 - val_top2metric: 0.1741 - val_top3metric: 0.2650\n",
      "Epoch 4/20\n",
      " - 254s - loss: 2.3992 - acc: 0.0940 - top2metric: 0.1872 - top3metric: 0.2773 - val_loss: 2.3920 - val_acc: 0.0909 - val_top2metric: 0.1832 - val_top3metric: 0.2709\n",
      "Epoch 5/20\n",
      " - 255s - loss: 1.3470 - acc: 0.4333 - top2metric: 0.6343 - top3metric: 0.7592 - val_loss: 0.6251 - val_acc: 0.7027 - val_top2metric: 0.8241 - val_top3metric: 0.9277\n",
      "Epoch 6/20\n",
      " - 255s - loss: 0.6282 - acc: 0.7218 - top2metric: 0.8618 - top3metric: 0.9510 - val_loss: 0.3820 - val_acc: 0.8009 - val_top2metric: 0.9223 - val_top3metric: 1.0000\n",
      "Epoch 7/20\n",
      " - 255s - loss: 0.5074 - acc: 0.7692 - top2metric: 0.9015 - top3metric: 0.9809 - val_loss: 0.3129 - val_acc: 0.8305 - val_top2metric: 0.9191 - val_top3metric: 1.0000\n",
      "Epoch 8/20\n",
      " - 255s - loss: 0.4315 - acc: 0.7992 - top2metric: 0.9190 - top3metric: 0.9909 - val_loss: 0.2901 - val_acc: 0.8600 - val_top2metric: 0.9868 - val_top3metric: 1.0000\n",
      "Epoch 9/20\n",
      " - 255s - loss: 0.4236 - acc: 0.8061 - top2metric: 0.9232 - top3metric: 0.9904 - val_loss: 0.2957 - val_acc: 0.8723 - val_top2metric: 0.9886 - val_top3metric: 1.0000\n",
      "Epoch 10/20\n",
      " - 255s - loss: 0.4082 - acc: 0.8137 - top2metric: 0.9302 - top3metric: 0.9923 - val_loss: 0.3510 - val_acc: 0.7845 - val_top2metric: 0.9059 - val_top3metric: 1.0000\n",
      "Epoch 11/20\n",
      " - 255s - loss: 0.3810 - acc: 0.8229 - top2metric: 0.9355 - top3metric: 0.9952 - val_loss: 0.2944 - val_acc: 0.8718 - val_top2metric: 0.9609 - val_top3metric: 1.0000\n",
      "Epoch 12/20\n",
      " - 255s - loss: 0.4130 - acc: 0.8148 - top2metric: 0.9347 - top3metric: 0.9931 - val_loss: 0.2379 - val_acc: 0.9177 - val_top2metric: 0.9864 - val_top3metric: 1.0000\n",
      "Epoch 13/20\n",
      " - 255s - loss: 0.3620 - acc: 0.8330 - top2metric: 0.9423 - top3metric: 0.9955 - val_loss: 0.3236 - val_acc: 0.8523 - val_top2metric: 0.9395 - val_top3metric: 0.9873\n",
      "Epoch 14/20\n",
      " - 255s - loss: 0.3482 - acc: 0.8393 - top2metric: 0.9473 - top3metric: 0.9967 - val_loss: 0.2729 - val_acc: 0.8409 - val_top2metric: 0.9332 - val_top3metric: 1.0000\n",
      "Epoch 15/20\n",
      " - 255s - loss: 0.3349 - acc: 0.8460 - top2metric: 0.9545 - top3metric: 0.9983 - val_loss: 0.2632 - val_acc: 0.8623 - val_top2metric: 0.9500 - val_top3metric: 1.0000\n",
      "Epoch 16/20\n",
      " - 255s - loss: 0.3176 - acc: 0.8542 - top2metric: 0.9606 - top3metric: 0.9989 - val_loss: 0.1845 - val_acc: 0.9545 - val_top2metric: 1.0000 - val_top3metric: 1.0000\n",
      "Epoch 17/20\n",
      " - 255s - loss: 0.2948 - acc: 0.8648 - top2metric: 0.9670 - top3metric: 0.9984 - val_loss: 0.3072 - val_acc: 0.8150 - val_top2metric: 0.9059 - val_top3metric: 1.0000\n",
      "Epoch 18/20\n",
      " - 255s - loss: 0.2980 - acc: 0.8662 - top2metric: 0.9697 - top3metric: 0.9992 - val_loss: 0.2442 - val_acc: 0.8491 - val_top2metric: 0.9514 - val_top3metric: 1.0000\n",
      "Epoch 19/20\n",
      " - 255s - loss: 0.2936 - acc: 0.8665 - top2metric: 0.9688 - top3metric: 0.9990 - val_loss: 0.2876 - val_acc: 0.8532 - val_top2metric: 0.9568 - val_top3metric: 0.9909\n",
      "Epoch 20/20\n",
      " - 255s - loss: 0.2742 - acc: 0.8715 - top2metric: 0.9736 - top3metric: 0.9990 - val_loss: 0.1859 - val_acc: 0.8755 - val_top2metric: 0.9864 - val_top3metric: 1.0000\n",
      "5500/5500 [==============================] - 16s 3ms/step\n",
      "model scored 27.272999999999996% on D:/jake/DDHO Data/displacement/2018_09_18/tip9/0noise/*.npy\n",
      "one_diff_error = 36.36\n",
      "two_diff_error = 9.09\n",
      "three_diff_error = 18.18\n",
      "which taus failed were: [  0. 500. 500. 500. 500. 500. 500. 500.   0.   0. 500.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2018_09_18/tip8/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip2/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip3/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip4/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip5/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip6/0noise/*.npy\",\n",
    "                                           \"D:/jake/DDHO Data/displacement/2018_09_18/tip7/0noise/*.npy\")\n",
    "train_x1, train_x2, train_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "#train model\n",
    "model = test.train_CNN(train_x1,train_x2,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "#load and test test data.  also save model and results\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2018_09_18/tip9/0noise/*.npy\")\n",
    "test.save_CNN('2018_09_19_expt_9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.visualize_weights(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/random_noise_1/*.npy\")\n",
    "#train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and test test data.  also save model and results\n",
    "\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/no_noise/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/0pt1noise/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/1noise/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/random_noise_1/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/displacement/2 per decade 10 ns to 1 ms slow only/random_noise_10/*.npy\")\n",
    "#test.test_experimental_CNN(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\")\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run3\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run8\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run22\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run28\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "test.save_CNN('2018_08_23 simulated displacement train rand noise 1 slow time only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.visualize_weights(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "train_data = test.load_simulated_train_data(\"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/no_noise/*.npy\")\n",
    "#train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load and test test data.  also save model and results\n",
    "\n",
    "test.test_simulated_CNN(\"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/0pt1noise/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/1noise/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/random_noise_1/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/random_noise_10/*.npy\",\n",
    "                       \"D:/jake/DDHO Data/inst_freq/2 per decade 10 ns to 1 ms 300 kHz/no_noise/*.npy\")\n",
    "#test.test_experimental_CNN(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\")\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run3\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run8\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run22\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run28\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "test.save_CNN('2018_08_29 simulated inst_freq train no_noise 300 kHz 400 100 KL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.visualize_weights(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_vis = train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_to_vis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.layer_to_visualize(1, img_to_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.visualize_weights(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "\n",
    "train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         \"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and test experimental test data.  also save model and results\n",
    "test.test_experimental_CNN(\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                          \"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                          \"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                          \"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\")\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run3\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run8\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run22\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run28\",\n",
    "                          #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "test.save_CNN('test_save_with_which_taus_failed_metric_2_20_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "#train_data = test.load_train_data()\n",
    "train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load and test synthetic test data.  also save model and results\n",
    "test.test_CNN(test_fraction=0.1)\n",
    "test.save_CNN('modular_CNN_Durmus_data_2018_06_21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and test experimental test data.  also save model and results\n",
    "test.test_experimental_CNN(\"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run29\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run30\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run31\")\n",
    "test.save_CNN('2018_08_14 train 30 test 29 30 31 slow timescale only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "#train_data = test.load_train_data()\n",
    "train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run1\",\n",
    "                                              \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run2\",\n",
    "                                              \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run29\",\n",
    "                                              \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load and test synthetic test data.  also save model and results\n",
    "test.test_CNN(test_fraction=0.1)\n",
    "test.save_CNN('modular_CNN_Durmus_data_2018_06_21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and test experimental test data.  also save model and results\n",
    "test.test_experimental_CNN(\"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run1\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run2\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run3\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run5\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run10\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run15\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run20\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run25\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run28\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run29\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run30\",\n",
    "                           \"D:/Jake/DDHO data/jake_data/2018_08_14 2 samples per decade BD/Run31\")\n",
    "test.save_CNN('2018_08_15 train 1 2 29 30 test many slow timescales only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and test experimental test data.  also save model and results\n",
    "test.test_experimental_CNN_averaged(\"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run5\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run6\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run7\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run8\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run9\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run10\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run20\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run30\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run40\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run43\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run44\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run45\")\n",
    "test.save_CNN('2018_07_19 train 5_6_7_43_44_45 test averaged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "#train_data = test.load_train_data()\n",
    "train_data = test.load_experimental_train_data(\"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run1\",\n",
    "                                              \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run2\",\n",
    "                                              \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run3\")\n",
    "                                         #\"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run6\",\n",
    "                                         #\"D:/Jake/DDHO data/jake_data/2018_07_19 first attempt at voltage pulse/Run7\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run5\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run10\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run15\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run20\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\",\n",
    "                                        #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run30\")\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run1\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V5/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V7/Run25\",\n",
    "                                         #\"D:/Jake/DDHO data/durmus_data/voltage scan/BD_V10/Run25\")\n",
    "train_x, train_y = test.preprocess_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = test.train_CNN(train_x,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load and test synthetic test data.  also save model and results\n",
    "test.test_CNN(test_fraction=0.1)\n",
    "test.save_CNN('modular_CNN_Durmus_data_2018_06_21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load and test experimental test data.  also save model and results\n",
    "test.test_experimental_CNN(\"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run1\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run2\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run3\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run4\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run5\",\n",
    "                          \"D:/Jake/DDHO data/jake_data/2018_08_01 2 samples per decade/Run6\")\n",
    "test.save_CNN('2018_08_01 first test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = test.model.layers[3].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_plt = weights[:,:,0]\n",
    "weight_plt.shape\n",
    "#weight_plt2 = weight_plt.reshape((400,))\n",
    "#plt.plot(weight_plt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = test.model.layers[1].get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

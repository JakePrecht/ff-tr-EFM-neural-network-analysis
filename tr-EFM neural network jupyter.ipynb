{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import configparser\n",
    "import sklearn.utils\n",
    "from igor.binarywave import load as loadibw\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Conv1D, MaxPooling1D, Flatten, Merge\n",
    "from keras.utils import np_utils\n",
    "from keras import callbacks\n",
    "from keras import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "\n",
    "class CNN():\n",
    "    \"\"\"This class contains every function needed to load training data, train a neural network, test a neural network, \n",
    "    save the trained network, and visualize various metrics/parameters.  See specific functions for more in-depth \n",
    "    explanations, or cells below this cell for examples of using this class.\n",
    "    \n",
    "    This neural network is designed to work with two different types of input data:\n",
    "    simulated or experimentally measured cantilever displacement data.\n",
    "    The simulations are run in Python (utilizing generate_train_data.py) and are in .npy format\n",
    "    The experiments are run on an Asylum Cypher ES atomic force microscope which uses Igor so the experimental data\n",
    "    are collected into .ibw files\"\"\"\n",
    "    \n",
    "        \n",
    "    def load_experimental_train_data(self, *paths, sample_rate = 10, total_time = 3.2768, pre_trigger = 0.5):\n",
    "        \"\"\"This function loads experimental displacement training data generated by the Cypher using my point scan script.\n",
    "        \n",
    "        Inputs:\n",
    "        sample_rate = 10 or 100 (Gage card sampling rate in MS)\n",
    "        total_time = total time of acquisition in ms\n",
    "        pre_trigger = at what percentage of the total acquisition time does the trigger occur?  A parameter set during AFM data acquisition\n",
    "        *paths = e.g. (\"G:/2018_10_05 CNN voltage pulses/New tip 3/Run1\",\n",
    "                       \"G:/2018_10_05 CNN voltage pulses/New tip 4/Run1\",\n",
    "                       \"G:/2018_10_05 CNN voltage pulses/New tip 5/Run1\")\n",
    "        where each path is to a folder containing several items:\n",
    "            1 to x) Subfolders named 0, 1, ... , x-1 where the training data for each 1 of the x Taus in the training data is in its own subfolder\n",
    "            x+1) Within the Run folder (so same folder as all the subfolders \"0\", \"1\", etc.) there needs to be a .cfg file with parameters k, Q, omega named 'Parameters.cfg'\n",
    "        So, for example, if Tau is {10,100,1000}, Run1 folder would contain:\n",
    "        Subfolder \"0\"\n",
    "        Subfolder \"1\"\n",
    "        Subfolder \"2\"\n",
    "        Parameters.cfg\n",
    "        ^It is imperative that the above naming convention is followed\n",
    "        \n",
    "        \n",
    "        Outputs:\n",
    "        df_main = a Pandas DataFrame containing all of the z(t) data #as well as k, Q, omega, and Tau for each point scan\n",
    "        \"\"\"\n",
    "        \n",
    "        #taus array gives the values of tau for each Runx.  They need to be in ascending order (i.e. taus[0] = tau for Run0, taus[1] = tau for Run1, etc.)  The requirement for inputting ascending order can easily be fixed by a sort function but Taus is not a parameter I have ever changed yet\n",
    "        taus= np.array([10e-9, 31.62e-9, 100e-9, 316.2e-9, 1e-6, 3.162e-6, 10e-6, 31.62e-6, 100e-6, 316.2e-6, 1e-3])\n",
    "        \n",
    "        #Pre-calculate important indices\n",
    "        total_time = total_time * 1e-3\n",
    "        sample_rate = sample_rate * 1e6\n",
    "        total_points = total_time * sample_rate\n",
    "        self.trigger_index = int(pre_trigger * total_points)\n",
    "        #print('trigger index is ' + str(self.trigger_index))\n",
    "        \n",
    "        \n",
    "        #Initialize DataFrame for collecting each run\n",
    "        df_main = pd.DataFrame()\n",
    "        \n",
    "        for i in paths:\n",
    "            \n",
    "            #grab paths for upcoming data loading\n",
    "            params_path = i + \"/Parameters.cfg\"\n",
    "            tau_paths = [item for item in glob.glob(i + \"/*\") if not os.path.basename(item).startswith('Param')]\n",
    "            sorted_tau_paths = sorted(tau_paths, key = lambda x: int(os.path.basename(os.path.normpath(x))))\n",
    "            #Above line properly sorts so that they are ordered 0,1,2,3,4,5,6,7,8,9,10 rather than 0,1,10,2,3,4...\n",
    "    \n",
    "            #Initialize DataFrame for this run\n",
    "            df_run = pd.DataFrame()\n",
    "            \n",
    "            #Loop through every tau in that run and load it in\n",
    "            for j in range(0,len(tau_paths)): #CHANGE THIS RANGE IF YOU WANT TO TRAIN ON DIFFERENT TIME SCALES.  CURRENTLY IS TRAINING ON ALL TIME SCALES\n",
    "                tau = taus[j]\n",
    "                displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "                #collect relevent parts of the data into df_run\n",
    "                for k in range(len(displacement_path)):\n",
    "                    #load displacement data from .ibw\n",
    "                    disp_array = loadibw(displacement_path[k])['wave']['wData'] \n",
    "                    #throw away all displacement before the trigger\n",
    "                    disp_array = disp_array[self.trigger_index:,:]\n",
    "                    disp_array = np.transpose(disp_array)\n",
    "        \n",
    "                    #Init columns for df\n",
    "                    columns=[]\n",
    "                    for l in range(disp_array.shape[1]):\n",
    "                        columns.append('t='+str(l))\n",
    "                    \n",
    "                    #init temporary DataFrame for appending this Tau's data into the main DataFrame for this run\n",
    "                    df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "                    \n",
    "                    #Commented out code below is for a feature still under development (implementing multiple tips)\n",
    "                    \"\"\"\n",
    "                    #load other parameters\n",
    "                    config = configparser.RawConfigParser()\n",
    "                    config.read(params_path)\n",
    "                    \n",
    "                    for (each_key,each_value) in config.items('Parameters'):\n",
    "                        setattr(self,each_key,config.getfloat('Parameters',each_key))\n",
    "                    \n",
    "                    df_temp['k'] = self.k\n",
    "                    df_temp['Q'] = self.q\n",
    "                    df_temp['omega'] = self.omega\"\"\"\n",
    "                    \n",
    "                    df_temp['Tau'] = pd.Series(index=df_temp.index) #create Tau column\n",
    "                    df_temp['Tau'] = tau #assign tau value to tau column (could probably be done in above step with data=tau flag?)\n",
    "                    \n",
    "                    df_run = df_run.append(df_temp,ignore_index=True) #append each tau value to this run          \n",
    "            \n",
    "            df_main = pd.concat([df_main,df_run], ignore_index=True) #append each run to the final collection DataFrame\n",
    "            \n",
    "        return df_main\n",
    "    \n",
    "        \n",
    "    \n",
    "    def preprocess_train_data(self,train_data, val_split = 0.2):\n",
    "        \"\"\"This function takes the raw training data (simulated or experimental) and prepares it for the machine learning input\n",
    "        It normalizes the data about zero\n",
    "        It also separates training and validation data and preprocesses them separately (training data first, then validation data with training data's settings to prevent data leakage)\n",
    "        \n",
    "        Inputs: \n",
    "        train_data = raw training data from load_experimental_train_data() or load_simulated_train_data()\n",
    "        val_split = \n",
    "        \n",
    "        Outputs:\n",
    "        train_x1_norm_reshaped = training x data (everything except the known, true Tau value)\n",
    "        val_x1_norm_reshaped = validation x data (everything except the known, true Tau value)\n",
    "        train_one_hot_tau = training data's known true Tau values in a one hot encoding\n",
    "        val_one_hot_tau = validation data's known true Tau values in a one hot encoding\n",
    "        \"\"\"\n",
    "        \n",
    "        #shuffle the training data since I load data sequentially.\n",
    "        #Shuffling makes the validation set a true representation of the data.\n",
    "        train_data = sklearn.utils.shuffle(train_data, random_state = 7)\n",
    "        \n",
    "        num_val_samples = int(val_split * len(train_data.index)) #calculate how many validation samples there are\n",
    "        val_x = train_data.tail(num_val_samples) #select the validation data from the total training pool\n",
    "        train_x = train_data[:int(-1*num_val_samples)] #select only the training split from the total training pool\n",
    "        \n",
    "        train_y = np.array(train_x['Tau']) #labeled, true Tau values for the CNN to learn from\n",
    "        val_y = np.array(val_x['Tau']) #labeled, true Tau values for the CNN to learn from\n",
    "        \n",
    "        #train_x1 = train_x.drop(['k','Q','omega','Tau'],axis=1) #Many tips implementation\n",
    "        train_x1 = train_x.drop(['Tau'],axis=1) #drop y values from training and validation data\n",
    "        val_x1 = val_x.drop(['Tau'],axis=1)\n",
    "        \n",
    "        #train_x2 = train_x[['k','Q','omega']] #Many tips implementation\n",
    "        \n",
    "        self.mean_train_x1 = np.mean(train_x1) #saving the mean_train_x for preprocessing the validation/test data in the same manner as our training dat\n",
    "        #self.mean_train_x2 = np.mean(train_x2) #Many tips implementation\n",
    "        \n",
    "        self.SD_train_x1 = np.std(train_x1) #saving the SD_train_x for preprocessing the validation/test data in the same manner as our training data\n",
    "        #self.SD_train_x2 = np.std(train_x2) #Many tips implementation\n",
    "        \n",
    "        train_x1_norm = (train_x1 - self.mean_train_x1) /  (self.SD_train_x1) #normalize and centralize the training data for best neural network performance\n",
    "        train_x1_norm_reshaped = np.expand_dims(train_x1_norm,axis=2) #formatting for input into CNN\n",
    "        \n",
    "        val_x1_norm = (val_x1 - self.mean_train_x1) /  (self.SD_train_x1) #normalize and centralize the training data for best neural network performance\n",
    "        val_x1_norm_reshaped = np.expand_dims(val_x1_norm,axis=2) #formatting for input into CNN\n",
    "        \n",
    "        #Many tips implementation\n",
    "        #train_x2_norm = (train_x2 - self.mean_train_x2) /  (self.SD_train_x2) #normalize and centralize the training data for best neural network performance\n",
    "        #train_x2_norm_reshaped = np.expand_dims(train_x2_norm,axis=2) #formatting for input into CNN\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        #Label encode the y-data by one hot-encoding for proper format for classification based NN:\n",
    "\n",
    "        #tau_index is used to recover the original tau's from a one-hot encoded output.\n",
    "        #e.g. tau = [10, 100, 1000, 10, 10] then\n",
    "        #unique_tau = [10, 100, 1000]\n",
    "        #tau_index = [0,1,2,0,0] is index of tau to corresponding unique_tau so\n",
    "        #unique_tau[tau_index] == tau \n",
    "        unique_tau_train, tau_index_train = np.unique(train_y,return_inverse=True)\n",
    "        unique_tau_val, tau_index_val = np.unique(val_y,return_inverse=True)\n",
    "\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau_train = np_utils.to_categorical(tau_index_train)\n",
    "        one_hot_tau_val = np_utils.to_categorical(tau_index_val)\n",
    "\n",
    "        self.number_of_classes = one_hot_tau_train.shape[1] #used to match number of output Softmax layers in my NN\n",
    "        \n",
    "        return train_x1_norm_reshaped, one_hot_tau_train, val_x1_norm_reshaped, one_hot_tau_val #train_x2_norm_reshaped, one_hot_tau\n",
    "    \n",
    "    \n",
    "    #Many tips implementation\n",
    "    #def train_CNN(self, train_x1, train_x2, train_y, num_epochs = 40, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3):\n",
    "    def train_CNN(self, train_x1, train_y, val_x, val_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3):\n",
    "        #Build CNN and start training!\n",
    "\n",
    "        #Save class variables for later use in visualize_weights()\n",
    "        self.filter_number1 = num_filter1\n",
    "        self.filter_number2 = num_filter2\n",
    "        self.kernel1_size = kernel1_size\n",
    "        self.kernel2_size = kernel2_size\n",
    "        \n",
    "        #Save class variable for later use in plot_training_statistics()\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        \n",
    "        #Initialize CNN branch 1 for main convolutional input data (displacement or instantaneous frequency)\n",
    "        #WHILE DEBUGGING I CHANGED \"BRANCH1 to MODEL\".  \n",
    "        #Change \"model\" to \"branch1\" for returning to many tips implementation!\n",
    "        model = Sequential()\n",
    "\n",
    "        #Add convolution layers\n",
    "        model.add(Conv1D(filters=num_filter1,kernel_size=kernel1_size,strides=2,padding='same',input_shape=(train_x1.shape[1],1)))\n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "        model.add(MaxPooling1D())\n",
    "\n",
    "        model.add(Conv1D(filters=num_filter2,kernel_size=kernel2_size,strides=2,padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "        model.add(MaxPooling1D())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        #Roughly 500 units length of branch 1 (8000 displacement points / (2**4 because each strides = 2 and each maxpool halves data length))\n",
    "        \n",
    "        #Add fully connected layers (maybe remove for many tips implementation?  Check notes on arcitecture experiments I did)\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal',activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal',activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        #Many tips implementation\n",
    "        \"\"\"\n",
    "        #Initialize CNN branch 2 for supplementary data (Q, k, and omega)\n",
    "        branch2 = Sequential()\n",
    "\n",
    "        #Add supplementary data inputs\n",
    "        branch2.add(Dense(units=100, kernel_initializer='he_normal', activation='linear', input_shape=(train_x2.shape[1],1)))\n",
    "        branch2.add(LeakyReLU(alpha=.01))\n",
    "        branch2.add(Dropout(0.3))\n",
    "        \n",
    "        branch2.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        branch2.add(LeakyReLU(alpha=.01))\n",
    "        branch2.add(Dropout(0.4))\n",
    "        \n",
    "        branch2.add(Flatten())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Merge branches 1 and 2\n",
    "        model = Sequential()\n",
    "        model.add(Merge([branch1,branch2], mode='concat'))\n",
    "\n",
    "        \n",
    "        #Add final fully connected layers\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Dense(units=100, kernel_initializer='he_normal', activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=.01))\n",
    "        model.add(Dropout(0.4))\n",
    "        \"\"\"\n",
    "\n",
    "        #Add classification layer\n",
    "        #If attempting regression modeling instead of tau bucketing, change here\n",
    "        model.add(Dense(units=self.number_of_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "        #Compile CNN and configure metrics/learning process\n",
    "        \"\"\"below functions are failure metrics that tell me if the true tau was in the top 2, top 3, or top 5 guesses made by the neural network\"\"\"\n",
    "        def inTop2(k=2):\n",
    "            def top2metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=2)\n",
    "            return top2metric\n",
    "        \n",
    "        def inTop3(k=3):\n",
    "            def top3metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=3)\n",
    "            return top3metric\n",
    "        \n",
    "        def inTop5(k=5):\n",
    "            def top5metric(y_true,y_pred):\n",
    "                return metrics.top_k_categorical_accuracy(y_true,y_pred,k=5)\n",
    "            return top5metric\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy', inTop2(), inTop3()])\n",
    "\n",
    "        #Prepare for visualization if using TensorBoard\n",
    "        #tbCallBack = callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "        #tbCallBack = callbacks.TensorBoard(log_dir=\"logs/{}\".format(time.time()))\n",
    "\n",
    "        #Train model\n",
    "        #Many tips implementation\n",
    "        #model.fit([train_x1, train_x2], train_y, batch_size=32, epochs=num_epochs,verbose=2, validation_split=0.05)#, callbacks=[tbCallBack])\n",
    "        history = model.fit(train_x1, train_y, batch_size=32, epochs=num_epochs,verbose=2, validation_data=(val_x,val_y))#, callbacks=[tbCallBack])\n",
    "        self.model = model #save model to self for calling from other functions later\n",
    "        \n",
    "        \n",
    "        #Save branches as class variables for visualize_weights()\n",
    "        self.branch1 = model\n",
    "        #Many tips implementation -- this has not been debugged and may be inaccurate\n",
    "        #self.branch1 = branch1\n",
    "        #self.branch2 = branch2\n",
    "        #self.model = model\n",
    "        return history\n",
    "    \n",
    "    \n",
    "    def plot_training_statistics(self, history):\n",
    "        \"\"\"Plot training data accuracy and validation data accuracy as a function of training epoch number\n",
    "        Inputs: history (dictionary of model fitting metrics--it is the output variable from train_CNN())\n",
    "        Outputs: inline figure of training accuracy vs validation accuracy\"\"\"\n",
    "        \n",
    "        num_epochs = self.num_epochs\n",
    "\n",
    "        #extract relevent information from history\n",
    "        train_loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        train_acc = model.history['acc']\n",
    "        val_acc = model.history['val_acc']\n",
    "\n",
    "        #x axis is num_epochs\n",
    "        x = range(1,self.num_epochs + 1)\n",
    "        xticks = range(1,self.num_epochs + 1)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows = 2)\n",
    "\n",
    "        ax1.plot(x, train_loss, label = 'Training')\n",
    "        ax1.plot(x, val_loss, label = 'Validation')\n",
    "        ax1.legend(loc = 'upper right')\n",
    "        if self.num_epochs < 6:\n",
    "            ax1.set_xticks(xticks)\n",
    "        else:\n",
    "            ax1.xaxis.set_major_locator(MaxNLocator(integer=True,steps=[1,2,5,10]))\n",
    "        ax1.set_ylabel('Loss')\n",
    "        \n",
    "\n",
    "        ax2.plot(x, train_acc, label = 'Training')\n",
    "        ax2.plot(x, val_acc, label = 'Validation')\n",
    "        ax2.legend(loc = 'lower right')\n",
    "        if self.num_epochs < 6:\n",
    "            ax2.set_xticks(xticks)\n",
    "        else:\n",
    "            ax2.xaxis.set_major_locator(MaxNLocator(integer=True,steps=[1,2,5,10]))\n",
    "        ax2.set_xlabel('Number of Epochs')\n",
    "        ax2.set_ylim(top=1)\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_experimental_test_data(self, path):\n",
    "        \"\"\"input string with file path\n",
    "    \n",
    "        outputs the requested percentage of the data (test_data_norm_reshaped) and their corresponding labels for evaluation (one_hot_tau)\n",
    "        these outputs are basically test_x and test_y that are formatted to be fed into model.evaluate()\n",
    "    \n",
    "        NOTE this function requires training functions to have been run (mean_train_x and SD_train_x must exist!!!)\n",
    "        ^^^This means for production, mean_train_x and SD_train_x must be saved variables that I import along with my trained NN\"\"\"\n",
    "        #taus = np.array([10e-9, 25e-9, 50e-9, 100e-9, 250e-9, 500e-9,\n",
    "        #                        1e-6, 5e-6, 10e-6, 100e-6, 1e-3])\n",
    "        taus= np.array([10e-9, 31.62e-9, 100e-9, 316.2e-9, 1e-6, 3.162e-6, 10e-6, 31.62e-6, 100e-6, 316.2e-6, 1e-3])\n",
    "        \n",
    "        params_path = path + \"/Parameters.cfg\"\n",
    "        tau_paths = [item for item in glob.glob(path + \"/*\") if not os.path.basename(item).startswith('Param')]\n",
    "        sorted_tau_paths = sorted(tau_paths, key = lambda x: int(os.path.basename(os.path.normpath(x))))\n",
    "        #Above line properly sorts so that they are ordered 0,1,2,3,4,5,6,7,8,9,10 rather than 0,1,10,2,3,4...\n",
    "        \n",
    "        #Init dataframe for this run.  This DataFrame will collect each array of data from each folder in for loop below\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        for j in range(0,len(tau_paths)):\n",
    "            tau = taus[j]\n",
    "            displacement_path = glob.glob(sorted_tau_paths[j]+\"/*\")\n",
    "            \n",
    "            disp_array = loadibw(displacement_path[0])['wave']['wData'] #load displacement from .ibw file into python\n",
    "            disp_array = disp_array[self.trigger_index:,:] #throw away all displacement before the trigger\n",
    "            disp_array = np.transpose(disp_array)\n",
    "        \n",
    "            #Put loaded stuff into dataframe and label tau\n",
    "            columns=[]\n",
    "            for k in range(disp_array.shape[1]):\n",
    "                columns.append('t='+str(k))\n",
    "        \n",
    "            df_temp = pd.DataFrame(data=disp_array, columns=columns)\n",
    "            \n",
    "            #Many tips implementation\n",
    "            \"\"\"\n",
    "            #load other parameters\n",
    "            config = configparser.RawConfigParser()\n",
    "            config.read(params_path)\n",
    "                    \n",
    "            #Get k, Q, omega from .txt\n",
    "            for (each_key,each_value) in config.items('Parameters'):\n",
    "                setattr(self,each_key,config.getfloat('Parameters',each_key))\n",
    "                    \n",
    "            #JAKE INSERT df_temp['k'] = imported k value, etc for Q, omega\n",
    "            df_temp['k'] = self.k\n",
    "            df_temp['Q'] = self.q\n",
    "            df_temp['omega'] = self.omega\n",
    "            \"\"\"\n",
    "                \n",
    "            df_temp['Tau'] = pd.Series(index=df_temp.index)\n",
    "            df_temp['Tau'] = tau\n",
    "            df = df.append(df_temp,ignore_index=True)\n",
    "        \n",
    "     \n",
    "        test_y = np.array(df['Tau'])\n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "        unique_tau, tau_index = np.unique(test_y,return_inverse=True)\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "    \n",
    "        #preprocess test_x\n",
    "        #test_x1 = df.drop(['k','Q','omega','Tau'],axis=1) #x1 is just displacement #Many tips implementation\n",
    "        test_x1 = df.drop(['Tau'],axis=1) #x1 is just displacement\n",
    "        #test_x2 = df[['k','Q','omega']]  #Many tips implementation\n",
    "        \n",
    "        test_x1_norm = (test_x1 - self.mean_train_x1 ) /  (self.SD_train_x1) #important to preprocess my test data same as my train data!!!!\n",
    "        #test_x2_norm = (test_x2 - self.mean_train_x2 ) /  (self.SD_train_x2) #important to preprocess my test data same as my train data!!!!\n",
    "        \n",
    "        test_x1_norm_reshaped = np.expand_dims(test_x1_norm,axis=2)\n",
    "        #test_x2_norm_reshaped = np.expand_dims(test_x2_norm,axis=2) #Many tips implementation\n",
    "    \n",
    "        return test_x1_norm_reshaped, one_hot_tau #test_x2_norm_reshaped, one_hot_tau\n",
    "    \n",
    "    \n",
    "    #Many tips implementation\n",
    "    #def test_closeness(self, test_x1, test_x2, test_y):\n",
    "    def test_closeness(self, test_x1, test_y):\n",
    "        \"\"\"This function looks at the predicted tau values from model.predict(test_x) and compares them \n",
    "        to the true tau values from test_y.  \n",
    "        It then returns three values telling you what percentage of the incorrect predictions varied by spacing of\n",
    "        one tau value, two tau values, or three tau values.\n",
    "        \n",
    "        E.G.\n",
    "        Say possible taus = [1,2,3,4,5,6,7,8,9]\n",
    "        model.predict(test_x) = [2,2,2,3,3,3,4,4,5,6]\n",
    "        test_y = [2,2,2,2,2,2,2,2,2,2]\n",
    "        \n",
    "        test_closeness returns [0.3,0.2,0.1]\n",
    "        because 30% of the predictions varied by one tau value (tau = 2 but 3 times it guessed tau = 3)\n",
    "        because 20% of the predictions varied by two tau values (tau = 2 but 2 times it guessed tau = 4)\n",
    "        because 10% of the predictions varied by three tau values (tau = 2 but 1 time it guessed tau = 5)\n",
    "        \"\"\"\n",
    "        \n",
    "        #pred_tau = self.model.predict([test_x1,test_x2],verbose=0) #Many tips implementation\n",
    "        pred_tau = self.model.predict(test_x1,verbose=0)\n",
    "        \n",
    "        pred_tau_am = pred_tau.argmax(axis=-1) #pluck out actual prediction value!\n",
    "        test_y_am = test_y.argmax(axis=-1) #does not actually need argmax function, but this makes it same format as pred_tau_am which is necessary\n",
    "        \n",
    "        incorrect_indices = np.nonzero(pred_tau_am != test_y_am) #indices of incorrect predictions\n",
    "        \n",
    "        total_samples = len(pred_tau)\n",
    "        total_fails = len(incorrect_indices[0])\n",
    "        \n",
    "        #init diff collection variables (how many tau values away the true value was from the predicted value)\n",
    "        num_diff_1 = 0\n",
    "        num_diff_2 = 0\n",
    "        num_diff_3 = 0\n",
    "        num_greater = 0\n",
    "        \n",
    "        #init array for seeing which taus it is bad at predicting\n",
    "        which_taus_failed = np.zeros(self.number_of_classes)\n",
    "        \n",
    "        for element in incorrect_indices[0]:\n",
    "            \n",
    "            #collect diff (how many tau values away the true value was from the predicted value)\n",
    "            diff = abs(pred_tau_am[element] - test_y_am[element])\n",
    "            if diff == 1:\n",
    "                num_diff_1 += 1\n",
    "            elif diff == 2:\n",
    "                num_diff_2 += 1\n",
    "            elif diff == 3:\n",
    "                num_diff_3 += 1\n",
    "            else:\n",
    "                num_greater += 1\n",
    "            \n",
    "            #collect how many of each tau failed\n",
    "            i=0\n",
    "            while True:\n",
    "                if test_y_am[element] == i:\n",
    "                    which_taus_failed[i] += 1\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "                    \n",
    "            which_taus_failed_percent = np.round((which_taus_failed / total_fails),4) * 100\n",
    "\n",
    "        \n",
    "        percent_num_diff_1 = round((num_diff_1 / total_samples), 4) * 100\n",
    "        percent_num_diff_2 = round((num_diff_2 / total_samples), 4) * 100\n",
    "        percent_num_diff_3 = round((num_diff_3 / total_samples), 4) * 100\n",
    "        percent_num_diff_greater = round((num_greater / total_samples), 4) * 100\n",
    "            \n",
    "        #Next section is for debugging purposes\n",
    "        #percent_incorrect = (len(incorrect_indices[0])/total_samples)\n",
    "        #percent_incorrect_calculated = percent_num_diff_1 + percent_num_diff_2 + percent_num_diff_3 + percent_num_diff_greater\n",
    "        #print('percent incorrect should be ' + str(percent_incorrect))\n",
    "        #print('percent incorrect calculated is ' + str(percent_incorrect_calculated))\n",
    "        \n",
    "        return percent_num_diff_1, percent_num_diff_2, percent_num_diff_3, which_taus_failed#_percent\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def test_experimental_CNN(self, *paths):\n",
    "        \n",
    "        score_string = 'data order is testing against '\n",
    "        for element in paths:\n",
    "            score_string += (str(element) + \" , \\n\") \n",
    "\n",
    "        \n",
    "        score_collect = [score_string]\n",
    "        score_collect.append('column order is loss, accuracy, top2metric, top3metric')\n",
    "        score_collect.append('top2metric = % that the true tau was one of the top 2 predictions')\n",
    "        score_collect.append(' ')\n",
    "        \n",
    "        for element in paths:\n",
    "            #test_x1, test_x2, test_y = self.load_experimental_test_data(element) #Many tips implementation\n",
    "            test_x1, test_y = self.load_experimental_test_data(element)\n",
    "            #score = self.model.evaluate([test_x1,test_x2],test_y,batch_size = 32) #Many tips implementation\n",
    "            score = self.model.evaluate(test_x1,test_y,batch_size = 32)\n",
    "            percentage = str(round(score[1],5) * 100)\n",
    "            print('model scored ' + percentage + '% on ' + str(element))\n",
    "            score_collect.append(str(score))\n",
    "            \n",
    "            \n",
    "            #error1, error2, error3, which_taus_failed = self.test_closeness(test_x1,test_x2,test_y) #Many tips implementation\n",
    "            error1, error2, error3, which_taus_failed = self.test_closeness(test_x1,test_y)\n",
    "            score_collect.append('one_diff_error = ' + str(error1))\n",
    "            score_collect.append('two_diff_error = ' + str(error2))\n",
    "            score_collect.append('three_diff_error = ' + str(error3))\n",
    "            score_collect.append('which taus failed were: ' + str(which_taus_failed))\n",
    "            print('one_diff_error = ' + str(error1))\n",
    "            print('two_diff_error = ' + str(error2))\n",
    "            print('three_diff_error = ' + str(error3))\n",
    "            print('which taus failed were: ' + str(which_taus_failed))\n",
    "            print(' ')\n",
    "            \n",
    "            score_collect.append('above scores were for ' + str(element))\n",
    "            score_collect.append(' ')\n",
    "            \n",
    "        \n",
    "        self.score_collect = score_collect\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def save_CNN(self, save_str):\n",
    "        #save model and test evaluation outputs\n",
    "        #example save_str: save_str = 'displacement_10us_random_noise_10_2018_06_13_80epoch'\n",
    "        #requires test_CNN to have been run already\n",
    "        path = 'C:/Users/jakeprecht/DDHO/saved CNN models/'\n",
    "        save_str_h5 = path + save_str + '.h5'\n",
    "        save_str_txt = path + save_str + '_results.txt'\n",
    "        save_str_weights = path + save_str + '_weights.h5'\n",
    "        \n",
    "        self.model.save(save_str_h5)  # creates a HDF5 file 'save_str_h5.h5'\n",
    "        self.model.save_weights(save_str_weights)\n",
    "        \n",
    "        output_scores = open(save_str_txt, 'w')\n",
    "        for item in self.score_collect:\n",
    "            output_scores.write(\"%s\\n\" % item)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def visualize_weights(self, layer_number):\n",
    "        #layer number 0 = conv layer 1\n",
    "        #layer number 1 = ReLU 1\"\"\"\n",
    "        weights, biases = self.branch1.layers[layer_number].get_weights()\n",
    "        \n",
    "        if layer_number == 0 or 1:\n",
    "            number_filters = self.filter_number1\n",
    "            kernel_length = self.kernel1_size         \n",
    "\n",
    "        #elif layer_number == 3:\n",
    "        #    number_filters = self.filter_number2\n",
    "        #    kernel_length = self.kernel2_size\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Input for layer_number must be 0 or 3 in current implementation (2018_08_08)\")\n",
    "            \n",
    "        fig = plt.figure()\n",
    "        for i in range(number_filters):\n",
    "            weight_plt = weights[:,:,i]\n",
    "            weight_plt2 = weight_plt.reshape((kernel_length,))\n",
    "            #ax = fig.add_subplot(number_filters,1,i+1)\n",
    "            plt.figure()\n",
    "            plt.plot(weight_plt2)\n",
    "            #ax.imshow(weight_plt2,cmap='gray')\n",
    "            \n",
    "        return\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17600 samples, validate on 4400 samples\n",
      "Epoch 1/3\n",
      " - 370s - loss: 1.0673 - acc: 0.5783 - top2metric: 0.7517 - top3metric: 0.8585 - val_loss: 0.4360 - val_acc: 0.8014 - val_top2metric: 0.9357 - val_top3metric: 0.9882\n",
      "Epoch 2/3\n",
      " - 373s - loss: 0.5080 - acc: 0.7832 - top2metric: 0.9294 - top3metric: 0.9823 - val_loss: 0.3645 - val_acc: 0.8284 - val_top2metric: 0.9491 - val_top3metric: 0.9900\n",
      "Epoch 3/3\n",
      " - 374s - loss: 0.4302 - acc: 0.8160 - top2metric: 0.9430 - top3metric: 0.9885 - val_loss: 0.4268 - val_acc: 0.8045 - val_top2metric: 0.9357 - val_top3metric: 0.9864\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x170ab1a710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load and prep training data\n",
    "test = CNN()\n",
    "\n",
    "train_data = test.load_experimental_train_data(\"G:/2018_11_02 PLL on gold/tip8/mono/Run1\")#,\n",
    "                                              #\"G:/2018_11_02 PLL on gold/tip8/mono/Run3\")\n",
    "\n",
    "train_x1, train_y, val_x, val_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "model = test.train_CNN(train_x1, train_y, val_x, val_y, num_epochs = 3, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "test.plot_training_statistics(model)\n",
    "\n",
    "#test.test_experimental_CNN(\"G:/2018_11_02 PLL on gold/tip8/mono/Run1\",\n",
    "                          #\"G:/2018_11_02 PLL on gold/tip8/mono/Run3\",\n",
    "                          #\"G:/2018_11_02 PLL on gold/tip8/mono/Run2\")\n",
    "\n",
    "#test.save_CNN('2018_11_02 tip 8 train 1 3 test 2 KL 400 100 epochs 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726 ns  3.98 ns per loop (mean  std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit len(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995 ns  3.33 ns per loop (mean  std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4400\n",
      "2200\n"
     ]
    }
   ],
   "source": [
    "val_split = 0.2\n",
    "num_val_samples = int(val_split * len(train_data.index))\n",
    "val_data = train_data.tail(num_val_samples)\n",
    "print(len(val_data.index))\n",
    "\n",
    "val_data = val_data[:int(-0.5*num_val_samples)]\n",
    "\n",
    "#val_data.drop(val_data.tail(5).index,inplace=True)\n",
    "print(len(val_data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN()\n",
    "\n",
    "train_data = test.load_experimental_train_data(\"G:/2018_11_02 PLL on gold/tip8/mono/Run1\")#,\n",
    "                                              #\"G:/2018_11_02 PLL on gold/tip8/mono/Run3\")\n",
    "\n",
    "train_x1, train_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "model = test.train_CNN(train_x1,train_y, num_epochs = 20, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "test.plot_training_statistics(model)\n",
    "\n",
    "#test.test_experimental_CNN(\"G:/2018_11_02 PLL on gold/tip8/mono/Run1\",\n",
    "                          #\"G:/2018_11_02 PLL on gold/tip8/mono/Run3\",\n",
    "                          #\"G:/2018_11_02 PLL on gold/tip8/mono/Run2\")\n",
    "\n",
    "#test.save_CNN('2018_11_02 tip 8 train 1 3 test 2 KL 400 100 epochs 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "#change below lines to history.history\n",
    "#train_loss = model.history['loss']\n",
    "#val_loss = model.history['val_loss']\n",
    "train_loss = range(1,num_epochs+1)\n",
    "val_loss = range(2,num_epochs+2)\n",
    "\n",
    "#train_acc = model.history['acc']\n",
    "#val_acc = model.history['val_acc']\n",
    "train_acc = np.arange(1,num_epochs+1)/num_epochs\n",
    "val_acc = np.arange(2,num_epochs+2)/num_epochs\n",
    "\n",
    "x = range(1,num_epochs + 1) #change to self.num_epochs\n",
    "xticks = range(1,num_epochs + 1) #change to self.num_epochs\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows = 2)\n",
    "\n",
    "ax1.plot(x, train_loss, label = 'Training Loss')\n",
    "ax1.plot(x, val_loss, label = 'Validation Loss')\n",
    "ax1.legend(loc = 'upper right')\n",
    "ax1.xaxis.set_major_locator(MaxNLocator(integer=True,steps=[1,2,5,10]))\n",
    "#ax1.set_xticks(xticks)\n",
    "\n",
    "ax2.plot(x, train_acc, label = 'Training Accuracy')\n",
    "ax2.plot(x, val_acc, label = 'Validation Accuracy')\n",
    "ax2.legend(loc = 'lower right')\n",
    "ax2.xaxis.set_major_locator(MaxNLocator(integer=True,steps=[1,2,5,10]))\n",
    "ax2.set_xlabel('Number of Epochs')\n",
    "ax2.set_ylim(top=1)\n",
    "ax2.set_ylabel('Accuracy')\n",
    "#ax2.set_xticks(xticks)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN()\n",
    "\n",
    "train_data = test.load_experimental_train_data(\"G:/2018_11_02 PLL on gold/tip8/mono/Run1\",\n",
    "                                              \"G:/2018_11_02 PLL on gold/tip8/mono/Run3\")\n",
    "\n",
    "train_x1, train_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "model = test.train_CNN(train_x1,train_y, num_epochs = 10, kernel1_size = 400, kernel2_size = 100, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "test.test_experimental_CNN(\"G:/2018_11_02 PLL on gold/tip8/mono/Run1\",\n",
    "                          \"G:/2018_11_02 PLL on gold/tip8/mono/Run3\",\n",
    "                          \"G:/2018_11_02 PLL on gold/tip8/mono/Run2\")\n",
    "\n",
    "test.save_CNN('2018_11_02 tip 8 train 1 3 test 2 KL 400 100 epochs 10')\n",
    "#test.save_CNN('2018_11_05_slow_time_only_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prep training data\n",
    "test = CNN_train()\n",
    "\n",
    "train_data = test.load_experimental_train_data(\"G:/2018_12_05 PLL on gold 100 MS/mono/Run1\",\n",
    "                                              \"G:/2018_12_05 PLL on gold 100 MS/mono/Run5\")\n",
    "\n",
    "train_x1, train_y = test.preprocess_train_data(train_data)\n",
    "\n",
    "model = test.train_CNN(train_x1,train_y, num_epochs = 20, kernel1_size = 2000, kernel2_size = 500, num_filter1 = 5, num_filter2 = 3)\n",
    "\n",
    "test.test_experimental_CNN(\"G:/2018_12_05 PLL on gold 100 MS/mono/Run1\",\n",
    "                          \"G:/2018_12_05 PLL on gold 100 MS/mono/Run3\",\n",
    "                          \"G:/2018_12_05 PLL on gold 100 MS/mono/Run5\")\n",
    "\n",
    "test.save_CNN('2018_12_05 100 MS test all time scales KL 2000 500 epochs 20')\n",
    "#test.save_CNN('2018_11_05_slow_time_only_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell contains all the functions for the simulated data.\n",
    "I removed them from the main CNN_train() class to reduce clutter.\n",
    "They are mostly functional and can probably be utilized again with minimal changes by reinserting them into CNN_train()\"\"\"\n",
    "\n",
    "    def test_simulated_CNN(self, *paths):\n",
    "        \n",
    "        score_string = 'data order is testing against '\n",
    "        for element in paths:\n",
    "            score_string += (str(element) + \" , \\n\") \n",
    "\n",
    "        \n",
    "        score_collect = [score_string]\n",
    "        score_collect.append('column order is loss, accuracy, top2metric, top3metric')\n",
    "        score_collect.append('top2metric = % that the true tau was one of the top 2 predictions')\n",
    "        score_collect.append(' ')\n",
    "        \n",
    "        \n",
    "        #score_collect = ['data order is no_noise, 0pt1noise, 1noise, random_noise_1, random_noise_10']\n",
    "        #score_collect.append('first column is loss, second column is accuracy')\n",
    "        \n",
    "        for i in paths:\n",
    "            test_x1, test_x2, test_y = self.load_simulated_test_data(i)#,test_fraction)\n",
    "            score = self.model.evaluate([test_x1,test_x2],test_y, batch_size=32)\n",
    "            percentage = str(round(score[1],5) * 100)\n",
    "            print('model scored ' + percentage + '% on ' + str(i))\n",
    "            score_collect.append(str(score))\n",
    "            \n",
    "            error1, error2, error3, which_taus_failed = self.test_closeness(test_x1,test_x2,test_y)\n",
    "            score_collect.append('one_diff_error = ' + str(error1))\n",
    "            score_collect.append('two_diff_error = ' + str(error2))\n",
    "            score_collect.append('three_diff_error = ' + str(error3))\n",
    "            score_collect.append('which taus failed were: ' + str(which_taus_failed))\n",
    "            print('one_diff_error = ' + str(error1))\n",
    "            print('two_diff_error = ' + str(error2))\n",
    "            print('three_diff_error = ' + str(error3))\n",
    "            print('which taus failed were: ' + str(which_taus_failed))\n",
    "            print(' ')\n",
    "            \n",
    "            score_collect.append('above scores were for ' + str(element)) #new code on 7/2/18\n",
    "            score_collect.append(' ')\n",
    "        \n",
    "        self.score_collect = score_collect\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def load_simulated_test_data(self, file_path):#, test_fraction):\n",
    "        \"\"\"input string with file path (e.g. \"D:/jake/DDHO Data/inst_freq/25us/0pt1noise/*.npy\" )\n",
    "        and also input the fraction of the data you wish to test (e.g. testing 10% of data would be 0.1)\n",
    "    \n",
    "        outputs the requested percentage of the data (test_data_norm_reshaped) and their corresponding labels for evaluation (one_hot_tau)\n",
    "        these outputs are basically test_x and test_y that are formatted to be fed into model.evaluate()\n",
    "    \n",
    "        NOTE this function requires previous cells to have been run (train_x must exist!!!)\"\"\"\n",
    "        #This function is currently in \"many tips implementation\" form\n",
    "        \n",
    "        file_path1 = glob.glob(file_path)\n",
    "        test_pixel = np.load(file_path1[0])\n",
    "        #this will be used in for loop below\n",
    "        #it is just used to grab the length of any one inst. freq. curve\n",
    "    \n",
    "        columns2=[]\n",
    "        for i in range(len(test_pixel)-4):#-4 because k,Q,omega, tau is at end of pixel data\n",
    "        #for i in range(len(test_pixel)-2):#-2 because tfp and tau are at end of pixel data\n",
    "            columns2.append('t='+str(i))#most columns are just time points and I'm making them here\n",
    "    \n",
    "        #columns2.append('Tfp') #because tfps are appended onto the end of my inst_freq data\n",
    "        columns2.append('k')\n",
    "        columns2.append('Q')\n",
    "        columns2.append('omega')\n",
    "        columns2.append('Tau') #because taus are appended onto the end of my displacement data\n",
    "    \n",
    "        #alternate way to load only a fraction of the data to save memory and time\n",
    "        #num_samples = len(file_path1)\n",
    "        #num_buckets = self.number_of_classes\n",
    "        #bucket_range = int(num_samples/num_buckets)\n",
    "        #test_fraction_range = int(test_fraction * bucket_range)\n",
    "        #load_list = []\n",
    "\n",
    "        #for i in range(num_buckets):\n",
    "        #    for j in range(test_fraction_range):\n",
    "        #        load_list.append(int((i * bucket_range) + (j)))\n",
    "    \n",
    "        #data1 = [np.load(file_path1[i]) for i in load_list]\n",
    "        data1 = [np.load(file_path1[i]) for i in range(len(file_path1))]\n",
    "\n",
    "        #make df for output\n",
    "        test_data = pd.DataFrame(data=data1,columns=columns2,dtype=np.float64)\n",
    "        test_data = test_data.drop('t=0',axis=1)\n",
    "        #these t=0 points end up as just NaNs and are useless anyways because by definition freq shift is 0 at trigger\n",
    "\n",
    "        test_y = np.array(test_data['Tau'])\n",
    "        #Label encode the y-data as preprocessing for one hot-encoding for classification NN:\n",
    "        unique_tau, tau_index = np.unique(test_y,return_inverse=True)\n",
    "        #make one-hot encoded tau vector\n",
    "        one_hot_tau = np_utils.to_categorical(tau_index)\n",
    "    \n",
    "        #preprocess test_x\n",
    "        test_x1 = test_data.drop(['k','Q','omega','Tau'],axis=1) #x1 is just displacement\n",
    "        test_x2 = test_data[['k','Q','omega']]\n",
    "        \n",
    "        test_x1_norm = (test_x1 - self.mean_train_x1 ) /  (self.SD_train_x1) #important to preprocess my test data same as my train data!!!!\n",
    "        test_x2_norm = (test_x2 - self.mean_train_x2 ) /  (self.SD_train_x2) #important to preprocess my test data same as my train data!!!!\n",
    "        \n",
    "        test_x1_norm_reshaped = np.expand_dims(test_x1_norm,axis=2)\n",
    "        test_x2_norm_reshaped = np.expand_dims(test_x2_norm,axis=2)\n",
    "        \n",
    "        \n",
    "        return test_x1_norm_reshaped, test_x2_norm_reshaped, one_hot_tau\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        def load_simulated_train_data(self, *paths):\n",
    "        \"\"\"This function loads training data generated by my Python simulations \n",
    "        (use Biexp.py or generate_train_data.py to generate simulated data)\n",
    "        \n",
    "        Inputs:\n",
    "        *paths = e.g. (\"D:/jake/DDHO Data/displacement/10us/random_noise_10/*.npy\",\n",
    "        \"D:/jake/DDHO Data/displacement/2018_09_18/tip1/0noise/*.npy\")\n",
    "        where each path is to all of the .npy files in those folders\n",
    "        ^ the .npy files are generated using my Biexp.py simulation code \n",
    "        (or a function utilizing Biexp.py such as generate_train_data.py)\n",
    "        \n",
    "        Outputs:\n",
    "        df_main = a Pandas DataFrame containing all of the z(t) data #as well as k, Q, omega, and Tau for each simulation\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialize DataFrame for collecting each run\n",
    "        df_main = pd.DataFrame()\n",
    "        \n",
    "        for i in paths:\n",
    "            \n",
    "            #grab a list of strings to every file path from the inputted folders\n",
    "            file_path = glob.glob(i)\n",
    "        \n",
    "            test_pixel = np.load(file_path[0])\n",
    "            #this will be used in for loop below to grab the length of any one simulated displacement curve\n",
    "        \n",
    "            #initialize DataFrame column names list (name for each feature point of NN)\n",
    "            columns=[]\n",
    "            for j in range(len(test_pixel)-4):#-4 because k,Q,omega,tau is at end of simulated pixel data\n",
    "                columns.append('t='+str(j))   #most columns are just time points and I'm making them here\n",
    "    \n",
    "            #name other columns that are not just time points (e.g. k, Q, omega)\n",
    "            #columns.append('Tfp') #because tfps are appended onto the 2nd to last column of my inst_freq data.  Uncomment for Inst_freq data!\n",
    "            columns.append('k')\n",
    "            columns.append('Q')\n",
    "            columns.append('omega')\n",
    "            columns.append('Tau') #because true taus are appended onto the end of my data\n",
    "    \n",
    "    \n",
    "            #load all of the data into an array for input into DataFrame\n",
    "            #each entry in the \"data1\" array is a numpy array of a displacement curve\n",
    "            data1 = [np.load(file_path[i]) for i in range(0,(len(file_path)))]\n",
    "    \n",
    "            #make df for output\n",
    "            train_data = pd.DataFrame(data=data1,columns=columns,dtype=np.float64)\n",
    "            train_data = train_data.drop('t=0',axis=1)\n",
    "            #these t=0 points end up as just NaNs after preprocessing and are useless anyways because by definition freq shift is 0 at trigger for all points\n",
    "            #dropping t=0 maybe unnecessary with displacement data?\n",
    "            \n",
    "            df_main = pd.concat([df_main,train_data], ignore_index=True) #append each run to the final collection DataFrame\n",
    "        \n",
    "        return df_main\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
